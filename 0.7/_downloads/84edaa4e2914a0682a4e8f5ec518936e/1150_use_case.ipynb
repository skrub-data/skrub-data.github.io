{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Use case: developing locally and deploying to production\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a team of data scientists, we are tasked with a project to predict whether an email\nis potentially malicious (i.e., spam or phishing). We develop and test our models\nlocally, either in a Jupyter notebook or within a Python script. Once we are satisfied\nwith the model's performance, we move on to deploying it.\n\nIn this use case, every time the email provider receives a new email, they want to\nverify whether it is spam before displaying it in the recipient's inbox. To achieve\nthis, they plan to integrate a machine learning model within a microservice. This\nmicroservice will accept an email's data as a JSON payload and return a score between\n0 and 1, indicating the likelihood that the email is spam.\n\nTo avoid rewriting the entire data pipeline when moving from model validation to\nproduction deployment, which is both error-prone and inefficient, we prefer to load an\nobject that encapsulates the same processing pipeline used during model development.\nThis is where the :class:`~skrub.SkrubLearner` can help.\n\nAdopting this workflow also has the benefit of forcing us to clearly define the type\nof data that will be available at the input of the microservice. It helps ensure we\nbuild models that rely only on information accessible at this specific point in the\nproduct pipeline. For example, since we want to detect spam before the email reaches\nthe recipient's inbox, we cannot use features that are only available after the\nrecipient opens the email.\n\nSince this example is focused on the pipeline construction itself, we won't look at\nour model performance.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating the training data\nIn this section, we define a few functions that help us with generating the\ntraining data in dictionary form. We are going to generate a fully random data set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import random\nimport string\nimport uuid\nfrom datetime import datetime, timedelta\n\nimport numpy as np\n\n\ndef generate_id():\n    return str(uuid.uuid4())\n\n\ndef generate_email():\n    length = random.randint(5, 10)\n    username = \"\".join(random.choice(string.ascii_lowercase) for _ in range(length))\n    domain = [\"google\", \"yahoo\", \"whatever\"]\n    tld = [\"fr\", \"en\", \"com\", \"net\"]\n    return f\"{username}@{random.choice(domain)}.{random.choice(tld)}\"\n\n\ndef generate_datetime():\n    random_seconds = random.randint(0, int(timedelta(days=2).total_seconds()))\n    random_datetime = datetime.now() - timedelta(seconds=random_seconds)\n    return random_datetime\n\n\ndef generate_text(min_str_length, max_str_length):\n    random_length = random.randint(min_str_length, max_str_length)\n    random_text = \"\".join(\n        random.choice(string.ascii_letters + string.digits + string.punctuation)\n        for _ in range(random_length)\n    )\n    return random_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We generate 1000 training samples and store them in a list of dictionaries:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this use case, the emails to be tested when the model is put in production\nare not contained in a dataframe, but in a JSON. As a result, our training data\nshould also be contained in a list of dictionaries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = [\n    {\n        \"id\": generate_id(),\n        \"sender\": generate_email(),\n        \"title\": generate_text(max_str_length=10, min_str_length=2),\n        \"content\": generate_text(max_str_length=100, min_str_length=10),\n        \"date\": generate_datetime(),\n        \"cc_emails\": [generate_email() for _ in range(random.randint(0, 5))],\n    }\n    for _ in range(n_samples)\n]\n\n\n# generate array of 1 and 0 to represent the target variable\ny = np.random.binomial(n=1, p=0.9, size=n_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the DataOps plan\nLet's start our DataOps plan by indicating what the features and the target\nvariables are.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import skrub\n\nX = skrub.X(X)\ny = skrub.y(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The variable X is currently a list of dictionaries, which estimators cannot\nhandle directly. Let's convert it to a pandas DataFrame using\n:func:`~skrub.DataOp.skb.apply_func`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\ndf = X.skb.apply_func(pd.DataFrame)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For this example, we will use a strong baseline, with skrub's\n:func:`~skrub.tabular_pipeline()`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tab_pipeline = skrub.tabular_pipeline(\"classification\")\n\n# We can now apply the predictive model to the data.\n# The DataOps plan is ready after applying the model to the data.\npredictions = df.skb.apply(tab_pipeline, y=y)\n\n# We can then explore the full plan:\npredictions.skb.draw_graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To end the explorative work, we need to build the learner, fit it, and save it to a\nfile.\nPassing ``fitted=True`` to the :func:`~skrub.DataOp.skb.make_learner`\nfunction makes it so that the learner is fitted on the data that has been passed to\nthe variables of the DataOps plan.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import joblib\n\nwith open(\"learner.pkl\", \"wb\") as f:\n    learner = predictions.skb.make_learner(fitted=True)\n    joblib.dump(learner, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Production phase\n\nIn our microservice, we receive a payload in JSON format.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_input = {\n    \"id\": generate_id(),\n    \"sender\": generate_email(),\n    \"title\": generate_text(max_str_length=10, min_str_length=2),\n    \"content\": generate_text(max_str_length=100, min_str_length=10),\n    \"date\": generate_datetime(),\n    \"cc_emails\": [generate_email() for _ in range(random.randint(0, 5))],\n}\n\n# We just have to load the learner and use it to predict the score for this input.\nwith open(\"learner.pkl\", \"rb\") as f:\n    loaded_learner = joblib.load(f)\n# ``X_input`` must be passed as a list so that it can be parsed correctly as a dataframe\n# by Pandas.\nprediction = loaded_learner.predict({\"X\": [X_input]})\nprediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nThanks to the skrub DataOps and learner, we ensure that all the transformations\nand preprocessing done during model development are exactly the same as those done in\nproduction. This makes deployment straightforward and reduces the risk of errors\nwhen moving from development to production environments.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}