{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the skrub examples in JupyterLite is experimental and you mayencounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import skrub` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/skrub-data/skrub/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport micropip\nawait micropip.install('skrub')\n%pip install pyodide-http\nimport pyodide_http\npyodide_http.patch_all()\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Various string encoders: a sentiment analysis example\n\nIn this example, we explore the performance of string and categorical encoders\navailable in skrub.\n\n.. |GapEncoder| replace::\n     :class:`~skrub.GapEncoder`\n\n.. |MinHashEncoder| replace::\n     :class:`~skrub.MinHashEncoder`\n\n.. |TextEncoder| replace::\n     :class:`~skrub.TextEncoder`\n\n.. |StringEncoder| replace::\n     :class:`~skrub.StringEncoder`\n\n.. |TableReport| replace::\n     :class:`~skrub.TableReport`\n\n.. |TableVectorizer| replace::\n     :class:`~skrub.TableVectorizer`\n\n.. |pipeline| replace::\n     :class:`~sklearn.pipeline.Pipeline`\n\n.. |HistGradientBoostingClassifier| replace::\n     :class:`~sklearn.ensemble.HistGradientBoostingClassifier`\n\n.. |RandomizedSearchCV| replace::\n     :class:`~sklearn.model_selection.RandomizedSearchCV`\n\n.. |GridSearchCV| replace::\n     :class:`~sklearn.model_selection.GridSearchCV`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Toxicity dataset\nWe focus on the toxicity dataset, a corpus of 1,000 tweets, evenly balanced\nbetween the binary labels \"Toxic\" and \"Not Toxic\".\nOur goal is to classify each entry between these two labels, using only the\ntext of the tweets as features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub.datasets import fetch_toxicity\n\ndataset = fetch_toxicity()\nX, y = dataset.X, dataset.y\nX[\"is_toxic\"] = y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When it comes to displaying large chunks of text, the |TableReport| is especially\nuseful! Click on any cell below to expand and read the tweet in full.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import TableReport\n\nTableReport(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GapEncoder\nFirst, let's vectorize our text column using the |GapEncoder|, one of the\n[high cardinality categorical encoders](https://inria.hal.science/hal-02171256v4)\nprovided by skrub.\nAs introduced in the `previous example<example_encodings>`, the |GapEncoder|\nperforms matrix factorization for topic modeling. It builds latent topics by\ncapturing combinations of substrings that frequently co-occur, and encoded vectors\ncorrespond to topic activations.\n\nTo interpret these latent topics, we select for each of them a few labels from\nthe input data with the highest activations. In the example below we select 3 labels\nto summarize each topic.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import GapEncoder\n\ngap = GapEncoder(n_components=30)\nX_trans = gap.fit_transform(X[\"text\"])\n# Add the original text as a first column\nX_trans.insert(0, \"text\", X[\"text\"])\nTableReport(X_trans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use a heatmap to highlight the highest activations, making them more visible\nfor comparison against the original text and vectors above.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom matplotlib import pyplot as plt\n\n\ndef plot_gap_feature_importance(X_trans):\n    x_samples = X_trans.pop(\"text\")\n\n    # We slightly format the topics and labels for them to fit on the plot.\n    topic_labels = [x.replace(\"text: \", \"\") for x in X_trans.columns]\n    labels = x_samples.str[:50].values + \"...\"\n\n    # We clip large outliers to make activations more visible.\n    X_trans = np.clip(X_trans, a_min=None, a_max=200)\n\n    plt.figure(figsize=(10, 10), dpi=200)\n    plt.imshow(X_trans.T)\n\n    plt.yticks(\n        range(len(topic_labels)),\n        labels=topic_labels,\n        ha=\"right\",\n        size=12,\n    )\n    plt.xticks(range(len(labels)), labels=labels, size=12, rotation=50, ha=\"right\")\n\n    plt.colorbar().set_label(label=\"Topic activations\", size=13)\n    plt.ylabel(\"Latent topics\", size=14)\n    plt.xlabel(\"Data entries\", size=14)\n    plt.tight_layout()\n    plt.show()\n\n\nplot_gap_feature_importance(X_trans.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have an understanding of the vectors produced by the |GapEncoder|,\nlet's evaluate its performance in toxicity classification. The |GapEncoder| excels\nat handling categorical columns with high cardinality, but here the column consists\nof free-form text. Sentences are generally longer, with more unique ngrams than\nhigh cardinality categories.\n\nTo benchmark the performance of the |GapEncoder| against the toxicity dataset,\nwe integrate it into a |TableVectorizer|, as introduced in the\n`previous example<example_encodings>`,\nand create a |pipeline| by appending a |HistGradientBoostingClassifier|, which\nconsumes the vectors produced by the |GapEncoder|.\n\nWe set ``n_components`` to 30; however, to achieve the best performance, we would\nneed to find the optimal value for this hyperparameter using either |GridSearchCV|\nor |RandomizedSearchCV|. We skip this part to keep the computation time for this\nsmall example.\n\nRecall that the ROC AUC is a metric that quantifies the ranking power of estimators,\nwhere a random estimator scores 0.5, and an oracle \u2014providing perfect predictions\u2014\nscores 1.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import make_pipeline\n\nfrom skrub import TableVectorizer\n\n\ndef plot_box_results(named_results):\n    fig, ax = plt.subplots()\n    names, scores = zip(\n        *[(name, result[\"test_score\"]) for name, result in named_results]\n    )\n    ax.boxplot(scores)\n    ax.set_xticks(range(1, len(names) + 1), labels=list(names), size=12)\n    ax.set_ylabel(\"ROC AUC\", size=14)\n    plt.title(\n        \"AUC distribution across folds (higher is better)\",\n        size=14,\n    )\n    plt.show()\n\n\nresults = []\n\ny = X.pop(\"is_toxic\").map({\"Toxic\": 1, \"Not Toxic\": 0})\n\ngap_pipe = make_pipeline(\n    TableVectorizer(high_cardinality=GapEncoder(n_components=30)),\n    HistGradientBoostingClassifier(),\n)\ngap_results = cross_validate(gap_pipe, X, y, scoring=\"roc_auc\")\nresults.append((\"GapEncoder\", gap_results))\n\nplot_box_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### MinHashEncoder\nWe now compare these results with the |MinHashEncoder|, which is faster\nand produces vectors better suited for tree-based estimators like\n|HistGradientBoostingClassifier|. To do this, we can simply replace\nthe |GapEncoder| with the |MinHashEncoder| in the previous pipeline\nusing ``set_params()``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import MinHashEncoder\n\nminhash_pipe = make_pipeline(\n    TableVectorizer(high_cardinality=MinHashEncoder(n_components=30)),\n    HistGradientBoostingClassifier(),\n)\nminhash_results = cross_validate(minhash_pipe, X, y, scoring=\"roc_auc\")\nresults.append((\"MinHashEncoder\", minhash_results))\n\nplot_box_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remarkably, the vectors produced by the |MinHashEncoder| offer less predictive\npower than those from the |GapEncoder| on this dataset.\n\n### TextEncoder\nLet's now shift our focus to pre-trained deep learning encoders. Our previous\nencoders are syntactic models that we trained directly on the toxicity dataset.\nTo generate more powerful vector representations for free-form text and diverse\nentries, we can instead use semantic models, such as BERT, which have been trained\non very large datasets.\n\n|TextEncoder| enables you to integrate any Sentence Transformer model from the\nHugging Face Hub (or from your local disk) into your |pipeline| to transform a text\ncolumn in a dataframe. By default, |TextEncoder| uses the e5-small-v2 model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import TextEncoder\n\ntext_encoder = TextEncoder(\n    \"sentence-transformers/paraphrase-albert-small-v2\",\n    device=\"cpu\",\n)\n\ntext_encoder_pipe = make_pipeline(\n    TableVectorizer(high_cardinality=text_encoder),\n    HistGradientBoostingClassifier(),\n)\ntext_encoder_results = cross_validate(text_encoder_pipe, X, y, scoring=\"roc_auc\")\nresults.append((\"TextEncoder\", text_encoder_results))\n\nplot_box_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### StringEncoder\n|TextEncoder| embeddings are very strong, but they are also quite expensive to\nuse. A simpler, faster alternative for encoding strings is the |StringEncoder|,\nwhich works by first performing a tf-idf (computing vectors of rescaled word\ncounts of the text [wiki](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)), and then\nfollowing it with TruncatedSVD to reduce the number of dimensions to, in this\ncase, 30.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import StringEncoder\n\nstring_encoder = StringEncoder(ngram_range=(3, 4), analyzer=\"char_wb\", random_state=0)\n\nstring_encoder_pipe = make_pipeline(\n    TableVectorizer(high_cardinality=string_encoder),\n    HistGradientBoostingClassifier(),\n)\n\nstring_encoder_results = cross_validate(string_encoder_pipe, X, y, scoring=\"roc_auc\")\nresults.append((\"StringEncoder\", string_encoder_results))\n\nplot_box_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance of the |TextEncoder| is significantly stronger than that of\nthe syntactic encoders, which is expected. But how long does it take to load\nand vectorize text on a CPU using a Sentence Transformer model? Below, we display\nthe tradeoff between predictive accuracy and training time. Note that since we are\nnot training the Sentence Transformer model, the \"fitting time\" refers to the\ntime taken for vectorization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_performance_tradeoff(results):\n    fig, ax = plt.subplots(figsize=(5, 4), dpi=200)\n    markers = [\"s\", \"o\", \"^\", \"x\"]\n    for idx, (name, result) in enumerate(results):\n        ax.scatter(\n            result[\"fit_time\"],\n            result[\"test_score\"],\n            label=name,\n            marker=markers[idx],\n        )\n        mean_fit_time = np.mean(result[\"fit_time\"])\n        mean_score = np.mean(result[\"test_score\"])\n        ax.scatter(\n            mean_fit_time,\n            mean_score,\n            color=\"k\",\n            marker=markers[idx],\n        )\n        std_fit_time = np.std(result[\"fit_time\"])\n        std_score = np.std(result[\"test_score\"])\n        ax.errorbar(\n            x=mean_fit_time,\n            y=mean_score,\n            yerr=std_score,\n            fmt=\"none\",\n            c=\"k\",\n            capsize=2,\n        )\n        ax.errorbar(\n            x=mean_fit_time,\n            y=mean_score,\n            xerr=std_fit_time,\n            fmt=\"none\",\n            c=\"k\",\n            capsize=2,\n        )\n        ax.set_xscale(\"log\")\n\n        ax.set_xlabel(\"Time to fit (seconds)\")\n        ax.set_ylabel(\"ROC AUC\")\n        ax.set_title(\"Prediction performance / training time trade-off\")\n\n    ax.annotate(\n        \"Best time / \\nperformance trade-off\",\n        xy=(0.05, 0.95),\n        xycoords=\"axes fraction\",\n        xytext=(0.2, 0.8),\n        textcoords=\"axes fraction\",\n        arrowprops=dict(arrowstyle=\"->\", lw=1.5, mutation_scale=15),\n    )\n    ax.legend(bbox_to_anchor=(1.02, 0.3))\n    plt.show()\n\n\nplot_performance_tradeoff(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The black points represent the average time to fit and AUC for each vectorizer,\nand the width of the bars represents one standard deviation.\n\nThe green outlier dot on the right side of the plot corresponds to the first time\nthe Sentence Transformers model was downloaded and loaded into memory.\nDuring the subsequent cross-validation iterations, the model is simply copied,\nwhich reduces computation time for the remaining folds.\n\nInterestingly, |StringEncoder| has a performance remarkably similar to that of\n|GapEncoder|, while being significantly faster.\n\n## Conclusion\nIn conclusion, |TextEncoder| provides powerful vectorization for text, but at\nthe cost of longer computation times and the need for additional dependencies,\nsuch as torch. |StringEncoder| represents a simpler alternative that can provide\ngood performance at a fraction of the cost of more complex methods.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}