{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the skrub examples in JupyterLite is experimental and you mayencounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import skrub` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/skrub-data/skrub/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport micropip\nawait micropip.install('skrub')\n%pip install seaborn\n%pip install pyodide-http\nimport pyodide_http\npyodide_http.patch_all()\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# AggJoiner on a credit fraud dataset\n\nMany problems involve tables whose entities have a one-to-many relationship.\nTo simplify aggregate-then-join operations for machine learning, we can include\nthe |AggJoiner| in our pipeline.\n\n\nIn this example, we are tackling a fraudulent loan detection use case.\nBecause fraud is rare, this dataset is extremely imbalanced, with a prevalence of around\n1.4%.\n\nThe data consists of two distinct entities: e-commerce \"baskets\", and \"products\".\nBaskets can be tagged fraudulent (1) or not (0), and are essentially a list of products\nof variable size. Each basket is linked to at least one products, e.g. basket 1 can have\nproduct 1 and 2.\n\n<img src=\"file://../../_static/08_example_data.png\" width=\"450 px\">\n\n|\n\nOur aim is to predict which baskets are fraudulent.\n\nThe products dataframe can be joined on the baskets dataframe using the ``basket_ID``\ncolumn.\n\nEach product has several attributes:\n\n- a category (marked by the column ``\"item\"``),\n- a model (``\"model\"``),\n- a brand (``\"make\"``),\n- a merchant code (``\"goods_code\"``),\n- a price per unit (``\"cash_price\"``),\n- a quantity selected in the basket (``\"Nbr_of_prod_purchas\"``)\n\n.. |AggJoiner| replace::\n     :class:`~skrub.AggJoiner`\n\n.. |Joiner| replace::\n     :class:`~skrub.Joiner`\n\n.. |DropCols| replace::\n     :class:`~skrub.DropCols`\n\n.. |TableVectorizer| replace::\n     :class:`~skrub.TableVectorizer`\n\n.. |TableReport| replace::\n     :class:`~skrub.TableReport`\n\n.. |MinHashEncoder| replace::\n     :class:`~skrub.MinHashEncoder`\n\n.. |TargetEncoder| replace::\n     :class:`~sklearn.preprocessing.TargetEncoder`\n\n.. |make_pipeline| replace::\n     :func:`~sklearn.pipeline.make_pipeline`\n\n.. |Pipeline| replace::\n     :class:`~sklearn.pipeline.Pipeline`\n\n.. |HGBC| replace::\n     :class:`~sklearn.ensemble.HistGradientBoostingClassifier`\n\n.. |OrdinalEncoder| replace::\n     :class:`~sklearn.preprocessing.OrdinalEncoder`\n\n.. |TunedThresholdClassifierCV| replace::\n     :class:`~sklearn.model_selection.TunedThresholdClassifierCV`\n\n.. |CalibrationDisplay| replace::\n     :class:`~sklearn.calibration.CalibrationDisplay`\n\n.. |pandas.melt| replace::\n     :func:`~pandas.melt`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import TableReport\nfrom skrub.datasets import fetch_credit_fraud\n\nbunch = fetch_credit_fraud()\nproducts, baskets = bunch.products, bunch.baskets\nTableReport(products)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "TableReport(baskets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Naive aggregation\n\nLet's explore a naive solution first.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Click `here<agg-joiner-anchor>` to skip this section and see the AggJoiner\n   in action!</p></div>\n\n\nThe first idea that comes to mind to merge these two tables is to aggregate the\nproducts attributes into lists, using their basket IDs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "products_grouped = products.groupby(\"basket_ID\").agg(list)\nTableReport(products_grouped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can expand all lists into columns, as if we were \"flattening\" the dataframe.\nWe end up with a products dataframe ready to be joined on the baskets dataframe, using\n``\"basket_ID\"`` as the join key.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\nproducts_flatten = []\nfor col in products_grouped.columns:\n    cols = [f\"{col}{idx}\" for idx in range(24)]\n    products_flatten.append(pd.DataFrame(products_grouped[col].to_list(), columns=cols))\nproducts_flatten = pd.concat(products_flatten, axis=1)\nproducts_flatten.insert(0, \"basket_ID\", products_grouped.index)\nTableReport(products_flatten)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Look at the \"Stats\" section of the |TableReport| above. Does anything strike you?\n\nNot only did we create 144 columns, but most of these columns are filled with NaN,\nwhich is very inefficient for learning!\n\nThis is because each basket contains a variable number of products, up to 24, and we\ncreated one column for each product attribute, for each position (up to 24) in\nthe dataframe.\n\nMoreover, if we wanted to replace text columns with encodings, we would create\n$d \\times 24 \\times 2$ columns (encoding of dimensionality $d$, for\n24 products, for the ``\"item\"`` and ``\"make\"`` columns), which would explode the\nmemory usage.\n\n\n## AggJoiner\nLet's now see how the |AggJoiner| can help us solve this. We begin with splitting our\nbasket dataset in a training and testing set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n\nX, y = baskets[[\"ID\"]], baskets[\"fraud_flag\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.1)\nX_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before aggregating our product dataframe, we need to vectorize our categorical\ncolumns. To do so, we use:\n\n- |MinHashEncoder| on \"item\" and \"model\" columns, because they both expose typos\n  and text similarities.\n- |OrdinalEncoder| on \"make\" and \"goods_code\" columns, because they consist in\n  orthogonal categories.\n\nWe bring this logic into a |TableVectorizer| to vectorize these columns in a\nsingle step.\nSee [this example](https://skrub-data.org/stable/auto_examples/01_encodings.html#specializing-the-tablevectorizer-for-histgradientboosting)\nfor more details about these encoding choices.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n\nfrom skrub import MinHashEncoder, TableVectorizer\n\nvectorizer = TableVectorizer(\n    high_cardinality=MinHashEncoder(),  # encode [\"item\", \"model\"]\n    specific_transformers=[\n        (OrdinalEncoder(), [\"make\", \"goods_code\"]),\n    ],\n)\nproducts_transformed = vectorizer.fit_transform(products)\nTableReport(products_transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our objective is now to aggregate this vectorized product dataframe by\n``\"basket_ID\"``, then to merge it on the baskets dataframe, still on\nthe ``\"basket_ID\"``.\n\n<img src=\"file://../../_static/08_example_aggjoiner.png\" width=\"900\">\n\n|\n\n|AggJoiner| can help us achieve exactly this. We need to pass the product dataframe as\nan auxiliary table argument to |AggJoiner| in ``__init__``. The ``aux_key`` argument\nrepresent both the columns used to groupby on, and the columns used to join on.\n\nThe basket dataframe is our main table, and we indicate the columns to join on with\n``main_key``. Note that we pass the main table during ``fit``, and we discuss the\nlimitations of this design in the conclusion at the bottom of this notebook.\n\nThe minimum (\"min\") is the most appropriate operation to aggregate encodings from\n|MinHashEncoder|, for reasons that are out of the scope of this notebook.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import AggJoiner\nfrom skrub import selectors as s\n\n# Skrub selectors allow us to select columns using regexes, which reduces\n# the boilerplate.\nminhash_cols_query = s.glob(\"item*\") | s.glob(\"model*\")\nminhash_cols = s.select(products_transformed, minhash_cols_query).columns\n\nagg_joiner = AggJoiner(\n    aux_table=products_transformed,\n    aux_key=\"basket_ID\",\n    main_key=\"ID\",\n    cols=minhash_cols,\n    operations=[\"min\"],\n)\nbaskets_products = agg_joiner.fit_transform(baskets)\nTableReport(baskets_products)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we understand how to use the |AggJoiner|, we can now assemble our pipeline by\nchaining two |AggJoiner| together:\n\n- the first one to deal with the |MinHashEncoder| vectors as we just saw\n- the second one to deal with the all the other columns\n\nFor the second |AggJoiner|, we use the mean, standard deviation, minimum and maximum\noperations to extract a representative summary of each distribution.\n\n|DropCols| is another skrub transformer which removes the \"ID\" column, which doesn't\nbring any information after the joining operation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.stats import loguniform, randint\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.pipeline import make_pipeline\n\nfrom skrub import DropCols\n\nmodel = make_pipeline(\n    AggJoiner(\n        aux_table=products_transformed,\n        aux_key=\"basket_ID\",\n        main_key=\"ID\",\n        cols=minhash_cols,\n        operations=[\"min\"],\n    ),\n    AggJoiner(\n        aux_table=products_transformed,\n        aux_key=\"basket_ID\",\n        main_key=\"ID\",\n        cols=[\"make\", \"goods_code\", \"cash_price\", \"Nbr_of_prod_purchas\"],\n        operations=[\"sum\", \"mean\", \"std\", \"min\", \"max\"],\n    ),\n    DropCols([\"ID\"]),\n    HistGradientBoostingClassifier(),\n)\nmodel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We tune the hyper-parameters of the |HGBC| model using ``RandomizedSearchCV``.\nBy default, the |HGBC| applies early stopping when there are at least 10_000\nsamples so we don't need to explicitly tune the number of trees (``max_iter``).\nTherefore we set this at a very high level of 1_000. We increase\n``n_iter_no_change`` to make sure early stopping does not kick in too early.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from time import time\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_distributions = dict(\n    histgradientboostingclassifier__learning_rate=loguniform(1e-2, 5e-1),\n    histgradientboostingclassifier__min_samples_leaf=randint(2, 64),\n    histgradientboostingclassifier__max_leaf_nodes=[None, 10, 30, 60, 90],\n    histgradientboostingclassifier__n_iter_no_change=[50],\n    histgradientboostingclassifier__max_iter=[1000],\n)\n\ntic = time()\nsearch = RandomizedSearchCV(\n    model,\n    param_distributions,\n    scoring=\"neg_log_loss\",\n    refit=False,\n    n_iter=10,\n    cv=3,\n    verbose=1,\n).fit(X_train, y_train)\nprint(f\"This operation took {time() - tic:.1f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The best hyper parameters are:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pd.Series(search.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To benchmark our performance, we plot the log loss of our model on the test set\nagainst the log loss of a dummy model that always output the observed probability of\nthe two classes.\n\nAs this dataset is extremely imbalanced, this dummy model should be a good baseline.\n\nThe vertical bar represents one standard deviation around the mean of the cross\nvalidation log-loss.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import log_loss\n\nresults = search.cv_results_\nbest_idx = search.best_index_\nlog_loss_model_mean = -results[\"mean_test_score\"][best_idx]\nlog_loss_model_std = results[\"std_test_score\"][best_idx]\n\ndummy = DummyClassifier(strategy=\"prior\").fit(X_train, y_train)\ny_proba_dummy = dummy.predict_proba(X_test)\nlog_loss_dummy = log_loss(y_true=y_test, y_pred=y_proba_dummy)\n\nfig, ax = plt.subplots()\nax.bar(\n    height=[log_loss_model_mean, log_loss_dummy],\n    x=[\"AggJoiner model\", \"Dummy\"],\n    color=[\"C0\", \"C4\"],\n)\nfor container in ax.containers:\n    ax.bar_label(container, padding=4)\n\nax.vlines(\n    x=\"AggJoiner model\",\n    ymin=log_loss_model_mean - log_loss_model_std,\n    ymax=log_loss_model_mean + log_loss_model_std,\n    linestyle=\"-\",\n    linewidth=1,\n    color=\"k\",\n)\nsns.despine()\nax.set_title(\"Log loss (lower is better)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\nWith |AggJoiner|, you can bring the aggregation and joining operations within a\nsklearn pipeline, and train models more efficiently.\n\nOne known limitation of both the |AggJoiner| and |Joiner| is that the auxiliary data\nto join is passed during the ``__init__`` method instead of the ``fit`` method, and\nis therefore fixed once the model has been trained.\nThis limitation causes two main issues:\n\n1. **Bigger model serialization:** Since the dataset has to be pickled along with\nthe model, it can result in a massive file size on disk.\n\n2. **Inflexibility with new, unseen data in a production environment:** To use new\nauxiliary data, you would need to replace the auxiliary table in the |AggJoiner| that\nwas used during ``fit`` with the updated data, which is a rather hacky approach.\n\nThese limitations will be addressed later in skrub.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}