{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the skrub examples in JupyterLite is experimental and you mayencounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import skrub` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/skrub-data/skrub/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport micropip\nawait micropip.install('skrub')\n%pip install seaborn\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Deduplicating misspelled categories\n\nReal world datasets often come with misspellings, for instance\nin manually inputted categorical variables.\nSuch misspelling break data analysis steps that require\nexact matching, such as a ``GROUP BY`` operation.\n\nMerging multiple variants of the same category is known as\n*deduplication*. It is implemented in skrub with the |deduplicate| function.\n\nDeduplication relies on *unsupervised learning*. It finds structures in\nthe data without providing a-priori known and explicit labels/categories.\nSpecifically, measuring the distance between strings can be used to\nfind clusters of strings that are similar to each other (e.g. differ only\nby a misspelling) and hence, flag and regroup potentially\nmisspelled category names in an unsupervised manner.\n\n\n.. |deduplicate| replace::\n    :func:`~skrub.deduplicate`\n\n.. |Gap| replace::\n     :class:`~skrub.GapEncoder`\n\n.. |MinHash| replace::\n     :class:`~skrub.MinHashEncoder`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A typical use case\n\nLet's take an example:\nas a data scientist, your job is to analyze the data from a hospital ward.\nIn the data, we notice that in most cases, the doctor prescribes\none of three following medications:\n\"Contrivan\", \"Genericon\" or \"Zipholan\".\n\nHowever, data entry is manual and - either because the doctor's\nhandwriting was hard to decipher, or due to mistakes during input -\nthere are multiple spelling mistakes in the dataset.\n\nLet's generate this example dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\n\nfrom skrub.datasets import make_deduplication_data\n\nduplicated_names = make_deduplication_data(\n    examples=[\"Contrivan\", \"Genericon\", \"Zipholan\"],  # our three medication names\n    entries_per_example=[500, 100, 1500],  # their respective number of occurrences\n    prob_mistake_per_letter=0.05,  # 5% probability of typo per letter\n    random_state=42,  # set seed for reproducibility\n)\n\nduplicated_names[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then extract the unique medication names in the data and\nvisualize how often they appear:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nunique_examples, counts = np.unique(duplicated_names, return_counts=True)\n\nplt.figure(figsize=(10, 15))\nplt.barh(unique_examples, counts)\nplt.ylabel(\"Medication name\")\nplt.xlabel(\"Count\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We clearly see the structure of the data:\nthe three original medications (\"Contrivan\", \"Genericon\" and \"Zipholan\")\nare the most common ones, but there are many spelling mistakes or\nslight variations of the original names.\n\nThe idea behind |deduplicate| is to use the fact that\nthe string distance of misspelled medications will be\nclosest to their original (most frequent) medication name\n- and therefore form clusters.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deduplication: suggest corrections of misspelled names\n\nThe |deduplicate| function uses clustering based on\nstring similarities to group duplicated names.\n\nLet's deduplicate our data:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import deduplicate\n\ndeduplicated_data = deduplicate(duplicated_names)\n\ndeduplicated_data[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And that's it! We now have the deduplicated data.\n\n.. topic:: Note:\n\n   The number of clusters will need some adjustment depending on the data.\n   If no fixed number of clusters is given, |deduplicate| tries to set it\n   automatically via the\n   [silhouette score](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize the distribution of categories in the deduplicated data:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "deduplicated_unique_examples, deduplicated_counts = np.unique(\n    deduplicated_data, return_counts=True\n)\ndeduplicated_series = pd.Series(deduplicated_counts, index=deduplicated_unique_examples)\n\nplt.figure(figsize=(10, 5))\nplt.barh(deduplicated_unique_examples, deduplicated_counts)\nplt.xlabel(\"Count\")\nplt.ylabel(\"Medication name\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, the silhouette score finds the ideal number of\nclusters (3) and groups the spelling mistakes.\n\nIn practice, the translation/deduplication will often be imperfect\nand require some tweaks.\nIn this case, we can construct and update a translation table based on the\ndata returned by |deduplicate|.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create a table that maps original to corrected categories\ntranslation_table = pd.Series(deduplicated_data, index=duplicated_names)\n\n# remove duplicates in the original data\ntranslation_table = translation_table[~translation_table.index.duplicated(keep=\"first\")]\n\ntranslation_table.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this table, we have the category name on the left,\nand the cluster it was translated to on the right.\nIf we want to adapt the translation table, we can\nmodify it manually.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing string pair-wise distance between names\n\nBelow, we use a heatmap to visualize the pairwise-distance between medication\nnames. A darker color means that two medication names are closer together\n(i.e. more similar), a lighter color means a larger distance.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import squareform\n\nfrom skrub import compute_ngram_distance\n\nngram_distances = compute_ngram_distance(unique_examples)\nsquare_distances = squareform(ngram_distances)\n\nimport seaborn as sns\n\nfig, ax = plt.subplots(figsize=(14, 12))\nsns.heatmap(\n    square_distances, yticklabels=unique_examples, xticklabels=unique_examples, ax=ax\n)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have three clusters appearing - the original medication\nnames and their misspellings that form a cluster around them.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this example, we have seen how to use the |deduplicate| function to\nautomatically detect and correct misspelled category names.\n\nNote that deduplication is especially useful when we either\nknow our ground truth (e.g. the original medication names),\nor when the similarity across strings does not\ncarry useful information for our machine learning task.\nOtherwise, we prefer using encoding methods such as |Gap|\nor |MinHash|.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}