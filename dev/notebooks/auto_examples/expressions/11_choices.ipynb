{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n.. currentmodule:: skrub\n\nTuning DataOps\n================\n\nA machine-learning pipeline typically contains some values or choices which\nmay influence its prediction performance, such as hyperparameters (e.g. the\nregularization parameter ``alpha`` of a ``RidgeClassifier``, the\n``learning_rate`` of a ``HistGradientBoostingClassifier``), which estimator to\nuse (e.g. ``RidgeClassifier`` or ``HistGradientBoostingClassifier``), or which\nsteps to include (e.g. should we join a table to bring additional information\nor not).\n\nWe want to tune those choices by trying several options and keeping those that\ngive the best performance on a validation set.\n\nSkrub `DataOps <skrub_pipeline>` provide a convenient way to specify\nthe range of possible values, by inserting it directly in place of the actual\nvalue. For example we can write:\n\n``RidgeClassifier(alpha=skrub.choose_from([0.1, 1.0, 10.0], name='\u03b1'))``\n\ninstead of:\n\n``RidgeClassifier(alpha=1.0)``.\n\nSkrub then inspects\nour DataOps plan to discover all the places where we used objects like\n``skrub.choose_from()`` and builds a grid of hyperparameters for us.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will illustrate hyperparameter tuning on the \"toxicity\" dataset. This\n dataset contains 1,000 texts and the task is to predict if they are\n flagged as being toxic or not.\n\nWe start from a very simple pipeline without any hyperparameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n\nimport skrub\nimport skrub.datasets\n\ndata = skrub.datasets.fetch_toxicity().toxicity\n\n# This dataset is sorted -- all toxic tweets appear first, so we shuffle it\ndata = data.sample(frac=1.0, random_state=1)\n\ntexts = data[[\"text\"]]\nlabels = data[\"is_toxic\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We mark the ``texts`` column as the input variable and the ``labels`` column as\nthe target variable.\nSee [the previous example](10_expressions.html) for a more detailed explanation\nof ``skrub.X`` and ``skrub.y``.\nWe then encode the text with a ``MinHashEncoder`` and fit a\n``HistGradientBoostingClassifier`` on the resulting features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = skrub.X(texts)\nX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y = skrub.y(labels)\ny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pred = X.skb.apply(skrub.MinHashEncoder()).skb.apply(\n    HistGradientBoostingClassifier(), y=y\n)\n\npred.skb.cross_validate(n_jobs=4)[\"test_score\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the sake of the example, we will focus on the number of ``MinHashEncoder``\ncomponents and the ``learning_rate`` of the ``HistGradientBoostingClassifier``\nto illustrate the ``skrub.choose_from(...)`` objects.\nWhen we use a scikit-learn hyperparameter-tuner like ``GridSearchCV`` or\n``RandomizedSearchCV``, we need to specify a grid of hyperparameters separately\nfrom the estimator, with something similar to\n``GridSearchCV(my_pipeline, param_grid={\"encoder__n_components: [5, 10, 20]\"})``.\nInstead, within a skrub DataOps plan we can use\n``skrub.choose_from(...)`` directly where the actual value\nwould normally go. Skrub then takes care of constructing the\n``GridSearchCV``'s parameter grid for us.\n\nSeveral utilities are available:\n\n- :func:`choose_from` to choose from a discrete set of values\n- :func:`choose_float` and :func:`choose_int` to sample numbers in a given range\n- :func:`choose_bool` to choose between ``True`` and ``False``\n- :func:`optional` to choose between something and ``None``; typically to make a\n  transformation step optional such as\n  ``X.skb.apply(skrub.optional(StandardScaler()))``\n\nChoices can be given an optional name which is used to display hyperparameter\nsearch results and plots, or to override their outcome (ADD A REFERENCE HERE).\n\nNote that :func:`skrub.choose_float()` and :func:`skrub.choose_int()` can be given a\n``log`` argument to sample in log scale, and that it is possible to specify the\nnumber of steps with the ``n_steps`` argument.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = skrub.X(texts), skrub.y(labels)\n\nencoder = skrub.MinHashEncoder(\n    n_components=skrub.choose_int(5, 15, n_steps=5, name=\"N components\")\n)\nclassifier = HistGradientBoostingClassifier(\n    learning_rate=skrub.choose_float(0.01, 0.9, log=True, name=\"lr\")\n)\npred = X.skb.apply(encoder).skb.apply(classifier, y=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From here, the ``pred`` DataOp can be used to perform hyperparameter search with\n``.skb.get_grid_search()`` or ``.skb.get_randomized_search()``. They accept\nthe same arguments as their scikit-learn counterparts (e.g. ``scoring``, ``cv``,\n``n_jobs``). Also, like ``.skb.get_pipeline()``, they accept a ``fitted``\nargument: if``fitted=True``, the search is fitted on the data we provided\nwhen initializing our pipeline's variables.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search = pred.skb.get_randomized_search(n_iter=8, n_jobs=4, random_state=1, fitted=True)\nsearch.results_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the plotly library is installed, we can visualize the results of the\nhyperparameter search with ``.plot_results()``.\nIn the plot below, each line represents a combination of hyperparameters (in\nthis case, only ``N components`` and ``learning rate``), and each column of\npoints represents either a hyperparameter, or the score of a given\ncombination of hyperparameters.\nThe color of the line represents the score of the combination of hyperparameters.\nThe plot is interactive, and it is  possible to select only a subset of the\nhyperparameters to visualize by dragging the mouse over each column to select\nthe desired range.\nThis is particularly useful when there are many combinations of hyperparameters,\nand we are interested in understanding which hyperparameters have the largest\nimpact on the score.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search.plot_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Default choice values\n\nThe goal of using the different ``choose_*`` functions is to tune choices on\nvalidation metrics with randomized or grid search. However, even when our\nexpression contains such choices we can still use it without tuning, for\nexample in previews or to get a quick first result before spending the\ncomputation time to run the search. When we use :meth:`.skb.get_pipeline()\n<Expr.skb.get_pipeline>`, we get a pipeline that does not perform any tuning\nand uses those default values. This default pipeline is used for\n:meth:`.skb.eval() <Expr.skb.eval>`.\n\nWe can control what should be the default value for each choice. For\n:func:`choose_int`, :func:`choose_float` and :func:`choose_bool`, we can use\nthe ``default`` parameter. For :func:`choose_from`, the default is the first\nitem from the list or dict of outcomes we provide. For :func:`optional`, we\ncan pass ``default=None`` to force the default to be the alternative\noutcome, ``None``.\n\nWhen we do not set an explicit default, skrub picks one for depending on the\nkind of choice, as detailed in `this table<choice-defaults-table>` in the\nUser Guide.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned we can control the default value:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "skrub.choose_float(1.0, 100.0, default=12.0).default()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Choices can appear in many places\n\nChoices are not limited to selecting estimator hyperparameters. They can also be\nused to choose between different estimators, or in place of any value used in\nour pipeline.\n\nFor example, here we pass a choice to pandas DataFrame's ``assign`` method.\nWe want to add a feature that captures the length of the text, but we are not\nsure if it is better to count length in characters or in words. We do not\nwant to add both because it would be redundant. We can add a column to the\ndataframe, which will be chosen among the length in characters or the length\nin words:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = skrub.X(texts), skrub.y(labels)\n\nX.assign(\n    length=skrub.choose_from(\n        {\"words\": X[\"text\"].str.count(r\"\\b\\w+\\b\"), \"chars\": X[\"text\"].str.len()},\n        name=\"length\",\n    )\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``choose_from`` can be given a dictionary if we want to provide\nnames for the individual outcomes, or a list, when names are not needed:\n``choose_from([1, 100], name='N')``,\n``choose_from({'small': 1, 'big': 100}, name='N')``.\n\nChoices can be nested arbitrarily. For example, here we want to choose\nbetween 2 possible encoder types: the ``MinHashEncoder`` or the\n``StringEncoder``. Each of the possible outcomes contains a choice itself:\nthe number of components.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = skrub.X(texts), skrub.y(labels)\n\nn_components = skrub.choose_int(5, 15, name=\"N components\")\n\nencoder = skrub.choose_from(\n    {\n        \"minhash\": skrub.MinHashEncoder(n_components=n_components),\n        \"lse\": skrub.StringEncoder(n_components=n_components),\n    },\n    name=\"encoder\",\n)\nX.skb.apply(encoder, cols=\"text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a similar vein, we might want to choose between a HGB classifier and a Ridge\nclassifier, each with its own set of hyperparameters.\nWe can then define a choice for the classifier and a choice for the\nhyperparameters of each classifier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n\nhgb = HistGradientBoostingClassifier(\n    learning_rate=skrub.choose_float(0.01, 0.9, log=True, name=\"lr\")\n)\nridge = RidgeClassifier(alpha=skrub.choose_float(0.01, 100, log=True, name=\"\u03b1\"))\nclassifier = skrub.choose_from({\"hgb\": hgb, \"ridge\": ridge}, name=\"classifier\")\npred = X.skb.apply(encoder).skb.apply(classifier, y=y)\nprint(pred.skb.describe_param_grid())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search = pred.skb.get_randomized_search(\n    n_iter=16, n_jobs=4, random_state=1, fitted=True\n)\nsearch.plot_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have a more complex plan, we can draw more conclusions from the\nparallel coordinate plot. For example, we can see that the\n``HistGradientBoostingClassifier``\nperforms better than the ``RidgeClassifier`` in most cases, that the ``StringEncoder``\noutperforms the ``MinHashEncoder``, and that the choice of the additional ``length``\nfeature does not have a significant impact on the score.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Concluding, we have seen how to use skrub's ``choose_from`` objects to tune\nhyperparameters, choose optional configurations, and nest choices. We then\nlooked at how the different choices affect the plan and the prediction\nscores.\n\nThere is more to say about skrub choices than what is covered in this\nexample. In particular, choices are not limited to choosing estimators and\ntheir hyperparameters: they can be used anywhere DataOps are used,\nsuch as the argument of a :func:`deferred` function, or the argument of\nother DataOps' method or operator. Finally, choices can be\ninter-dependent. Please find more information in the `user guide\n<skrub_pipeline_validation>`.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}