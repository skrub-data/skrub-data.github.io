{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Introduction to machine-learning pipelines with skrub DataOps\n\nIn this example, we show how we can use Skrub's `DataOps <userguide_data_ops>`\nto build a machine learning pipeline that records all the operations involved in\npre-processing data and training a model. We will also show how to save the model,\nload it back, and then use it to make predictions on new, unseen data.\n\nThis example is meant to be an introduction to Skrub DataOps, and as such it\nwill not cover all the features. Further examples in the gallery\n`data_ops_examples_ref` will go into more detail on how to use Skrub DataOps\nfor more complex tasks.\n\n.. currentmodule:: skrub\n\n.. |fetch_employee_salaries| replace:: :func:`datasets.fetch_employee_salaries`\n.. |TableReport| replace:: :class:`TableReport`\n.. |var| replace:: :func:`var`\n.. |skb.mark_as_X| replace:: :meth:`DataOp.skb.mark_as_X`\n.. |skb.mark_as_y| replace:: :meth:`DataOp.skb.mark_as_y`\n.. |TableVectorizer| replace:: :class:`TableVectorizer`\n.. |skb.apply| replace:: :meth:`.skb.apply() <DataOp.skb.apply>`\n.. |HistGradientBoostingRegressor| replace::\n   :class:`~sklearn.ensemble.HistGradientBoostingRegressor`\n.. |.skb.full_report()| replace:: :meth:`.skb.full_report() <DataOp.skb.full_report>`\n.. |choose_float| replace:: :func:`choose_float`\n.. |make_randomized_search| replace::\n   :meth:`.skb.make_randomized_search <DataOp.skb.make_randomized_search>`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The data\n\nWe begin by loading the employee salaries dataset, which is a regression dataset\nthat contains information about employees and their current annual salaries.\nBy default, the |fetch_employee_salaries| function returns the training set.\nWe will load the test set later, to evaluate our model on unseen data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub.datasets import fetch_employee_salaries\n\ntraining_data = fetch_employee_salaries(split=\"train\").employee_salaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can take a look at the dataset using the |TableReport|.\nThis dataset contains numerical, categorical, and datetime features. The column\n``current_annual_salary`` is the target variable we want to predict.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import skrub\n\nskrub.TableReport(training_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assembling our DataOps plan\n\nOur goal is to predict the ``current_annual_salary`` of employees based on their\nother features. We will use skrub's DataOps to combine both skrub and scikit-learn\nobjects into a single DataOps plan, which will allow us to preprocess the data,\ntrain a model, and tune hyperparameters.\n\nWe begin by defining a skrub |var|, which is the entry point for our DataOps plan.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data_var = skrub.var(\"data\", training_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define the initial features ``X`` and the target variable ``y``.\nWe use the |skb.mark_as_X| and |skb.mark_as_y| methods to mark these variables\nin the DataOps plan. This allows skrub to properly split these objects into\ntraining and validation steps when executing cross-validation or hyperparameter\ntuning.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = data_var.drop(\"current_annual_salary\", axis=1).skb.mark_as_X()\ny = data_var[\"current_annual_salary\"].skb.mark_as_y()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our first step is to vectorize the features in ``X``. We will use the\n|TableVectorizer| to convert the categorical and numerical features into a\nnumerical format that can be used by machine learning algorithms.\nWe apply the vectorizer to ``X`` using the |skb.apply| method, which allows us to\napply any scikit-learn compatible transformer to the skrub variable.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import TableVectorizer\n\nvectorizer = TableVectorizer()\n\nX_vec = X.skb.apply(vectorizer)\nX_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By clicking on ``Show graph``, we can see the DataOps plan that has been created:\nthe plan shows the steps that have been applied to the data so far.\nNow that we have the vectorized features, we can proceed to train a model.\nWe use a scikit-learn |HistGradientBoostingRegressor| to predict the target variable.\nWe apply the model to the vectorized features using ``.skb.apply``, and pass\n``y`` as the target variable.\nNote that the resulting ``predictor`` variable shows prediction results on the\npreview subsample, but the model will be properly fitted when we create the learner.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingRegressor\n\nhgb = HistGradientBoostingRegressor()\n\npredictor = X_vec.skb.apply(hgb, y=y)\npredictor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have built our entire plan, we can explore it in more detail\nwith the |.skb.full_report()| method::\n\n    predictions.skb.full_report()\n\nThis produces a folder on disk rather than displaying inline in a notebook so\nwe do not run it here. But you can\n[see the output here](../../_static/employee_salaries_report/index.html).\n\nThis method evaluates each step in\nthe plan and shows detailed information about the operations that are being performed.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Turning the DataOps plan into a learner, for later reuse\n\nNow that we have defined the predictor, we can create a ``learner``, a\nstandalone object that contains all the steps in the DataOps plan. We fit the\nlearner, so that it can be used to make predictions on new data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trained_learner = predictor.skb.make_learner(fitted=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A big advantage of the learner is that it can be pickled and saved to disk,\nallowing us to reuse the trained model later without needing to retrain it.\nThe learner contains all steps in the DataOps plan, including the fitted\nvectorizer and the trained model. We can save it using Python's ``pickle`` module.\nHere we use ``pickle.dumps`` to serialize the learner object into a byte string.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pickle\n\nsaved_model = pickle.dumps(trained_learner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now load the saved model back into memory using ``pickle.loads``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loaded_model = pickle.loads(saved_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we can make predictions on new data using the loaded model, by passing\na dictionary with the skrub variable names as keys.\nWe don't have to create a new variable, as this will be done internally by the\nlearner.\nIn fact, the ``learner`` is similar to a scikit-learn estimator, but rather\nthan taking ``X`` and ``y`` as inputs, it takes a dictionary (the \"environment\")\nwhere each key corresponds to the name of a skrub variable in the plan (in this\ncase, \"data\").\n\nWe can now get the test set of the employee salaries dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unseen_data = fetch_employee_salaries(split=\"test\").employee_salaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can use the loaded model to make predictions on the unseen data by\npassing a dictionary with the variable name as the key.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted_values = loaded_model.predict({\"data\": unseen_data})\npredicted_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also evaluate the model's performance using the `score` method, which\nuses the scikit-learn scoring function used by the predictor:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loaded_model.score({\"data\": unseen_data})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this example, we have briefly introduced the skrub DataOps and how they can\nbe used to build powerful machine learning pipelines. We have shown how to preprocess\ndata and train a model. We have also demonstrated how to save and load the trained\nmodel, and how to make predictions on new data.\n\nHowever, skrub DataOps are significantly more powerful than what we have shown here.\nFor more advanced examples, see `data_ops_examples_ref`.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}