{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n.. currentmodule:: skrub\n\n\n# Hyperparameter tuning with DataOps\n\nA machine-learning pipeline typically contains values or choices which\nmay influence its prediction performance, such as hyperparameters (e.g., the\nregularization parameter ``alpha`` of a :class:`~sklearn.linear_model.RidgeClassifier`,\nthe ``learning_rate`` of a :class:`~sklearn.ensemble.HistGradientBoostingClassifier`),\nwhich estimator to use (e.g., ``RidgeClassifier`` or\n``HistGradientBoostingClassifier``),\nor which steps to include (e.g., should we join a table to bring additional information\nor not).\n\nWe want to tune these choices by trying several options and keeping those that\ngive the best performance on a validation set.\n\nSkrub `DataOps <user_guide_data_ops_index>` provide a convenient way to specify\nthe range of possible values by inserting them directly in place of the actual\nvalue. For example, we can write:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n\nimport skrub\n\nRidgeClassifier(alpha=skrub.choose_from([0.1, 1.0, 10.0], name=\"\u03b1\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "instead of:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "RidgeClassifier(alpha=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Skrub then inspects our DataOps plan to discover all the places where we used objects\nlike :func:`~skrub.choose_from()` and builds a grid of hyperparameters for us.\n\nWe will illustrate hyperparameter tuning on the \"toxicity\" dataset. This\ndataset contains 1,000 texts and the task is to predict if they are\nflagged as being toxic or not.\n\nWe start from a very simple pipeline without any hyperparameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n\nimport skrub\nimport skrub.datasets\n\ndata = skrub.datasets.fetch_toxicity().toxicity\n\n# This dataset is sorted -- all toxic tweets appear first, so we shuffle it\ndata = data.sample(frac=1.0, random_state=1)\n\ntexts = data[[\"text\"]]\nlabels = data[\"is_toxic\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We mark the ``texts`` column as the input variable and the ``labels`` column as\nthe target variable.\n\nSee [the previous example](10_data_ops_intro.html)\nfor a more detailed explanation\nof :func:`skrub.X` and :func:`skrub.y`.\n\nWe then encode the text with a :class:`~skrub.MinHashEncoder` and fit a\n:class:`~sklearn.ensemble.HistGradientBoostingClassifier` on the resulting features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = skrub.X(texts)\nX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y = skrub.y(labels)\ny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pred = X.skb.apply(skrub.MinHashEncoder()).skb.apply(\n    HistGradientBoostingClassifier(), y=y\n)\npred.skb.cross_validate(n_jobs=4)[\"test_score\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we will focus on the ``n_components`` of the\n``MinHashEncoder`` and the ``learning_rate`` of the ``HistGradientBoostingClassifier``\nto illustrate the choices objects.\n\nWhen we use a scikit-learn hyperparameter-tuner like\n:class:`~sklearn.model_selection.GridSearchCV` or\n:class:`~sklearn.model_selection.RandomizedSearchCV`, we need to specify a grid of\nhyperparameters separately from the estimator, with something similar to\n``GridSearchCV(my_pipeline, param_grid={\"encoder__n_components: [5, 10, 20]\"})``.\n\nInstead, within a skrub DataOps plan we can use\n``skrub.choose_from(...)`` directly where the actual value\nwould normally go. Skrub then takes care of constructing the\n:class:`~sklearn.model_selection.GridSearchCV`'s parameter grid for us.\n\nNote that :func:`skrub.choose_float()` and :func:`skrub.choose_int()` can be given a\n``log`` argument to sample in log scale, and that it is possible to specify the\nnumber of steps with the ``n_steps`` argument.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = skrub.X(texts), skrub.y(labels)\n\nencoder = skrub.MinHashEncoder(\n    n_components=skrub.choose_int(5, 15, n_steps=5, name=\"N components\")\n)\nclassifier = HistGradientBoostingClassifier(\n    learning_rate=skrub.choose_float(0.01, 0.9, log=True, name=\"lr\")\n)\npred = X.skb.apply(encoder).skb.apply(classifier, y=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From here, the ``pred`` DataOp can be used to perform hyperparameter search with\n``.skb.make_grid_search()`` or ``.skb.make_randomized_search()``. They accept\nthe same arguments as their scikit-learn counterparts (e.g., ``scoring``, ``cv``,\n``n_jobs``). Also, like ``.skb.make_learner()``, they accept a ``fitted``\nargument: if ``fitted=True``, the search is fitted on the data we provided\nwhen initializing our pipeline's variables.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search = pred.skb.make_randomized_search(\n    n_iter=8, n_jobs=4, random_state=1, fitted=True\n)\nsearch.results_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the plotly library is installed, we can visualize the results of the\nhyperparameter search with :func:`~skrub.ParamSearch.plot_results`.\nIn the plot below, each line represents a combination of hyperparameters (in\nthis case, only ``N components`` and ``learning rate``), and each column of\npoints represents either a hyperparameter or the score of a given\ncombination of hyperparameters.\n\nThe color of the line represents the score of the combination of hyperparameters.\nThe plot is interactive, and you can select only a subset of the\nhyperparameters to visualize by dragging the mouse over each column to select\nthe desired range.\n\nThis is particularly useful when there are many combinations of hyperparameters,\nand we want to understand which hyperparameters have the largest\nimpact on the score.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search.plot_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can retrieve the best learner from the search results, and save it\nto disk. This learner will contain the best hyperparameter configuration\nfound during the search, and can be used to make predictions on new data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pickle\n\nbest_learner = search.best_learner_\nsaved_model = pickle.dumps(best_learner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Default choice values\n\nThe goal of using the different ``choose_*`` functions is to tune choices on\nvalidation metrics with randomized or grid search. However, even when our\nexpression contains such choices we can still use it without tuning, for\nexample in previews or to get a quick first result before spending the\ncomputation time to run the search. When we use :meth:`.skb.make_learner()\n<DataOp.skb.make_learner>`, we get a pipeline that does not perform any tuning\nand uses those default values. This default pipeline is used for\n:meth:`.skb.eval() <DataOp.skb.eval>`.\n\nWe can control what should be the default value for each choice. For\n:func:`choose_int`, :func:`choose_float` and :func:`choose_bool`, we can use\nthe ``default`` parameter. For :func:`choose_from`, the default is the first\nitem from the list or dict of outcomes we provide. For :func:`optional`, we\ncan pass ``default=None`` to force the default to be the alternative\noutcome, ``None``.\n\nWhen we do not set an explicit default, skrub picks one for depending on the\nkind of choice, as detailed in `this table<choice-defaults-table>` in the\nUser Guide.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned we can control the default value:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "skrub.choose_float(1.0, 100.0, default=12.0).default()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choices can appear in many places\n\nChoices are not limited to selecting estimator hyperparameters. They can also be\nused to choose between different estimators, or in place of any value used in\nour pipeline.\n\nFor example, here we pass a choice to pandas DataFrame's ``assign`` method.\nWe want to add a feature that captures the length of the text, but we are not\nsure if it is better to count length in characters or in words. We do not\nwant to add both because it would be redundant. We can add a column to the\ndataframe, which will be chosen among the length in characters or the length\nin words:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = skrub.X(texts), skrub.y(labels)\n\nX.assign(\n    length=skrub.choose_from(\n        {\"words\": X[\"text\"].str.count(r\"\\b\\w+\\b\"), \"chars\": X[\"text\"].str.len()},\n        name=\"length\",\n    )\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``choose_from`` can be given a dictionary if we want to provide\nnames for the individual outcomes, or a list, when names are not needed:\n``choose_from([1, 100], name='N')``,\n``choose_from({'small': 1, 'big': 100}, name='N')``.\n\nChoices can be nested arbitrarily. For example, here we want to choose\nbetween 2 possible encoder types: the ``MinHashEncoder`` or the\n``StringEncoder``. Each of the possible outcomes contains a choice itself:\nthe number of components.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = skrub.X(texts), skrub.y(labels)\n\nn_components = skrub.choose_int(5, 15, name=\"N components\")\n\nencoder = skrub.choose_from(\n    {\n        \"minhash\": skrub.MinHashEncoder(n_components=n_components),\n        \"lse\": skrub.StringEncoder(n_components=n_components),\n    },\n    name=\"encoder\",\n)\nX.skb.apply(encoder, cols=\"text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a similar vein, we might want to choose between a HGB classifier and a Ridge\nclassifier, each with its own set of hyperparameters.\nWe can then define a choice for the classifier and a choice for the\nhyperparameters of each classifier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n\nhgb = HistGradientBoostingClassifier(\n    learning_rate=skrub.choose_float(0.01, 0.9, log=True, name=\"lr\")\n)\nridge = RidgeClassifier(alpha=skrub.choose_float(0.01, 100, log=True, name=\"\u03b1\"))\nclassifier = skrub.choose_from({\"hgb\": hgb, \"ridge\": ridge}, name=\"classifier\")\npred = X.skb.apply(encoder).skb.apply(classifier, y=y)\nprint(pred.skb.describe_param_grid())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search = pred.skb.make_randomized_search(\n    n_iter=16, n_jobs=4, random_state=1, fitted=True\n)\nsearch.plot_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have a more complex plan, we can draw more conclusions from the\nparallel coordinate plot. For example, we can see that the\n``HistGradientBoostingClassifier``\nperforms better than the ``RidgeClassifier`` in most cases, that the ``StringEncoder``\noutperforms the ``MinHashEncoder``, and that the choice of the additional ``length``\nfeature does not have a significant impact on the score.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we've seen how to use skrub's ``choose_from`` objects to tune\nhyperparameters, choose optional configurations, and nest choices. We then\nexamined how different choices affect the plan and prediction scores.\n\nThere is more to learn about skrub choices than what is covered here.\nIn particular, choices are not limited to choosing estimators and\ntheir hyperparameters: they can be used anywhere DataOps are used,\nsuch as the argument of a :func:`deferred` function, or the argument of\nother DataOps' methods or operators. Additionally, choices can be\ninter-dependent. Find more information in the `user guide\n<user_guide_data_ops_tuning_validating_dataops>`.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}