{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Investigating and interpreting dirty categories\n\nWhat are dirty categorical variables and how can\na good encoding help with statistical learning?\n\nWe illustrate how categorical encodings obtained with\nthe |Gap| can be interpreted in terms of latent topics.\n\nWe use as example the [employee salaries](https://www.openml.org/d/42125)\ndataset.\n\n\n.. |Gap| replace::\n     :class:`~skrub.GapEncoder`\n\n.. |OneHotEncoder| replace::\n     :class:`~sklearn.preprocessing.OneHotEncoder`\n\n.. |SE| replace::\n     :class:`~skrub.SimilarityEncoder`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What do we mean by dirty categories?\n\nLet's look at the dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import datasets\n\nemployee_salaries = datasets.fetch_employee_salaries()\nprint(employee_salaries.description)\ndata = employee_salaries.X\nprint(data.head(n=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the number of unique entries per column:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(data.nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, some entries have many unique values:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(data[\"employee_position_title\"].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These different entries are often variations of the same entity.\nFor example, there are 3 kinds of \"Accountant/Auditor\".\n\nSuch variations will break traditional categorical encoding methods:\n\n* Using a simple |OneHotEncoder|\n  will create orthogonal features, whereas it is clear that\n  those 3 terms have a lot in common.\n\n* If we wanted to use word embedding methods such as\n  [Word2vec](https://www.tensorflow.org/tutorials/text/word2vec),\n  we would have to go through a cleaning phase: those algorithms\n  are not trained to work on data such as \"Accountant/Auditor I\".\n  However, this can be error-prone and time-consuming.\n\nThe problem becomes easier if we can capture relationships between\nentries.\n\nTo simplify understanding, we will focus on the column describing the\nemployee's position title:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "values = data[[\"employee_position_title\", \"gender\"]]\nvalues.insert(0, \"current_annual_salary\", employee_salaries.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## String similarity between entries\n\nThat's where our encoders get into play.\nIn order to robustly embed dirty semantic data, the |SE|\ncreates a similarity matrix based on an n-gram representation of the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sorted_values = values[\"employee_position_title\"].sort_values().unique()\n\nfrom skrub import SimilarityEncoder\n\nsimilarity_encoder = SimilarityEncoder()\ntransformed_values = similarity_encoder.fit_transform(sorted_values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting the new representation using multi-dimensional scaling\n\nLet's now plot a couple of points at random using a low-dimensional\nrepresentation to get an intuition of what the |SE| is doing:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import MDS\n\nmds = MDS(dissimilarity=\"precomputed\", n_init=10, random_state=42)\ntwo_dim_data = mds.fit_transform(1 - transformed_values)\n# transformed values lie in the 0-1 range,\n# so 1-transformed_value yields a positive dissimilarity matrix\nprint(two_dim_data.shape)\nprint(sorted_values.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first quickly fit a KNN so that the plots does not get too busy:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.neighbors import NearestNeighbors\n\nn_points = 5\nnp.random.seed(42)\n\nrandom_points = np.random.choice(\n    len(similarity_encoder.categories_[0]), n_points, replace=False\n)\nnn = NearestNeighbors(n_neighbors=2).fit(transformed_values)\n_, indices_ = nn.kneighbors(transformed_values[random_points])\nindices = np.unique(indices_.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we plot it, adding the categories in the scatter plot:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nf, ax = plt.subplots()\nax.scatter(x=two_dim_data[indices, 0], y=two_dim_data[indices, 1])\n# adding the legend\nfor x in indices:\n    ax.text(\n        x=two_dim_data[x, 0],\n        y=two_dim_data[x, 1],\n        s=sorted_values[x],\n        fontsize=8,\n    )\nax.set_title(\"multi-dimensional-scaling representation using a 3gram similarity matrix\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap of the similarity matrix\n\nWe can also plot the distance matrix for those observations:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f2, ax2 = plt.subplots(figsize=(6, 6))\ncax2 = ax2.matshow(transformed_values[indices, :][:, indices])\nax2.set_yticks(np.arange(len(indices)))\nax2.set_xticks(np.arange(len(indices)))\nax2.set_yticklabels(sorted_values[indices], rotation=30)\nax2.set_xticklabels(sorted_values[indices], rotation=60, ha=\"right\")\nax2.xaxis.tick_bottom()\nax2.set_title(\"Similarities across categories\")\nf2.colorbar(cax2)\nf2.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As shown in the previous plot, we see that the nearest neighbor of\n\"Communication Equipment Technician\"\nis \"Telecommunication Technician\", although it is also\nvery close to senior \"Supply Technician\": therefore, we grasp the\n\"Communication\" part (not initially present in the category as a unique word)\nas well as the \"Technician\" part of this category.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature interpretation with the |Gap|\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The |Gap| is a better encoder than the\n|SE| in the sense that it is more scalable and\ninterpretable, which we will present now.\n\nFirst, let's retrieve the dirty column to encode:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dirty_column = \"employee_position_title\"\nX_dirty = data[[dirty_column]]\nprint(X_dirty.head(), end=\"\\n\\n\")\nprint(f\"Number of dirty entries = {len(X_dirty)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n### Encoding dirty job titles\n\nThen, we'll create an instance of the |Gap| with 10 components:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import GapEncoder\n\nenc = GapEncoder(n_components=10, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we'll fit the model on the dirty categorical data and transform it\nin order to obtain encoded vectors of size 10:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_enc = enc.fit_transform(X_dirty)\nprint(f\"Shape of encoded vectors = {X_enc.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpreting encoded vectors\n\nThe |Gap| can be understood as a continuous encoding\non a set of latent topics estimated from the data. The latent topics\nare built by capturing combinations of substrings that frequently\nco-occur, and encoded vectors correspond to their activations.\nTo interpret these latent topics, we select for each of them a few labels\nfrom the input data with the highest activations.\nIn the example below we select 3 labels to summarize each topic.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "topic_labels = enc.get_feature_names_out(n_labels=3)\nfor k in range(len(topic_labels)):\n    labels = topic_labels[k]\n    print(f\"Topic n\u00b0{k}: {labels}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, topics capture labels that frequently co-occur. For instance,\nthe labels \"firefighter\", \"rescuer\", \"rescue\" appear together in\n\"Firefighter/Rescuer III\", or \"Fire/Rescue Lieutenant\".\n\nThis enables us to understand the encoding of different samples\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "encoded_labels = enc.transform(X_dirty[:20])\nplt.figure(figsize=(8, 10))\nplt.imshow(encoded_labels)\nplt.xlabel(\"Latent topics\", size=12)\nplt.xticks(range(0, 10), labels=topic_labels, rotation=50, ha=\"right\")\nplt.ylabel(\"Data entries\", size=12)\nplt.yticks(range(0, 20), labels=X_dirty[:20].to_numpy().flatten())\nplt.colorbar().set_label(label=\"Topic activations\", size=12)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, each dirty category encodes on a small number of topics,\nThese can thus be reliably used to summarize each topic, which are in\neffect latent categories captured from the data.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}