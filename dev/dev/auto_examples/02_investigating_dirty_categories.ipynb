{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Investigating dirty categories\n\nWhat are dirty categorical variables and how can a good encoding help\nwith statistical learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What do we mean by dirty categories?\n\nLet's look at a dataset called employee salaries:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dirty_cat import datasets\n\nemployee_salaries = datasets.fetch_employee_salaries()\nprint(employee_salaries.description)\ndata = employee_salaries.X\nprint(data.head(n=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is how many unique entries there is per column\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(data.nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, some entries have many unique values:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(data['employee_position_title'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These different entries are often variations on the same entities:\nfor example, there are 3 kinds of Accountant/Auditor.\n\nSuch variations will break traditional categorical encoding methods:\n\n* Using simple one-hot encoding will create orthogonal features,\n  whereas it is clear that those 3 terms have a lot in common.\n\n* If we wanted to use word embedding methods such as Word2vec,\n  we would have to go through a cleaning phase: those algorithms\n  are not trained to work on data such as 'Accountant/Auditor I'.\n  However, this can be error-prone and time-consuming.\n\nThe problem becomes easier if we can capture relationships between\nentries.\n\nTo simplify understanding, we will focus on the column describing the\nemployee's position title:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "values = data[['employee_position_title', 'gender']]\nvalues.insert(0, 'current_annual_salary', employee_salaries.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## String similarity between entries\n\nThat's where our encoders get into play.\nIn order to robustly embed dirty semantic data, the SimilarityEncoder\ncreates a similarity matrix based on the 3-gram structure of the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sorted_values = values['employee_position_title'].sort_values().unique()\n\nfrom dirty_cat import SimilarityEncoder\n\nsimilarity_encoder = SimilarityEncoder()\ntransformed_values = similarity_encoder.fit_transform(\n    sorted_values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting the new representation using multi-dimensional scaling\n\nLet's now plot a couple of points at random using a low-dimensional\nrepresentation to get an intuition of what the similarity encoder is doing:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import MDS\n\nmds = MDS(dissimilarity='precomputed', n_init=10, random_state=42)\ntwo_dim_data = mds.fit_transform(1 - transformed_values)\n# transformed values lie in the 0-1 range,\n# so 1-transformed_value yields a positive dissimilarity matrix\nprint(two_dim_data.shape)\nprint(sorted_values.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first quickly fit a KNN so that the plots does not get too busy:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.neighbors import NearestNeighbors\n\nn_points = 5\nnp.random.seed(42)\n\nrandom_points = np.random.choice(len(similarity_encoder.categories_[0]),\n                                 n_points, replace=False)\nnn = NearestNeighbors(n_neighbors=2).fit(transformed_values)\n_, indices_ = nn.kneighbors(transformed_values[random_points])\nindices = np.unique(indices_.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we plot it, adding the categories in the scatter plot:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nf, ax = plt.subplots()\nax.scatter(x=two_dim_data[indices, 0], y=two_dim_data[indices, 1])\n# adding the legend\nfor x in indices:\n    ax.text(x=two_dim_data[x, 0], y=two_dim_data[x, 1], s=sorted_values[x],\n            fontsize=8)\nax.set_title(\n    'multi-dimensional-scaling representation using a 3gram similarity matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap of the similarity matrix\n\nWe can also plot the distance matrix for those observations:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f2, ax2 = plt.subplots(figsize=(6, 6))\ncax2 = ax2.matshow(transformed_values[indices, :][:, indices])\nax2.set_yticks(np.arange(len(indices)))\nax2.set_xticks(np.arange(len(indices)))\nax2.set_yticklabels(sorted_values[indices], rotation='30')\nax2.set_xticklabels(sorted_values[indices], rotation='60', ha='right')\nax2.xaxis.tick_bottom()\nax2.set_title('Similarities across categories')\nf2.colorbar(cax2)\nf2.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As shown in the previous plot, we see that the nearest neighbor of\n\"Communication Equipment Technician\"\nis \"telecommunication technician\", although it is also\nvery close to senior \"supply technician\": therefore, we grasp the\n\"communication\" part (not initially present in the category as a unique word)\nas well as the technician part of this category.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding categorical data using SimilarityEncoder\n\nA typical data-science workflow uses one-hot encoding to represent\ncategories.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n\n# encoding simply a subset of the observations\nn_obs = 20\nemployee_position_titles = values['employee_position_title'].head(\n    n_obs).to_frame()\ncategorical_encoder = OneHotEncoder(sparse=False)\none_hot_encoded = categorical_encoder.fit_transform(employee_position_titles)\nf3, ax3 = plt.subplots(figsize=(6, 6))\nax3.matshow(one_hot_encoded)\nax3.set_title('Employee Position Title values, one-hot encoded')\nax3.axis('off')\nf3.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The corresponding is very sparse\n\nSimilarityEncoder can be used to replace one-hot encoding capturing the\nsimilarities:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f4, ax4 = plt.subplots(figsize=(6, 6))\nsimilarity_encoded = similarity_encoder.fit_transform(employee_position_titles)\nax4.matshow(similarity_encoded)\nax4.set_title('Employee Position Title values, similarity encoded')\nax4.axis('off')\nf4.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other examples in the dirty_cat documentation show how\nsimilarity encoding impacts prediction performance.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}