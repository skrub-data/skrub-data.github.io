{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Deduplicating misspelled categories with deduplicate\n\nReal world datasets often come with slight misspellings in the category\nnames, for instance if the category is manually input. Such misspellings\nbreak many data-analyses steps that require exact matching, such as a\n'GROUP BY'.\n\nMerging the multiple variants of the same category or entity is known as\n*deduplication*. It is performed by the |dd| function.\n\nDeduplication relies on *unsupervised learning*, to find structure in\ndata without providing explicit labels/categories of the data a-priori.\nSpecifically clustering of the distance between strings can be used to\nfind clusters of strings that are similar to each other (e.g. differ only\nby a misspelling) and hence gives us an easy tool to flag potentially\nmisspelled category names in an unsupervised manner.\n\n\n.. |dd| replace:: :func:`~skrub.deduplicate`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## An example dataset\n\nImagine the following example:\nAs a data scientist, our job is to analyze the data from a hospital ward.\nWe notice that most of the cases involve the prescription of one of three different\nmedications: \"Contrivan\", \"Genericon\", or \"Zipholan\".\nHowever, data entry is manual and - either because the prescribing doctor's\nhandwriting was hard to decipher, or due to mistakes during data input - there are\nmultiple spelling mistakes for these three medications.\n\nLet's generate some example data that demonstrate this.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom skrub.datasets import make_deduplication_data\n\n# our three medication names\nmedications = [\"Contrivan\", \"Genericon\", \"Zipholan\"]\nentries_per_medications = [500, 100, 1500]\n\n# 5% probability of a typo per letter\nprob_mistake_per_letter = 0.05\n\nduplicated_names = make_deduplication_data(\n    medications,\n    entries_per_medications,\n    prob_mistake_per_letter,\n    random_state=42,  # set seed for reproducibility\n)\n# we extract the unique medication names in the data & how often they appear\nunique_examples, counts = np.unique(duplicated_names, return_counts=True)\n# and build a series out of them\nimport pandas as pd\n\nex_series = pd.Series(counts, index=unique_examples)\n\n# This is our data:\nex_series.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nex_series.plot.barh(figsize=(10, 15))\nplt.xlabel(\"Medication name\")\nplt.ylabel(\"Counts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now see clearly the structure of the data: The three original medications\nare the most common ones, however there are many spelling mistakes and hence\nmany slight variations of the names of the original medications.\n\nThe idea is to use the fact that the string-distance of each misspelled medication\nname will be closest to either the correctly or incorrectly spelled orginal\nmedication name - and therefore form clusters.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## We can visualize the pair-wise distance between all medication names\n\nBelow we use a heatmap to visualize the pairwise-distance between medication names.\nA darker color means that two medication names are closer together (i.e. more similar),\na lighter color means a larger distance. We can see that we are dealing with three\nclusters - the original medication names and their misspellings that cluster around them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import compute_ngram_distance\nfrom scipy.spatial.distance import squareform\n\nngram_distances = compute_ngram_distance(unique_examples)\nsquare_distances = squareform(ngram_distances)\n\nimport seaborn as sns\n\nfig, axes = plt.subplots(1, 1, figsize=(12, 12))\nsns.heatmap(\n    square_distances, yticklabels=ex_series.index, xticklabels=ex_series.index, ax=axes\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## Deduplication: suggest corrections of misspelled names\n\nThe |dd| function uses clustering based on\nstring similarities to group duplicated names\n\nThe number of clusters will need some adjustment depending on the data you have.\nIf no fixed number of clusters is given, |dd| tries to set it automatically\nvia the [silhouette score](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import deduplicate\n\ndeduplicated_data = deduplicate(duplicated_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize the distribution of categories in the deduplicated data:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "deduplicated_unique_examples, deduplicated_counts = np.unique(\n    deduplicated_data, return_counts=True\n)\ndeduplicated_series = pd.Series(deduplicated_counts, index=deduplicated_unique_examples)\n\ndeduplicated_series.plot.barh(figsize=(10, 15))\nplt.xlabel(\"Medication name\")\nplt.ylabel(\"Counts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example we can correct all spelling mistakes by using the ideal number\nof clusters as determined by the silhouette score.\n\nHowever, often the translation/deduplication won't be perfect and will require some tweaks.\nIn this case, we can construct and update a translation table based on the data\nreturned by |dd|.\nIt consists of the (potentially) misspelled category names as indices and the\n(potentially) correct categories as values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create a table that maps original -> corrected categories\ntranslation_table = pd.Series(deduplicated_data, index=duplicated_names)\n\n# remove duplicates in the original data\ntranslation_table = translation_table[~translation_table.index.duplicated(keep=\"first\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the number of correct spellings will likely be much smaller than the\nnumber of original categories, we can print the estimated cluster and their\nmost common exemplars (the guessed correct spelling):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def print_corrections(spell_correct):\n    correct = np.unique(spell_correct.values)\n    for c in correct:\n        print(\n            f\"Guessed correct spelling: {c!r} for \"\n            f\"{spell_correct[spell_correct==c].index.values}\"\n        )\n\n\nprint_corrections(translation_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In case we want to adapt the translation table post-hoc we can easily\nmodified it manually and apply it, for instance modifying the\ncorrespondance for the last entry as such:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "translation_table.iloc[-1] = \"Completely new category\"\nnew_deduplicated_names = translation_table[duplicated_names]\nassert (new_deduplicated_names == \"Completely new category\").sum() > 0"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}