{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Multiples tables: building machine learning pipelines with DataOps\n\nIn this example, we show how to build a DataOps plan to handle\npre-processing, validation and hyperparameter tuning of a dataset with **multiple\ntables**.\n\nWe consider the credit fraud dataset, which contains two tables: one for\nbaskets (orders) and one for products. The goal is to predict whether a basket\n(a single order that has been placed with the website) is fraudulent or not,\nbased on the products it contains.\n\n.. currentmodule:: skrub\n\n.. |choose_from| replace:: :func:`skrub.choose_from`\n.. |choose_int| replace:: :func:`skrub.choose_int`\n.. |choose_float| replace:: :func:`skrub.choose_float`\n.. |MinHashEncoder| replace:: :class:`~skrub.MinHashEncoder`\n.. |StringEncoder| replace:: :class:`~skrub.StringEncoder`\n.. |TableVectorizer| replace:: :class:`~skrub.TableVectorizer`\n.. |var| replace:: :func:`skrub.var`\n.. |TableReport| replace:: :class:`~skrub.TableReport`\n.. |HistGradientBoostingClassifier| replace::\n   :class:`~sklearn.ensemble.HistGradientBoostingClassifier`\n.. |make_randomized_search| replace:: :func:`~skrub.DataOp.skb.make_randomized_search`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The credit fraud dataset\n\nThe ``baskets`` table contains a basket ID and a flag indicating if the order\nwas fraudulent or not (the customer never made the payment).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import skrub\nimport skrub.datasets\n\ndataset = skrub.datasets.fetch_credit_fraud()\nskrub.TableReport(dataset.baskets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``products`` table contains information about the products that have been\npurchased, and the basket they belong to. A basket contains at least one product.\nProducts can be associated with the corresponding basket through the \"basket_ID\"\ncolumn.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "skrub.TableReport(dataset.products)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A data-processing challenge\nThe general structure of the DataOps plan we want to build looks like this:\n\n<img src=\"file://../../_static/credit_fraud_diagram.svg\" width=\"300\">\n\nWe want to fit a |HistGradientBoostingClassifier| to predict the fraud\nflag (y). However, since the features for each basket are stored in\nthe products table, we need to extract these features, aggregate them\nat the basket level, and merge the result with the basket data.\n\nWe can use the |TableVectorizer| to vectorize the products, but we\nthen need to aggregate the resulting vectors to obtain a single row per basket.\nUsing a scikit-learn Pipeline is tricky because the |TableVectorizer| would be\nfitted on a table with a different number of rows than the target y (the baskets\ntable), which scikit-learn does not allow.\n\nWhile we could fit the |TableVectorizer| manually, this would forfeit\nscikit-learn\u2019s tooling for managing transformations, storing fitted estimators,\nsplitting data, cross-validation, and hyper-parameter tuning.\nWe would also have to handle the aggregation and join ourselves, likely with\nerror-prone Pandas code.\n\nFortunately, skrub DataOps provide a powerful alternative for building flexible\nplans that address these problems.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a multi-table DataOps plan\n\nWe start by creating skrub variables, which are the inputs to our plan.\nIn our example, we create two skrub |var| objects: ``products`` and ``baskets``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "products = skrub.var(\"products\", dataset.products)\nbaskets = skrub.var(\"baskets\", dataset.baskets)\n\nbasket_ids = baskets[[\"ID\"]].skb.mark_as_X()\nfraud_flags = baskets[\"fraud_flag\"].skb.mark_as_y()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We mark the ``basket_ids`` variable as ``X`` and the ``fraud_flags`` variable as ``y``\nso that DataOps can use their indices for train-test splitting and cross-validation.\nWe then build the plan by applying transformations to those inputs.\n\nSince our DataOps expect dataframes for products, baskets and fraud\nflags, we manipulate those objects as we would manipulate pandas dataframes.\nFor instance, we filter products to keep only those that match one of the\nbaskets in the ``baskets`` table, and then add a column containing the total\namount for each kind of product in a basket:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kept_products = products[products[\"basket_ID\"].isin(basket_ids[\"ID\"])]\nproducts_with_total = kept_products.assign(\n    total_price=kept_products[\"Nbr_of_prod_purchas\"] * kept_products[\"cash_price\"]\n)\nproducts_with_total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then build a skrub ``TableVectorizer`` with different choices of\nthe type of encoder for high-cardinality categorical or string columns, and\nthe number of components it uses.\n\nWith skrub, there\u2019s no need to specify a separate grid of hyperparameters outside\nthe pipeline.\nInstead, within a DataOps plan, we can directly replace a parameter\u2019s value using\none of skrub\u2019s ``choose_*`` functions, which define the range of values to consider\nduring hyperparameter selection. In this example, we use |choose_int| to select\nthe number of components for the encoder and |choose_from| to select the type\nof encoder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n = skrub.choose_int(5, 15, name=\"n_components\")\nencoder = skrub.choose_from(\n    {\n        \"MinHash\": skrub.MinHashEncoder(n_components=n),\n        \"LSA\": skrub.StringEncoder(n_components=n),\n    },\n    name=\"encoder\",\n)\nvectorizer = skrub.TableVectorizer(high_cardinality=encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can restrict the vectorizer to a subset of columns: in our case, we want to\nvectorize all columns except the ``\"basket_ID\"`` column, which is not a\nfeature but a link to the basket it belongs to.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vectorized_products = products_with_total.skb.apply(\n    vectorizer, exclude_cols=\"basket_ID\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then aggregate the vectorized products by basket ID, and then merge the result\nwith the baskets table.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "aggregated_products = vectorized_products.groupby(\"basket_ID\").agg(\"mean\").reset_index()\naugmented_baskets = basket_ids.merge(\n    aggregated_products, left_on=\"ID\", right_on=\"basket_ID\"\n).drop(columns=[\"ID\", \"basket_ID\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we add a supervised estimator, and use |choose_float| to\nadd the learning rate as a hyperparameter to tune.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n\nhgb = HistGradientBoostingClassifier(\n    learning_rate=skrub.choose_float(0.01, 0.9, log=True, name=\"learning_rate\")\n)\npredictions = augmented_baskets.skb.apply(hgb, y=fraud_flags)\npredictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And our DataOps plan is complete!\n\nWe can now use |make_randomized_search| to perform hyperparameter\ntuning and find the best hyperparameters for our model. Below, we display the\nhyperparameter combinations that define our search space.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(predictions.skb.describe_param_grid())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "|make_randomized_search| returns a :class:`~skrub.ParamSearch` object, which contains\nour search result and some plotting logic.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search = predictions.skb.make_randomized_search(\n    scoring=\"roc_auc\", n_iter=8, n_jobs=4, random_state=0, fitted=True\n)\nsearch.results_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also display the results of the search in a parallel coordinates plot:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search.plot_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems here that using the LSA as an encoder brings better test scores,\nbut at the expense of training and scoring time.\n\nWe can get the best performing :class:`~skrub.SkrubLearner` via\n``best_learner_``, and use it for inference on new data with:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\nnew_baskets = pd.DataFrame([dict(ID=\"abc\")])\nnew_products = pd.DataFrame(\n    [\n        dict(\n            basket_ID=\"abc\",\n            item=\"COMPUTER\",\n            cash_price=200,\n            make=\"APPLE\",\n            model=\"XXX-X\",\n            goods_code=\"239246782\",\n            Nbr_of_prod_purchas=1,\n        )\n    ]\n)\nsearch.best_learner_.predict_proba({\"baskets\": new_baskets, \"products\": new_products})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this example, we have shown how to build a multi-table machine learning\npipeline with skrub DataOps. We have seen how DataOps allow us to use familiar\nPandas operations to manipulate dataframes, and how we can build a DataOps plan\nthat works with multiple tables and performs hyperparameter tuning on the\nresulting pipeline.\n\nIf you want to learn more about tuning hyperparameters using skrub DataOps, see\nthe `Tuning Pipelines example <example_tuning_pipelines>` for an\nin-depth tutorial.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}