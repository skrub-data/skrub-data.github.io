{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Building complex tabular pipelines\n\nSkrub provides an easy way to build complex, flexible machine-learning\npipelines. It solves several problems that are not easily addressed with the\nstandard scikit-learn tools such as the ``Pipeline`` and ``ColumnTransformer``.\n\n**Multiple tables:** we have several tables of different shapes (for example,\nwe may have \"Customers\", \"Orders\" and \"Products\" tables). But scikit-learn\nestimators expect a single design matrix ``X`` and array of targets ``y`` with one row\nper observation.\n\n**DataFrame wrangling:** we need to easily perform typical dataframe operations\nsuch as projections, joins and aggregations leveraging the powerful APIs of\n``pandas`` or ``polars``.\n\n**Iterative development:** we want to build a pipeline step by step, while\ninspecting the intermediate results so that the feedback loop is short and\nerrors are discovered early.\n\n**Hyperparameter tuning:** many choices such as estimators, hyperparameters,\neven the architecture of the pipeline can be informed by validation scores.\nSpecifying the grid of hyperparameters separately from the model (as in\n``GridSearchCV``) is very difficult for complex pipelines.\n\nSkrub can help us tackle these challenges. In this example, we show a pipeline\nto handle a dataset with 2 tables. Despite being very simple, this pipeline\nwould be difficult to implement, validate and deploy correctly without skrub.\nWe show the script with minimal comments to motivate the tools that are\nexplained in detail in subsequent examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The credit fraud dataset\n\nThis dataset comes from an e-commerce website. We have a set of \"baskets\",\norders that have been placed with the website. The task is to detect which of\nthose orders were fraudulent (the customer never made the payment).\n\nThe ``baskets`` table only contains a basket ID and the flag indicating if it\nwas fraudulent or not.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import skrub\nimport skrub.datasets\n\ndataset = skrub.datasets.fetch_credit_fraud()\nskrub.TableReport(dataset.baskets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each basket contains one or more products. Each row in the ``products`` table\ncorresponds to a type of product that was present in a basket. Products can\nbe associated with the corresponding basket through the ``\"basket_ID\"``\ncolumn.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "skrub.TableReport(dataset.products)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A data-processing challenge\n\nWe want to fit a ``HistGradientBoostingClassifier`` to predict the fraud\nflag. We need to build a design matrix with one row per basket (and thus per\nfraud flag). Our ``baskets`` table only contains IDs. We need to enrich it by\nadding features constructed from the ``products`` table.\n\nAs the ``products`` table contains strings and categories (such as\n``\"SAMSUNG\"``), we need to vectorize those entries to extract numeric\nfeatures. This is easily done with skrub's ``TableVectorizer``. As each\nbasket can contain several products, all the product lines corresponding to a\nbasket then need to be aggregated into a single feature vector that can be\nattached to the basket.\n\nThus the general structure of the pipeline looks like this:\n\n<img src=\"file://../../_static/credit_fraud_diagram.svg\" width=\"300\">\n\nThe difficulty is that the products need to be aggregated before joining\nto ``baskets``, and in order to compute a meaningful aggregation, they must\nbe vectorized *before* the aggregation. So we have a ``TableVectorizer`` to\nfit on a table which does not (yet) have the same number of rows as the\ntarget ``y`` \u2014 something that the scikit-learn ``Pipeline``, with its\nsingle-input, linear structure, does not accommodate.\n\nWe can fit it ourselves, outside of any pipeline with something like::\n\n    vectorizer = skrub.TableVectorizer()\n    vectorized_products = vectorizer.fit_transform(products)\n\nHowever, because it is dissociated from the main estimator which handles\n``X`` (the baskets), we have to manage this transformer ourselves. We lose\nthe usual scikit-learn machinery for grouping all transformation steps,\nstoring fitted estimators, splitting the input data and cross-validation, and\nhyper-parameter tuning.\n\nMoreover, we later need some pandas code to perform the aggregation and join.\nAgain, as this transformation is not in a scikit-learn estimator, we have to\nkeep track of it ourselves so that we can later apply it to unseen data,\nwhich is error-prone, and we cannot tune any choices (like the choice of the\naggregation function).\n\nTo cope with these difficulties, skrub provides an alternative way to build\nmore flexible pipelines.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A solution with skrub\n\nHere we do not explain all the details, in-depth explanations are left for\nthe next example.\n\nWhen building a skrub pipeline we do not provide an explicit list of\ntransformation steps. Rather, we manipulate skrub objects that represent\nintermediate results, and the pipeline is built implicitly as we perform\noperations (such as applying operators or calling functions) on those\nobjects.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by creating skrub \"variables\", which are given\na name and represent the inputs to our pipeline \u2014 here, the products,\nbaskets, and fraud flags. They are given a name and an (optional) initial\nvalue, which is used to show previews of the pipeline's output, detect errors\nearly, and provide data for cross-validation and hyperparameter search.\n\nWe then build the pipeline by applying transformations to those inputs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "products = skrub.var(\"products\", dataset.products)\nbaskets = skrub.var(\"baskets\", dataset.baskets[[\"ID\"]]).skb.mark_as_X()\nfraud_flags = skrub.var(\"fraud_flags\", dataset.baskets[\"fraud_flag\"]).skb.mark_as_y()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Above, ``mark_as_X()`` and ``mark_as_y()`` tell skrub that the baskets and\nflags are respectively our design matrix and targets, ie the tables that\nshould be split into training and testing sets for cross-validation. Here\nthey are direct inputs to the pipeline but they don't have to be \u2014 any\nintermediate result could be marked as X or y.\n\nBecause our pipeline expects DataFrames for the products, baskets and fraud\nflags, we manipulate those objects just like we would manipulate DataFrames.\nAll attribute accesses will be transparently forwarded to the actual input\nDataFrames when we run the pipeline.\n\nFor example let us filter products to keep only those that match one of the\nbaskets in the ``\"baskets\"`` table, then add a column containing the total\namount for each kind of product in a basket:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "products = products[products[\"basket_ID\"].isin(baskets[\"ID\"])]\nproducts = products.assign(\n    total_price=products[\"Nbr_of_prod_purchas\"] * products[\"cash_price\"]\n)\nproducts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note we are getting previews of the output of intermediate results. For\nexample we can see the added ``\"total_price\"`` column in the output above.\nThe dropdown at the top allows us to check the structure of the pipeline and\nall the steps it contains.\n\nWith skrub we do not need to specify a grid of hyperparameters separately\nfrom the pipeline. Instead, we can replace a parameter's value with a skrub\n\"choice\" which indicates the range of values we would like to consider during\nhyperparameter selection.\n\nThose choices can be nested arbitrarily. They are not restricted to\nparameters of a scikit-learn estimator, but they can be anything: choosing\nbetween different estimators, arguments to function calls, whole sections of\nthe pipeline etc.\n\nIn-depth information about choices and hyperparameter/model selection is\nprovided in example (TODO add link).\n\nHere we build a skrub ``TableVectorizer`` that contains a couple of choices:\nthe type of encoder for high-cardinality categorical or string columns, and\nthe number of components it uses.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n = skrub.choose_int(5, 15, log=True, name=\"n_components\")\nencoder = skrub.choose_from(\n    {\n        \"MinHash\": skrub.MinHashEncoder(n_components=n),\n        \"LSA\": skrub.StringEncoder(n_components=n),\n    },\n    name=\"encoder\",\n)\nvectorizer = skrub.TableVectorizer(high_cardinality=encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A transformer does not have to apply to the full DataFrame; we can easily\nrestrict it to some columns, using the ``cols`` or ``exclude_cols``\nparameters. ``cols`` can be a simple list of column names but also a Skrub\nselector TODO link. Here we vectorize all columns except the ``\"basket_ID\"``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vectorized_products = products.skb.apply(vectorizer, exclude_cols=\"basket_ID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Having access to the underlying dataframe's API, we can perform the\ndata-wrangling we need. All those transformations are being implicitly added\nas steps in our machine-learning pipeline.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "aggregated_products = vectorized_products.groupby(\"basket_ID\").agg(\"mean\").reset_index()\naugmented_baskets = baskets.merge(\n    aggregated_products, left_on=\"ID\", right_on=\"basket_ID\"\n).drop(columns=[\"ID\", \"basket_ID\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we add a supervised estimator and our pipeline is complete.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n\nhgb = HistGradientBoostingClassifier(\n    learning_rate=skrub.choose_float(0.01, 0.9, log=True, name=\"learning_rate\")\n)\npredictions = augmented_baskets.skb.apply(hgb, y=fraud_flags)\npredictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can ask for a full report of the pipeline and inspect the results at every\nstep::\n\n    predictions.skb.full_report()\n\nThis produces a folder on disk rather than displaying inline in a notebook so\nwe do not run it here. But you can\n[see the output](../../_static/credit_fraud_report/index.html).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the choices we inserted at different locations in our pipeline, skrub\ncan build a grid of hyperparameters and run the hyperparameter search for us,\nbacked by scikit-learn's ``GridSearchCV`` or ``RandomizedSearchCV``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(predictions.skb.describe_param_grid())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search = predictions.skb.get_randomized_search(\n    scoring=\"roc_auc\", n_iter=8, n_jobs=4, random_state=0, fitted=True\n)\nsearch.results_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also ask skrub to display a parallel coordinates plot of the results.\nIn this plot, each line corresponds to a combination of hyperparameter\n(choice) values. It goes through the corresponding test score, and training\nand scoring computation durations. The other columns show the hyperparameter\nvalues. By clicking and dragging the mouse on any column, we can restrict the\nset of lines we see. This allows quickly inspecting which hyperparameters are\nmost important, which values perform best, and trade-offs between the quality\nof predictions and computation time.\n\nTODO: Gif of how to use the plot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search.plot_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIf after reading this example you are curious to know more and learn how to\nbuild your own complex, multi-table pipelines with easy hyperparameter\ntuning, please see the next examples for an in-depth tutorial.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}