{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Getting Started\n\nThis guide showcases some of the features of skrub.\nMuch of skrub revolves around simplifying many of the tasks that are involved\nin pre-processing raw data into a format that shallow or classic machine-learning\nmodels can understand, that is, numerical data.\n\nSkrub achieves this by vectorizing, assembling, and encoding tabular data through\nthe features we present in this example and the following ones.\n\n.. |TableReport| replace:: :class:`~skrub.TableReport`\n.. |Cleaner| replace:: :class:`~skrub.Cleaner`\n.. |set_config| replace:: :func:`~skrub.set_config`\n.. |tabular_pipeline| replace:: :func:`~skrub.tabular_pipeline`\n.. |TableVectorizer| replace:: :class:`~skrub.TableVectorizer`\n.. |Joiner| replace:: :class:`~skrub.Joiner`\n.. |SquashingScaler| replace:: :class:`~skrub.SquashingScaler`\n.. |DatetimeEncoder| replace:: :class:`~skrub.DatetimeEncoder`\n.. |ApplyToCols| replace:: :class:`~skrub.ApplyToCols`\n.. |StringEncoder| replace:: :class:`~skrub.StringEncoder`\n.. |TextEncoder| replace:: :class:`~skrub.TextEncoder`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preliminary exploration with the |TableReport|\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub.datasets import fetch_employee_salaries\n\ndataset = fetch_employee_salaries()\nemployees_df, salaries = dataset.X, dataset.y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Typically, the first step with new data is exploration and parsing.\nTo quickly get an overview of a dataframe's contents, use the |TableReport|.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import TableReport\n\nTableReport(employees_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use the interactive display above to explore the dataset visually.\n\nIt is also possible to tell skrub to replace the default pandas and polars\ndisplays with |TableReport| by modifying the global config with\n|set_config|.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>You can see a few more `example reports`_ online. We also\n   provide an experimental online demo_ that allows you to select a CSV or\n   parquet file and generate a report directly in your web browser, without\n   installing anything.\n\n\nFrom the report above, we see that there are columns with date and time stored\nas `object` dtype (cf. \"Stats\" tab of the report).\nDatatypes not being parsed correctly is a scenario that occurs commonly after\nreading a table. We can use the |Cleaner| to address this.\nIn the next section, we show that this transformer does additional cleaning.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sanitizing data with the |Cleaner|\nHere, we use the |Cleaner|, a transformer that sanitizing the\ndataframe by parsing nulls and dates, and by dropping \"uninformative\" columns\n(e.g., columns with too many nulls or that are constant).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import Cleaner\n\nemployees_df = Cleaner().fit_transform(employees_df)\nTableReport(employees_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see from the \"Stats\" tab that now the column `date_first_hired` has been\nparsed correctly as a Datetime.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Easily building a strong baseline for tabular machine learning\n\nThe goal of skrub is to ease tabular data preparation for machine learning.\nThe |tabular_pipeline| function provides an easy way to build a simple\nbut reliable machine learning model that works well on most tabular data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n\nfrom skrub import tabular_pipeline\n\nmodel = tabular_pipeline(\"regressor\")\nmodel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "results = cross_validate(model, employees_df, salaries)\nresults[\"test_score\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To handle rich tabular data and feed it to a machine learning model, the\npipeline returned by |tabular_pipeline| preprocesses and encodes\nstrings, categories and dates using the |TableVectorizer|.\nSee its documentation or `sphx_glr_auto_examples_0010_encodings.py` for\nmore details. An overview of the chosen defaults is available in\n`user_guide_tabular_pipeline`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding any data as numerical features\n\nTabular data can contain a variety of datatypes, from numerical to\ndatetimes, categories, strings, and text. Encoding features in a meaningful\nway requires significant effort and is a major part of the feature engineering\nprocess required to properly train machine learning models.\n\nSkrub helps with this by providing various transformers that automatically\nencode different datatypes into ``float32`` features.\n\nFor **numerical features**, the |SquashingScaler| applies a robust\nscaling technique that is less sensitive to outliers. Check the\n`relative example <sphx_glr_auto_examples_0100_squashing_scaler.py>`\nfor more information on the feature.\n\nFor **datetime columns**, skrub provides the |DatetimeEncoder|\nwhich can extract useful features such as year, month, day, as well as additional\nfeatures such as weekday or day of year. Periodic encoding with trigonometric\nor spline features is also available. Refer to the |DatetimeEncoder|\ndocumentation for more detail.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\ndata = pd.DataFrame(\n    {\n        \"event\": [\"A\", \"B\", \"C\"],\n        \"date_1\": [\"2020-01-01\", \"2020-06-15\", \"2021-03-22\"],\n        \"date_2\": [\"2020-01-15\", \"2020-07-01\", \"2021-04-05\"],\n    }\n)\ndata = Cleaner().fit_transform(data)\nTableReport(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Skrub transformers are applied column-by-column, but it's possible to use\nthe |ApplyToCols| meta-transformer to apply a transformer to\nmultiple columns at once. Complex column selection is possible using\n`skrub's column selectors <user_guide_selectors>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import ApplyToCols, DatetimeEncoder\n\nApplyToCols(\n    DatetimeEncoder(add_total_seconds=False), cols=[\"date_1\", \"date_2\"]\n).fit_transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, when a column contains **categorical or string data**, it can be\nencoded using various encoders provided by skrub. The default encoder is\nthe |StringEncoder|, which encodes categories using\n[Latent Semantic Analysis (LSA)](https://scikit-learn.org/stable/modules/decomposition.html#about-truncated-svd-and-latent-semantic-analysis-(lsa)).\nIt is a simple and efficient way to encode categories and works well in\npractice.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(\n    {\n        \"city\": [\"Paris\", \"London\", \"Berlin\", \"Madrid\", \"Rome\"],\n        \"country\": [\"France\", \"UK\", \"Germany\", \"Spain\", \"Italy\"],\n    }\n)\nTableReport(data)\nfrom skrub import StringEncoder\n\nStringEncoder(n_components=3).fit_transform(data[\"city\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your data includes a lot of text, you may want to use the\n|TextEncoder|,\nwhich uses pre-trained language models retrieved from the HuggingFace hub to\ncreate meaningful text embeddings.\nSee `user_guide_encoders_index` for more details on all the categorical encoders\nprovided by skrub, and `sphx_glr_auto_examples_0010_encodings.py` for a\ncomparison between the different methods.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assembling data\n\nSkrub allows imperfect assembly of data, such as joining dataframes\non columns that contain typos. Skrub's joiners have ``fit`` and\n``transform`` methods, storing information about the data across calls.\n\nThe |Joiner| allows fuzzy-joining multiple tables, where each row of\na main table will be augmented with values from the best match in the auxiliary table.\nYou can control how distant fuzzy-matches are allowed to be with the\n``max_dist`` parameter.\n\nSkrub also allows you to aggregate multiple tables according to various strategies.\nYou can see other ways to join multiple tables in\n`user_guide_joining_dataframes`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced use cases\nIf your use case involves more complex data preparation, hyperparameter tuning,\nor model selection, if you want to build a multi-table pipeline that requires\nassembling and preparing multiple tables, or if you want to ensure that the\ndata preparation can be reproduced exactly, you can use the skrub Data Ops,\na powerful framework that provides tools to build complex data processing pipelines.\nSee the related `user guide <user_guide_data_ops_index>` and the\n`data_ops_examples_ref`\nexamples for more details.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n\nWe have briefly covered pipeline creation, vectorizing, assembling, and encoding\ndata. We presented the main functionalities of skrub, but there is much\nmore to explore!\n\nPlease refer to our `user_guide` for a more in-depth presentation of\nskrub's concepts, or visit our\n[examples](https://skrub-data.org/stable/auto_examples) for more\nillustrations of the tools that we provide!\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}