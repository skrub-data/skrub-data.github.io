{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Dirty categories: machine learning with non normalized strings\n\nIncluding strings that represent categories often calls for much data\npreparation. In particular categories may appear with many morphological\nvariants, when they have been manually input or assembled from diverse\nsources.\n\nHere we look at a dataset on wages [#]_ where the column *Employee\nPosition Title* contains dirty categories. On such a column, standard\ncategorical encodings leads to very high dimensions and can lose\ninformation on which categories are similar.\n\nWe investigate various encodings of this dirty column for the machine\nlearning workflow, predicting the *current annual salary* with gradient\nboosted trees. First we manually assemble a complex encoder for the full\ndataframe, after which we show a much simpler way, albeit with less fine\ncontrol.\n\n\n.. [#] https://www.openml.org/d/42125\n\n\n .. |SV| replace::\n     :class:`~dirty_cat.SuperVectorizer`\n\n .. |Pipeline| replace::\n     :class:`~sklearn.pipeline.Pipeline`\n\n .. |OneHotEncoder| replace::\n     :class:`~sklearn.preprocessing.OneHotEncoder`\n\n .. |ColumnTransformer| replace::\n     :class:`~sklearn.compose.ColumnTransformer`\n\n .. |RandomForestRegressor| replace::\n     :class:`~sklearn.ensemble.RandomForestRegressor`\n\n .. |Gap| replace::\n     :class:`~dirty_cat.GapEncoder`\n\n .. |SE| replace:: :class:`~dirty_cat.SimilarityEncoder`\n\n .. |permutation importances| replace::\n     :func:`~sklearn.inspection.permutation_importance`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The data\n\nWe first retrieve the dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dirty_cat.datasets import fetch_employee_salaries\n\nemployee_salaries = fetch_employee_salaries()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "X, the input data (descriptions of employees):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = employee_salaries.X\nX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and y, our target column (the annual salary)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y = employee_salaries.y\ny.name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's carry out some basic preprocessing:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\nX['date_first_hired'] = pd.to_datetime(X['date_first_hired'])\nX['year_first_hired'] = X['date_first_hired'].apply(lambda x: x.year)\n# Get a mask of the rows with missing values in \"gender\"\nmask = X.isna()['gender']\n# And remove them\nX.dropna(subset=['gender'], inplace=True)\ny = y[~mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assembling a machine-learning pipeline that encodes the data\n\n### The learning pipeline\n\nTo build a learning pipeline, we need to assemble encoders for each\ncolumn, and apply a supervised learning model on top.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### The categorical encoders\n\nAn encoder is needed to turn a categorical column into a numerical\nrepresentation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n\none_hot = OneHotEncoder(handle_unknown='ignore', sparse=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We assemble these to apply them to the relevant columns.\nThe ColumnTransformer is created by specifying a set of transformers\nalongside with the column names on which each must be applied\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import make_column_transformer\n\nencoder = make_column_transformer(\n    (one_hot, ['gender', 'department_name', 'assignment_category']),\n    ('passthrough', ['year_first_hired']),\n    # Last but not least, our dirty column\n    (one_hot, ['employee_position_title']),\n    remainder='drop',\n   )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Pipelining an encoder with a learner\n\nWe will use a HistGradientBoostingRegressor, which is a good predictor\nfor data with heterogeneous columns\n(we need to require the experimental feature for scikit-learn versions\nearlier than 1.0)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import sklearn\nfrom sklearn.utils.fixes import parse_version\nif parse_version(sklearn.__version__) < parse_version(\"1.0\"):\n    from sklearn.experimental import enable_hist_gradient_boosting\n# We can now import the HGBR from ensemble\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\n# We then create a pipeline chaining our encoders to a learner\n\nfrom sklearn.pipeline import make_pipeline\n\npipeline = make_pipeline(encoder, HistGradientBoostingRegressor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pipeline can be readily applied to the dataframe for prediction\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dirty-category encoding\n\nThe one-hot encoder is actually not well suited to the 'Employee\nPosition Title' column, as this column contains 400 different entries:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nnp.unique(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now experiment with encoders specially made for handling\ndirty columns\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dirty_cat import (SimilarityEncoder, TargetEncoder,\n                       MinHashEncoder, GapEncoder)\n\nencoders = {\n    'one-hot': one_hot,\n    'similarity': SimilarityEncoder(),\n    'target': TargetEncoder(handle_unknown='ignore'),\n    'minhash': MinHashEncoder(n_components=100),\n    'gap': GapEncoder(n_components=100),\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now loop over the different encoding methods,\ninstantiate a new |Pipeline| each time, fit it\nand store the returned cross-validation score:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n\nall_scores = dict()\n\nfor name, method in encoders.items():\n    encoder = make_column_transformer(\n        (one_hot, ['gender', 'department_name', 'assignment_category']),\n        ('passthrough', ['year_first_hired']),\n        # Last but not least, our dirty column\n        (method, ['employee_position_title']),\n        remainder='drop',\n    )\n\n    pipeline = make_pipeline(encoder, HistGradientBoostingRegressor())\n    scores = cross_val_score(pipeline, X, y)\n    print(f'{name} encoding')\n    print(f'r2 score:  mean: {np.mean(scores):.3f}; '\n          f'std: {np.std(scores):.3f}\\n')\n    all_scores[name] = scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plotting the results\n\nFinally, we plot the scores on a boxplot:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import seaborn\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(4, 3))\nax = seaborn.boxplot(data=pd.DataFrame(all_scores), orient='h')\nplt.ylabel('Encoding', size=20)\nplt.xlabel('Prediction accuracy     ', size=20)\nplt.yticks(size=20)\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The clear trend is that encoders grasping the similarities in the category\n(similarity, minhash, and gap) perform better than those discarding it.\n\nSimilarityEncoder is the best performer, but it is less scalable on big\ndata than MinHashEncoder and GapEncoder. The most scalable encoder is\nthe MinHashEncoder. GapEncoder, on the other hand, has the benefit that\nit provides interpretable features\n(see `sphx_glr_auto_examples_03_feature_interpretation_gap_encoder.py`)\n\n|\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n## A simpler way: automatic vectorization\n\nThe code to assemble a column transformer is a bit tedious. We will\nnow explore a simpler, automated, way of encoding the data.\n\nLet's start again from the raw data:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "employee_salaries = fetch_employee_salaries()\nX = employee_salaries.X\ny = employee_salaries.y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll drop the \"date_first_hired\" column as it's redundant with\n\"year_first_hired\".\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = X.drop(['date_first_hired'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We still have a complex and heterogeneous dataframe:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The |SV| can to turn this dataframe into a form suited for\nmachine learning.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the SuperVectorizer in a supervised-learning pipeline\n\nAssembling the |SV| in a |Pipeline| with a powerful learner,\nsuch as gradient boosted trees, gives **a machine-learning method that\ncan be readily applied to the dataframe**.\n\nThe |SV| requires at least dirty_cat 0.2.0.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dirty_cat import SuperVectorizer\n\npipeline = make_pipeline(\n    SuperVectorizer(auto_cast=True),\n    HistGradientBoostingRegressor()\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's perform a cross-validation to see how well this model predicts\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(pipeline, X, y, scoring='r2')\n\nprint(f'scores={scores}')\nprint(f'mean={np.mean(scores)}')\nprint(f'std={np.std(scores)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The prediction performed here is pretty much as good as above\nbut the code here is much simpler as it does not involve specifying\ncolumns manually.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyzing the features created\n\nLet us perform the same workflow, but without the |Pipeline|, so we can\nanalyze the SuperVectorizer's mechanisms along the way.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sup_vec = SuperVectorizer(auto_cast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We split the data between train and test, and transform them:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.15, random_state=42\n)\n\nX_train_enc = sup_vec.fit_transform(X_train, y_train)\nX_test_enc = sup_vec.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The encoded data, X_train_enc and X_test_enc are numerical arrays:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They have more columns than the original dataframe, but not much more:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train.shape, X_train_enc.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Inspecting the features created\n\nThe |SV| assigns a transformer for each column. We can inspect this\nchoice:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n\npprint(sup_vec.transformers_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is what is being passed to the |ColumnTransformer| under the hood.\nIf you're familiar with how the latter works, it should be very intuitive.\nWe can notice it classified the columns \"gender\" and \"assignment_category\"\nas low cardinality string variables.\nA |OneHotEncoder| will be applied to these columns.\n\nThe vectorizer actually makes the difference between string variables\n(data type ``object`` and ``string``) and categorical variables\n(data type ``category``).\n\nNext, we can have a look at the encoded feature names.\n\nBefore encoding:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X.columns.to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After encoding (we only plot the first 8 feature names):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feature_names = sup_vec.get_feature_names_out()\nfeature_names[:8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, it gave us interpretable columns.\nThis is because we used |Gap| on the column \"division\",\nwhich was classified as a high cardinality string variable.\n(default values, see |SV|'s docstring).\n\nIn total, we have reasonable number of encoded columns.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "len(feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature importances in the statistical model\n\nIn this section, we will train a regressor, and plot the feature importances\n\n.. topic:: Note:\n\n   To minimize compute time, use the feature importances computed by the\n   |RandomForestRegressor|, but you should prefer |permutation importances|\n   instead (which are less subject to biases)\n\nFirst, let's train the |RandomForestRegressor|,\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor()\nregressor.fit(X_train_enc, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Retrieving the feature importances\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "importances = regressor.feature_importances_\nstd = np.std(\n    [\n        tree.feature_importances_\n        for tree in regressor.estimators_\n    ],\n    axis=0\n)\nindices = np.argsort(importances)\n# Sort from least to most\nindices = list(reversed(indices))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the results:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 9))\nplt.title(\"Feature importances\")\nn = 20\nn_indices = indices[:n]\nlabels = np.array(feature_names)[n_indices]\nplt.barh(range(n), importances[n_indices], color=\"b\", yerr=std[n_indices])\nplt.yticks(range(n), labels, size=15)\nplt.tight_layout(pad=1)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can deduce from this data that the three factors that define the\nmost the salary are: being hired for a long time, being a manager, and\nhaving a permanent, full-time job :)\n\n\n.. topic:: The SuperVectorizer automates preprocessing\n\n  As this notebook demonstrates, many preprocessing steps can be\n  automated by the |SV|, and the resulting pipeline can still be\n  inspected, even with non-normalized entries.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}