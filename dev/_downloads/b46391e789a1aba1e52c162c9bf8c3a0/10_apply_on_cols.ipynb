{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Hands-On with Column Selection and Transformers\n\nIn previous examples, we saw how skrub provides powerful abstractions like\n:class:`~skrub.TableVectorizer` and :func:`~skrub.tabular_learner` to create pipelines.\n\nIn this new example, we show how to create more flexible pipelines by selecting\nand transforming dataframe columns using arbitrary logic.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We begin with loading a dataset with heterogeneous datatypes, and replacing Pandas's\ndisplay with the TableReport display via :func:`skrub.set_config`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import skrub\nfrom skrub.datasets import fetch_employee_salaries\n\nskrub.set_config(use_tablereport=True)\ndata = fetch_employee_salaries()\nX, y = data.X, data.y\nX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our goal is now to apply a :class:`~skrub.StringEncoder` to two columns of our\nchoosing: ``division`` and ``employee_position_title``.\n\nWe can achieve this using :class:`~skrub.ApplyToCols`, whose job is to apply a\ntransformer to multiple columns independently, and let unmatched columns through\nwithout changes.\nThis can be seen as a handy drop-in replacement of the\n:class:`~sklearn.compose.ColumnTransformer`.\n\nSince we selected two columns and set the number of components to ``30`` each,\n:class:`~skrub.ApplyToCols` will create ``2*30`` embedding columns in the dataframe\n``Xt``, which we prefix with ``lsa_``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import ApplyToCols, StringEncoder\n\napply_string_encoder = ApplyToCols(\n    StringEncoder(n_components=30),\n    cols=[\"division\", \"employee_position_title\"],\n    rename_columns=\"lsa_{}\",\n)\nXt = apply_string_encoder.fit_transform(X)\nXt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In addition to the :class:`~skrub.ApplyToCols` class, the\n:class:`~skrub.ApplyToFrame` class is useful for transformers that work on multiple\ncolumns at once, such as the :class:`~sklearn.decomposition.PCA` which reduces the\nnumber of components.\n\nTo select columns without hardcoding their names, we introduce\n`selectors<selectors>`, which allow for flexible matching pattern and composable\nlogic.\n\nThe regex selector below will match all columns prefixed with ``\"lsa\"``, and pass them\nto :class:`~skrub.ApplyToFrame` which will assemble these columns into a dataframe and\nfinally pass it to the PCA.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n\nfrom skrub import ApplyToFrame\nfrom skrub import selectors as s\n\napply_pca = ApplyToFrame(PCA(n_components=8), cols=s.regex(\"lsa\"))\nXt = apply_pca.fit_transform(Xt)\nXt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These two selectors are scikit-learn transformers and can be chained together within\na :class:`~sklearn.pipeline.Pipeline`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline\n\nmake_pipeline(\n    apply_string_encoder,\n    apply_pca,\n).fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that selectors also come in handy in a pipeline to select or drop columns, using\n:class:`~skrub.SelectCols` and :class:`~skrub.DropCols`!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n\nfrom skrub import SelectCols\n\n# Select only numerical columns\npipeline = make_pipeline(\n    SelectCols(cols=s.numeric()),\n    StandardScaler(),\n).set_output(transform=\"pandas\")\npipeline.fit_transform(Xt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's run through one more example to showcase the expressiveness of the selectors.\nSuppose we want to apply an :class:`~sklearn.preprocessing.OrdinalEncoder` on\ncategorical columns with low cardinality (e.g., fewer than ``40`` unique values).\n\nWe define a column filter using skrub selectors with a lambda function. Note that\nthe same effect can be obtained directly by using\n:func:`~srkub.selectors.cardinality_below`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n\nlow_cardinality = s.filter(lambda col: col.nunique() < 40)\nApplyToCols(OrdinalEncoder(), cols=s.string() & low_cardinality).fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice how we composed the selector with :func:`~skrub.selectors.string()`\nusing a logical operator. This resulting selector matches string\ncolumns with cardinality below ``40``.\n\nWe can also define the opposite selector ``high_cardinality`` using the negation\noperator ``~`` and apply a :class:`skrub.StringEncoder` to vectorize those\ncolumns.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingRegressor\n\nhigh_cardinality = ~low_cardinality\npipeline = make_pipeline(\n    ApplyToCols(\n        OrdinalEncoder(),\n        cols=s.string() & low_cardinality,\n    ),\n    ApplyToCols(\n        StringEncoder(),\n        cols=s.string() & high_cardinality,\n    ),\n    HistGradientBoostingRegressor(),\n).fit(X, y)\npipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interestingly, the pipeline above is similar to the datatype dispatching performed by\n:class:`~skrub.TableVectorizer`, also used in :func:`~skrub.tabular_learner`.\n\nClick on the dropdown arrows next to the datatype to see the columns are mapped to\nthe different transformers in :class:`~skrub.TableVectorizer`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import tabular_learner\n\ntabular_learner(\"regressor\").fit(X, y)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}