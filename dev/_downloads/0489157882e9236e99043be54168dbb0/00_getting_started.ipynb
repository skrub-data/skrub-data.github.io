{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Getting Started\n\nThis guide showcases the features of ``skrub``, an open-source package that aims at\nbridging the gap between tabular data sources and machine-learning models.\n\n\nMuch of ``skrub`` revolves around vectorizing, assembling, and encoding tabular data,\nto prepare data in a format that shallow or classic machine-learning models understand.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading example datasets\n\nThe :obj:`~skrub.datasets` module allows us to download tabular datasets and\ndemonstrate ``skrub``'s features.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>You can control the directory where the datasets are stored by:\n\n   - setting in your environment the ``SKRUB_DATA_DIRECTORY`` variable to an\n     absolute directory path,\n   - using the parameter ``data_directory`` in fetch functions, which takes\n     precedence over the envar.\n\n   By default, the datasets are stored in a folder named \"skrub_data\" in the\n   user home folder.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub.datasets import fetch_employee_salaries\n\ndataset = fetch_employee_salaries()\nemployees_df, salaries = dataset.X, dataset.y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explore all the available datasets in `downloading_a_dataset_ref`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating an interactive report for a dataframe\n\nThe :class:`~skrub.Cleaner` allows to clean the\ndataframe, parsing nulls, dates, and dropping columns with too many nulls.\nTo quickly get an overview of a dataframe's contents, use the\n:class:`~skrub.TableReport`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import Cleaner, TableReport\n\nemployees_df = Cleaner().fit_transform(employees_df)\nTableReport(employees_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use the interactive display above to explore the dataset visually.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>You can see a few more `example reports`_ online. We also\n   provide an experimental online demo_ that allows you to select a CSV or\n   parquet file and generate a report directly in your web browser, without\n   installing anything.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is also possible to tell skrub to replace the default pandas & polars\ndisplays with ``TableReport``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import patch_display, unpatch_display\n\npatch_display()\n\nemployees_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The effect of ``patch_display`` can be undone with ``skrub.unpatch_display()``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unpatch_display()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Easily building a strong baseline for tabular machine learning\n\nThe goal of ``skrub`` is to ease tabular data preparation for machine learning.\nThe :func:`~skrub.tabular_learner` function provides an easy way to build a simple\nbut reliable machine-learning model, working well on most tabular data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n\nfrom skrub import tabular_learner\n\nmodel = tabular_learner(\"regressor\")\nresults = cross_validate(model, employees_df, salaries)\nresults[\"test_score\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To handle rich tabular data and feed it to a machine-learning model, the\npipeline returned by :func:`~skrub.tabular_learner` preprocesses and encodes\nstrings, categories and dates using the :class:`~skrub.TableVectorizer`.\nSee its documentation or `sphx_glr_auto_examples_01_encodings.py` for\nmore details. An overview of the chosen defaults is available in\n`end_to_end_pipeline`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assembling data\n\n``Skrub`` allows imperfect assembly of data, such as joining dataframes\non columns that contain typos. ``Skrub``'s joiners have ``fit`` and\n``transform`` methods, storing information about the data across calls.\n\nThe :class:`~skrub.Joiner` allows fuzzy-joining multiple tables, each row of\na main table will be augmented with values from the best match in the auxiliary table.\nYou can control how distant fuzzy-matches are allowed to be with the\n``max_dist`` parameter.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following, we add information about countries to a table containing\nairports and the cities they are in:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\nfrom skrub import Joiner\n\nairports = pd.DataFrame(\n    {\n        \"airport_id\": [1, 2],\n        \"airport_name\": [\"Charles de Gaulle\", \"Aeroporto Leonardo da Vinci\"],\n        \"city\": [\"Paris\", \"Roma\"],\n    }\n)\n# notice the \"Rome\" instead of \"Roma\"\ncapitals = pd.DataFrame(\n    {\"capital\": [\"Berlin\", \"Paris\", \"Rome\"], \"country\": [\"Germany\", \"France\", \"Italy\"]}\n)\njoiner = Joiner(\n    capitals,\n    main_key=\"city\",\n    aux_key=\"capital\",\n    max_dist=0.8,\n    add_match_info=False,\n)\njoiner.fit_transform(airports)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Information about countries have been added, even if the rows aren't exactly matching.\n\nIt's also possible to augment data by joining and aggregating multiple\ndataframes with the :class:`~skrub.AggJoiner`. This is particularly useful to\nsummarize information scattered across tables, for instance adding statistics\nabout flights to the dataframe of airports:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import AggJoiner\n\nflights = pd.DataFrame(\n    {\n        \"flight_id\": range(1, 7),\n        \"from_airport\": [1, 1, 1, 2, 2, 2],\n        \"total_passengers\": [90, 120, 100, 70, 80, 90],\n        \"company\": [\"DL\", \"AF\", \"AF\", \"DL\", \"DL\", \"TR\"],\n    }\n)\nagg_joiner = AggJoiner(\n    aux_table=flights,\n    main_key=\"airport_id\",\n    aux_key=\"from_airport\",\n    cols=[\"total_passengers\"],  # the cols to perform aggregation on\n    operations=[\"mean\", \"std\"],  # the operations to compute\n)\nagg_joiner.fit_transform(airports)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For joining multiple auxiliary tables on a main table at once, use the\n:class:`~skrub.MultiAggJoiner`.\n\nSee other ways to join multiple tables in `assembling`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding data\n\nWhen a column contains categories with variations and typos, it can\nbe encoded using one of ``skrub``'s encoders, such as the\n:class:`~skrub.GapEncoder`.\n\nThe :class:`~skrub.GapEncoder` creates a continuous encoding, based on\nthe activation of latent categories. It will create the encoding based on\ncombinations of substrings which frequently co-occur.\n\nFor instance, we might want to encode a column ``X`` that contains\ninformation about cities, being either Madrid or Rome :\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import GapEncoder\n\nX = pd.Series(\n    [\n        \"Rome, Italy\",\n        \"Rome\",\n        \"Roma, Italia\",\n        \"Madrid, SP\",\n        \"Madrid, spain\",\n        \"Madrid\",\n        \"Romq\",\n        \"Rome, It\",\n    ],\n    name=\"city\",\n)\nenc = GapEncoder(n_components=2, random_state=0)  # 2 topics in the data\nenc.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :class:`~skrub.GapEncoder` has found the following two topics:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "enc.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Which correspond to the two cities.\n\nLet's see the activation of each topic depending on the rows of ``X``:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "encoded = enc.fit_transform(X).assign(original=X)\nencoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The higher the activation, the closer the row to the latent topic. These\ncolumns can now be understood by a machine-learning model.\n\nThe other encoders are presented in `encoding`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n\nWe have briefly covered pipeline creation, vectorizing, assembling, and encoding\ndata. We presented the main functionalities of ``skrub``, but there is much\nmore to it !\n\nPlease refer to our `user_guide` for a more in-depth presentation of\n``skrub``'s concepts, or visit our\n[examples](https://skrub-data.org/stable/auto_examples) for more\nillustrations of the tools that we provide !\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}