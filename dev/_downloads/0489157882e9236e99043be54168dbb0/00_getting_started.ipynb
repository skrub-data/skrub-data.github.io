{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Getting Started\n\nThis guide showcases some of the features of ``skrub``, an open-source package\nthat aims at bridging the gap between tabular data stored in Pandas or Polars\ndataframes, and machine-learning models.\n\nMuch of ``skrub`` revolves around simplifying many of the tasks that are involved\nin pre-processing raw data into a format that shallow or classic machine-learning\nmodels can understand, that is, numerical data.\n\n``skrub`` does this by vectorizing, assembling, and encoding tabular data through\na number of features that we present in this example and the following.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading example datasets\n\nThe :obj:`~skrub.datasets` module allows us to download tabular datasets and\ndemonstrate ``skrub``'s features.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>You can control the directory where the datasets are stored by:\n\n   - setting in your environment the ``SKRUB_DATA_DIRECTORY`` variable to an\n     absolute directory path,\n   - using the parameter ``data_directory`` in fetch functions, which takes\n     precedence over the envar.\n\n   By default, the datasets are stored in a folder named \"skrub_data\" in the\n   user home folder.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub.datasets import fetch_employee_salaries\n\ndataset = fetch_employee_salaries()\nemployees_df, salaries = dataset.X, dataset.y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Explore all the available datasets in `datasets_ref`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preliminary exploration and parsing of data\nTypically, the first operations that are done on new data involve data exploration\nand parsing.\nTo quickly get an overview of a dataframe's contents, use the\n:class:`~skrub.TableReport`.\nHere, we also use the :class:`~skrub.Cleaner`, a transformer that cleans the\ndataframe by parsing nulls and dates, and by dropping \"uninformative\" columns\n(e.g., that contain too many nulls, or that are constant).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import Cleaner, TableReport\n\nTableReport(employees_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the Report above, we can see that there are datetime columns, so we use the\n:class:`~skrub.Cleaner` to parse them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "employees_df = Cleaner().fit_transform(employees_df)\nTableReport(employees_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can use the interactive display above to explore the dataset visually.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>You can see a few more `example reports`_ online. We also\n   provide an experimental online demo_ that allows you to select a CSV or\n   parquet file and generate a report directly in your web browser, without\n   installing anything.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is also possible to tell ``skrub`` to replace the default pandas & polars\ndisplays with ``TableReport`` by modifying the global config with\n:func:`~skrub.set_config`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import set_config\n\nset_config(use_table_report=True)\n\nemployees_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This setting can easily be reverted:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "set_config(use_table_report=False)\n\nemployees_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Easily building a strong baseline for tabular machine learning\n\nThe goal of ``skrub`` is to ease tabular data preparation for machine learning.\nThe :func:`~skrub.tabular_pipeline` function provides an easy way to build a simple\nbut reliable machine learning model that works well on most tabular data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n\nfrom skrub import tabular_pipeline\n\nmodel = tabular_pipeline(\"regressor\")\nresults = cross_validate(model, employees_df, salaries)\nresults[\"test_score\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To handle rich tabular data and feed it to a machine learning model, the\npipeline returned by :func:`~skrub.tabular_pipeline` preprocesses and encodes\nstrings, categories and dates using the :class:`~skrub.TableVectorizer`.\nSee its documentation or `sphx_glr_auto_examples_01_encodings.py` for\nmore details. An overview of the chosen defaults is available in\n`user_guide_tabular_pipeline`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assembling data\n\n``skrub`` allows imperfect assembly of data, such as joining dataframes\non columns that contain typos. ``skrub``'s joiners have ``fit`` and\n``transform`` methods, storing information about the data across calls.\n\nThe :class:`~skrub.Joiner` allows fuzzy-joining multiple tables, each row of\na main table will be augmented with values from the best match in the auxiliary table.\nYou can control how distant fuzzy-matches are allowed to be with the\n``max_dist`` parameter.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following, we add information about countries to a table containing\nairports and the cities they are in:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\nfrom skrub import Joiner\n\nairports = pd.DataFrame(\n    {\n        \"airport_id\": [1, 2],\n        \"airport_name\": [\"Charles de Gaulle\", \"Aeroporto Leonardo da Vinci\"],\n        \"city\": [\"Paris\", \"Roma\"],\n    }\n)\n# Notice the \"Rome\" instead of \"Roma\"\ncapitals = pd.DataFrame(\n    {\"capital\": [\"Berlin\", \"Paris\", \"Rome\"], \"country\": [\"Germany\", \"France\", \"Italy\"]}\n)\njoiner = Joiner(\n    capitals,\n    main_key=\"city\",\n    aux_key=\"capital\",\n    max_dist=0.8,\n    add_match_info=False,\n)\njoiner.fit_transform(airports)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Information about countries have been added, even if the rows aren't exactly matching.\n\n``skrub`` allows to aggregate multiple tables according to various strategies: you\ncan see other ways to join multiple tables in `userguide_joining_tables`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding any data as numerical features\n\nTabular data can contain a variety of datatypes, ranging from numerical, to\ndatetimes, to categories, strings, and text. Encoding features in a meaningful\nway requires a lot of effort and is a major part of the feature engineering\nprocess that is required to properly train machine learning models.\n\n``skrub`` helps with this by providing various transformers that automatically\nencode different datatypes into ``float32`` features.\n\nFor **numerical features**, the :class:`~skrub.SquashingScaler` applies a robust\nscaling technique that is less sensitive to outliers. Check the\n`relative example <sphx_glr_auto_examples_11_squashing_scaler.py>`\nfor more information on the feature.\n\nFor **datetime columns**, ``skrub`` provides the :class:`~skrub.DatetimeEncoder`\nwhich can extract useful features such as year, month, day, as well as additional\nfeatures such as weekday or day of year. Periodic encoding with trigonometric\nor spline features is also available. Refer to the :class:`~skrub.DatetimeEncoder`\ndocumentation for more detail.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\ndata = pd.DataFrame(\n    {\n        \"event\": [\"A\", \"B\", \"C\"],\n        \"date_1\": [\"2020-01-01\", \"2020-06-15\", \"2021-03-22\"],\n        \"date_2\": [\"2020-01-15\", \"2020-07-01\", \"2021-04-05\"],\n    }\n)\ndata = Cleaner().fit_transform(data)\nTableReport(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``skrub`` transformers are applied column-by-column, but it is possible to use\nthe :class:`~skrub.ApplyToCols` meta-transformer to apply a transformer to\nmultiple columns at once. Complex column selection is possible using\n`skrub's column selectors <userguide_selectors>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import ApplyToCols, DatetimeEncoder\n\nApplyToCols(\n    DatetimeEncoder(add_total_seconds=False), cols=[\"date_1\", \"date_2\"]\n).fit_transform(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, when a column contains **categorical or string data**, it can be\nencoded using various encoders provided by ``skrub``. The default encoder is\nthe :class:`~skrub.StringEncoder`, which encodes categories using\n[Latent Semantic Analysis (LSA)](https://scikit-learn.org/stable/modules/decomposition.html#about-truncated-svd-and-latent-semantic-analysis-(lsa)).\nIt is a simple and efficient way to encode categories, and works well in\npractice.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(\n    {\n        \"city\": [\"Paris\", \"London\", \"Berlin\", \"Madrid\", \"Rome\"],\n        \"country\": [\"France\", \"UK\", \"Germany\", \"Spain\", \"Italy\"],\n    }\n)\nTableReport(data)\nfrom skrub import StringEncoder\n\nStringEncoder(n_components=3).fit_transform(data[\"city\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If your data includes a lot of text, you may want to use the\n:class:`~skrub.TextEncoder`,\nwhich uses pre-trained language models retrieved from the HuggingFace hub to\ncreate meaningful text embeddings.\nSee `userguide_encoders` for more details on all the categorical encoders\nprovided by ``skrub``, and `sphx_glr_auto_examples_01_encodings.py` for a\ncomparison between the different methods.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced use cases\nIf your use case involves more complex data preparation, hyperparameter tuning,\nor model selection, if you want to build a multi-table pipeline that requires\nassembling and preparing multiple tables, or if you want to make sure that the\ndata preparation can be reproduced exactly, you can use the ``skrub`` Data Ops,\na powerful framework which provides tools to build complex data processing pipelines.\nSee the relative `user guide <userguide_data_ops>` and the\n`data_ops_examples_ref`\nexamples for more details.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n\nWe have briefly covered pipeline creation, vectorizing, assembling, and encoding\ndata. We presented the main functionalities of ``skrub``, but there is much\nmore to it!\n\nPlease refer to our `user_guide` for a more in-depth presentation of\n``skrub``'s concepts, or visit our\n[examples](https://skrub-data.org/stable/auto_examples) for more\nillustrations of the tools that we provide!\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}