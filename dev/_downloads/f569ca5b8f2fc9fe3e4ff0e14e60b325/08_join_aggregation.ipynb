{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Self-aggregation on MovieLens\n\nMovieLens is a famous movie dataset used for both explicit\nand implicit recommender systems. It provides a main table,\n\"ratings\", that can be viewed as logs or transactions, comprised\nof only 4 columns: ``userId``, ``movieId``, ``rating`` and ``timestamp``.\nMovieLens also gives a contextual table \"movies\", including\n``movieId``, ``title`` and ``types``, to enable content-based feature extraction.\n\nFrom the perspective of machine-learning pipelines, one challenge is to\ntransform the transaction log into features that can be fed to supervised learning.\n\nIn this notebook, we only deal with the main table \"ratings\".\nOur objective is **not to achieve state-of-the-art performance** on\nthe explicit regression task, but rather to illustrate how to perform\nfeature engineering in a simple way using |AggJoiner| and |AggTarget|.\nNote that our performance is higher than the baseline of using the mean\nrating per movies.\n\nThe benefit of using  |AggJoiner| and |AggTarget| is that they readily\nprovide a full pipeline, from the original tables to the prediction, that can\nbe cross-validated or applied to new data to serve prediction. At the end of\nthis example, we showcase hyper-parameter optimization on the whole pipeline.\n\n\n.. |AggJoiner| replace::\n     :class:`~skrub.AggJoiner`\n\n.. |AggTarget| replace::\n     :class:`~skrub.AggTarget`\n\n.. |TableVectorizer| replace::\n     :class:`~skrub.TableVectorizer`\n\n.. |DatetimeEncoder| replace::\n     :class:`~skrub.DatetimeEncoder`\n\n.. |TargetEncoder| replace::\n     :class:`~sklearn.preprocessing.TargetEncoder`\n\n.. |make_pipeline| replace::\n     :class:`~sklearn.pipeline.make_pipeline`\n\n.. |Pipeline| replace::\n     :class:`~sklearn.pipeline.Pipeline`\n\n.. |GridSearchCV| replace::\n     :class:`~sklearn.model_selection.GridSearchCV`\n\n.. |TimeSeriesSplit| replace::\n     :class:`~sklearn.model_selection.TimeSeriesSplit`\n\n.. |HGBR| replace::\n     :class:`~sklearn.ensemble.HistGradientBoostingRegressor`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The data\n\nWe begin with loading the ratings table from MovieLens.\nNote that we use the light version (100k rows).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\nfrom skrub.datasets import fetch_movielens\n\n\nratings = fetch_movielens(dataset_id=\"ratings\")\nratings = ratings.X.sort_values(\"timestamp\").reset_index(drop=True)\nratings[\"timestamp\"] = pd.to_datetime(ratings[\"timestamp\"], unit=\"s\")\n\nX = ratings[[\"userId\", \"movieId\", \"timestamp\"]]\ny = ratings[\"rating\"]\nX.shape, y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding the timestamp with a TableVectorizer\n\nOur first step is to extract features from the timestamp, using the\n|TableVectorizer|. Natively, it uses the |DatetimeEncoder| on datetime\ncolumns, and doesn't interact with numerical columns.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import TableVectorizer, DatetimeEncoder\n\n\ntable_vectorizer = TableVectorizer(\n    datetime_transformer=DatetimeEncoder(add_day_of_the_week=True)\n)\ntable_vectorizer.set_output(transform=\"pandas\")\nX_date_encoded = table_vectorizer.fit_transform(X)\nX_date_encoded.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now make a couple of plots and gain some insight on our dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"darkgrid\")\n\n\ndef make_barplot(x, y, title):\n    norm = plt.Normalize(y.min(), y.max())\n    cmap = plt.get_cmap(\"magma\")\n\n    sns.barplot(x=x, y=y, palette=cmap(norm(y)))\n    plt.title(title)\n    plt.xticks(rotation=30)\n    plt.ylabel(None)\n    plt.tight_layout()\n\n\n# O is Monday, 6 is Sunday\n\ndaily_volume = X_date_encoded[\"timestamp_day_of_week\"].value_counts().sort_index()\n\nmake_barplot(\n    x=daily_volume.index,\n    y=daily_volume.values,\n    title=\"Daily volume of ratings\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also display the distribution of our target ``y``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rating_count = y.value_counts().sort_index()\n\nmake_barplot(\n    x=rating_count.index,\n    y=rating_count.values,\n    title=\"Distribution of ratings given to movies\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AggTarget: aggregate y, then join\n\nWe have just extracted datetime features from timestamps.\n\nLet's now perform an expansion for the target ``y``, by aggregating it before\njoining it back on the main table. The biggest risk of doing target expansion\nwith multiple dataframe operations yourself is to end up leaking the target.\n\nTo solve this, the |AggTarget| transformer allows you to\naggregate the target ``y`` before joining it on the main table, without\nrisk of leaking. Note that to perform aggregation then joining on the features\n``X``, you need to use |AggJoiner| instead.\n\nYou can also think of it as a generalization of the |TargetEncoder|, which\nencodes categorical features based on the target.\n\nWe only focus on aggregating the target by **users**, but later we will\nalso consider aggregating by **movies**. Here, we compute the histogram of the\ntarget with 3 bins, before joining it back on the initial table.\n\nThis feature answer questions like\n*\"How many times has this user given a bad, medium or good rate to movies?\"*.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import AggTarget\n\n\nagg_target_user = AggTarget(\n    main_key=\"userId\",\n    suffix=\"_user\",\n    operation=\"hist(3)\",\n)\nX_transformed = agg_target_user.fit_transform(X, y)\n\nX_transformed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_transformed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly, we join on ``movieId`` instead of ``userId``.\n\nThis feature answer questions like\n*\"How many times has this movie received a bad, medium or good rate from users?\"*.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "agg_target_movie = AggTarget(\n    main_key=\"movieId\",\n    suffix=\"_movie\",\n    operation=\"hist(3)\",\n)\nX_transformed = agg_target_movie.fit_transform(X, y)\nX_transformed.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_transformed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chaining everything together in a pipeline\n\nTo perform cross-validation and enable hyper-parameter tuning, we gather\nall elements into a scikit-learn |Pipeline| by using |make_pipeline|,\nand define a scikit-learn |HGBR|.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\n\npipeline = make_pipeline(\n    table_vectorizer,\n    agg_target_user,\n    agg_target_movie,\n    HistGradientBoostingRegressor(learning_rate=0.1, max_depth=4, max_iter=40),\n)\n\npipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyper-parameters tuning and cross validation\n\nWe can finally create our hyper-parameter search space, and use a\n|GridSearchCV|. We select the cross validation splitter to be\nthe |TimeSeriesSplit| to prevent leakage, since our data are timestamped\nlogs.\n\nNote that you need the name of the pipeline elements to assign them\nhyper-parameters search.\n\nYou can lookup the name of the pipeline elements by doing:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "list(pipeline.named_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, you can use scikit-learn |Pipeline| to name your transformers:\n``Pipeline([(\"agg_target_user\", agg_target_user), ...])``\n\nWe now perform the grid search over the ``AggTarget`` transformers to find the\noperation maximizing our validation score.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n\noperations = [\"mean\", \"hist(3)\", \"hist(5)\", \"hist(7)\", \"value_counts\"]\nparam_grid = [\n    {\n        f\"aggtarget-2__operation\": [op],\n    }\n    for op in operations\n]\n\ncv = GridSearchCV(pipeline, param_grid, cv=TimeSeriesSplit(n_splits=10))\ncv.fit(X, y)\n\nresults = pd.DataFrame(cv.cv_results_)\n\ncols = [f\"split{idx}_test_score\" for idx in range(10)]\nresults = results.set_index(\"param_aggtarget-2__operation\")[cols].T\nresults"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The score used in this regression task is the R2. Remember that the R2\nevaluates the relative performance compared to the naive baseline consisting\nin always predicting the mean value of ``y_test``.\nTherefore, the R2 is 0 when ``y_pred = y_true.mean()`` and is upper bounded\nto 1 when ``y_pred = y_true``.\n\nTo get a better sense of the learning performances of our simple pipeline,\nwe also compute the average rating of each movie in the training set,\nand uses this average to predict the ratings in the test set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\n\n\ndef baseline_r2(X, y, train_idx, test_idx):\n    \"\"\"Compute the average rating for all movies in the train set,\n    and map these averages to the test set as a prediction.\n\n    If a movie in the test set is not present in the training set,\n    we simply predict the global average rating of the training set.\n    \"\"\"\n    X_train, y_train = X.iloc[train_idx].copy(), y.iloc[train_idx]\n    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n    X_train[\"y\"] = y_train\n\n    movie_avg_rating = X_train.groupby(\"movieId\")[\"y\"].mean().to_frame().reset_index()\n\n    y_pred = X_test.merge(movie_avg_rating, on=\"movieId\", how=\"left\")[\"y\"]\n    y_pred = y_pred.fillna(y_pred.mean())\n\n    return r2_score(y_true=y_test, y_pred=y_pred)\n\n\nall_baseline_r2 = []\nfor train_idx, test_idx in TimeSeriesSplit(n_splits=10).split(X, y):\n    all_baseline_r2.append(baseline_r2(X, y, train_idx, test_idx))\n\nresults.insert(0, \"naive mean estimator\", all_baseline_r2)\n\n# we only keep the 5 out of 10 last results\n# because the initial size of the train set is rather small\nsns.boxplot(results.tail(5), palette=\"magma\")\nplt.ylabel(\"R2 score\")\nplt.title(\"Hyper parameters grid-search results\")\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The naive estimator has a lower performance than our pipeline, which means\nthat our extracted features brought some predictive power.\n\nIt seems that using the ``\"value_counts\"`` as an aggregation operator for\n|AggTarget| yields better performances than using the mean (which is\nequivalent to using the |TargetEncoder|).\n\nHere, the number of bins encoding the target is proportional to the\nperformance: computing the mean yields a single statistic, whereas histograms\nyield a density over a reduced set of bins, and ``\"value_counts\"`` yields an\nexhaustive histogram over all the possible values of ratings\n(here 10 different values, from 0.5 to 5).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}