{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the skrub examples in JupyterLite is experimental and you mayencounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import skrub` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/skrub-data/skrub/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport micropip\nawait micropip.install('skrub')\n%pip install pyodide-http\nimport pyodide_http\npyodide_http.patch_all()\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Encoding: from a dataframe to a numerical matrix for machine learning\n\nThis example shows how to transform a rich dataframe with columns of various types\ninto a numerical matrix on which machine-learning algorithms can be applied.\nWe study the case of predicting wages using the\n[employee salaries](https://www.openml.org/d/42125) dataset.\n\n.. |TableVectorizer| replace::\n    :class:`~skrub.TableVectorizer`\n\n.. |Pipeline| replace::\n    :class:`~sklearn.pipeline.Pipeline`\n\n.. |OneHotEncoder| replace::\n     :class:`~sklearn.preprocessing.OneHotEncoder`\n\n.. |GapEncoder| replace::\n    :class:`~skrub.GapEncoder`\n\n.. |MinHashEncoder| replace::\n    :class:`~skrub.MinHashEncoder`\n\n.. |DatetimeEncoder| replace::\n    :class:`~skrub.DatetimeEncoder`\n\n.. |HGBR| replace::\n    :class:`~sklearn.ensemble.HistGradientBoostingRegressor`\n\n.. |RandomForestRegressor| replace::\n     :class:`~sklearn.ensemble.RandomForestRegressor`\n\n.. |permutation importances| replace::\n     :func:`~sklearn.inspection.permutation_importance`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Easy learning on a dataframe\n\nLet's first retrieve the dataset, using one of the downloaders from the\n:mod:`skrub.datasets` module. As all the downloaders,\n:func:`~skrub.datasets.fetch_employee_salaries` returns a dataset with attributes\n``X``, and ``y``. ``X`` is a dataframe which contains the features (aka design matrix,\nexplanatory variables, independent variables). ``y`` is a column (pandas Series) which\ncontains the target (aka dependent, response variable) that we want to learn to\npredict from ``X``. In this case ``y`` is the annual salary.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub.datasets import fetch_employee_salaries\n\ndataset = fetch_employee_salaries()\nemployees, salaries = dataset.X, dataset.y\nemployees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most machine-learning algorithms work with arrays of numbers. The\nchallenge here is that the ``employees`` dataframe is a heterogeneous\nset of columns: some are numerical (``'year_first_hired'``), some dates\n(``'date_first_hired'``), some have a few categorical entries\n(``'gender'``), some many (``'employee_position_title'``). Therefore\nour table needs to be \"vectorized\": processed to extract numeric\nfeatures.\n\n``skrub`` provides an easy way to build a simple but reliable\nmachine-learning model which includes this step, working well on most\ntabular data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n\nfrom skrub import tabular_pipeline\n\nmodel = tabular_pipeline(\"regressor\")\nresults = cross_validate(model, employees, salaries)\nresults[\"test_score\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The estimator returned by :obj:`tabular_pipeline` combines 2 steps:\n\n- a |TableVectorizer| to preprocess the dataframe and vectorize the features\n- a supervised learner (by default a |HGBR|)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the rest of this example, we focus on the first step and explore the\ncapabilities of skrub's |TableVectorizer|.\n\n|\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More details on encoding tabular data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import TableVectorizer\n\nvectorizer = TableVectorizer()\nvectorized_employees = vectorizer.fit_transform(employees)\nvectorized_employees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From our 8 columns, the |TableVectorizer| has extracted 143 numerical\nfeatures. Most of them are one-hot encoded representations of the categorical\nfeatures. For example, we can see that 3 columns ``'gender_F'``, ``'gender_M'``,\n``'gender_nan'`` were created to encode the ``'gender'`` column.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By performing appropriate transformations on our complex data, the |TableVectorizer|\nproduced numeric features that we can use for machine-learning:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingRegressor\n\nHistGradientBoostingRegressor().fit(vectorized_employees, salaries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The |TableVectorizer| bridges the gap between tabular data and machine-learning\npipelines. It allows us to apply a machine-learning estimator to our dataframe without\nmanual data wrangling and feature extraction.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspecting the TableVectorizer\n\nThe |TableVectorizer| distinguishes between 4 basic kinds of columns (more may be\nadded in the future).\nFor each kind, it applies a different transformation, which we can configure. The\nkinds of columns and the default transformation for each of them are:\n\n- numeric columns: simply casting to floating-point\n- datetime columns: extracting features such as year, day, hour with the\n  |DatetimeEncoder|\n- low-cardinality categorical columns: one-hot encoding\n- high-cardinality categorical columns: a simple and effective text representation\n  pipeline provided by the |GapEncoder|\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can inspect which transformation was chosen for a each column and retrieve the\nfitted transformer. ``vectorizer.kind_to_columns_`` provides an overview of how the\nvectorizer categorized columns in our input:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vectorizer.kind_to_columns_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The reverse mapping is given by:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vectorizer.column_to_kind_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``vectorizer.transformers_`` gives us a dictionary which maps column names to the\ncorresponding transformer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vectorizer.transformers_[\"date_first_hired\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also see which features in the vectorizer's output were derived from a given\ninput column.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vectorizer.input_to_outputs_[\"date_first_hired\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vectorized_employees[vectorizer.input_to_outputs_[\"date_first_hired\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can go in the opposite direction: given a column in the input, find out\nfrom which input column it was derived.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vectorizer.output_to_input_[\"department_BOA\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataframe preprocessing\n\nNote that ``\"date_first_hired\"`` has been recognized and processed as a datetime\n\u00dfcolumn.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vectorizer.column_to_kind_[\"date_first_hired\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But looking closer at our original dataframe, it was encoded as a string.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "employees[\"date_first_hired\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note the ``dtype: object`` in the output above.\nBefore applying the transformers we specify, the |TableVectorizer| performs a few\npreprocessing steps.\n\nFor example, strings commonly used to represent missing values such as ``\"N/A\"`` are\nreplaced with actual ``null``. As we saw above, columns containing strings that\nrepresent dates (e.g. ``'2024-05-15'``) are detected and converted  to proper\ndatetimes.\n\nWe can inspect the list of steps that were applied to a given column:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vectorizer.all_processing_steps_[\"date_first_hired\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These preprocessing steps depend on the column:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vectorizer.all_processing_steps_[\"department\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A simple Pipeline for tabular data\n\nThe |TableVectorizer| outputs data that can be understood by a scikit-learn\nestimator. Therefore we can easily build a 2-step scikit-learn ``Pipeline``\nthat we can fit, test or cross-validate and that works well on tabular data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.pipeline import make_pipeline\n\npipeline = make_pipeline(TableVectorizer(), HistGradientBoostingRegressor())\n\nresults = cross_validate(pipeline, employees, salaries)\nscores = results[\"test_score\"]\nprint(f\"R2 score:  mean: {np.mean(scores):.3f}; std: {np.std(scores):.3f}\")\nprint(f\"mean fit time: {np.mean(results['fit_time']):.3f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specializing the TableVectorizer for HistGradientBoosting\n\nThe encoders used by default by the |TableVectorizer| are safe choices for a wide\nrange of downstream estimators. If we know we want to use it with a |HGBR| (or\nclassifier) model, we can make some different choices that are only well-suited for\ntree-based models but can yield a faster pipeline.\nWe make 2 changes.\n\nThe |HGBR| has built-in support for categorical features, so we do not need to one-hot\nencode them.\nWe do need to tell it which features should be treated as categorical with the\n``categorical_features`` parameter. In recent versions of scikit-learn, we can set\n``categorical_features='from_dtype'``, and it will treat all columns in the input that\nhave a ``Categorical`` dtype as such. Therefore we change the encoder for\nlow-cardinality columns: instead of ``OneHotEncoder``, we use skrub's\n``ToCategorical``. This transformer will simply ensure our columns have an actual\n``Categorical`` dtype (as opposed to string for example), so that they can be\nrecognized by the |HGBR|.\n\nThe second change replaces the |GapEncoder| with a |MinHashEncoder|.\nThe |GapEncoder| is a topic model.\nIt produces interpretable embeddings in a vector space where distances are meaningful,\nwhich is great for interpretation and necessary for some downstream supervised\nlearners such as linear models. However fitting the topic model is costly in\ncomputation time and memory. The |MinHashEncoder| produces features that are not easy\nto interpret, but that decision trees can efficiently use to test for the occurrence\nof particular character n-grams (more details are provided in its documentation).\nTherefore it can be a faster and very effective alternative, when the supervised\nlearner is built on top of decision trees, which is the case for the |HGBR|.\n\nThe resulting pipeline is identical to the one produced by default by\n:obj:`tabular_pipeline`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import MinHashEncoder, ToCategorical\n\nvectorizer = TableVectorizer(\n    low_cardinality=ToCategorical(), high_cardinality=MinHashEncoder()\n)\npipeline = make_pipeline(\n    vectorizer, HistGradientBoostingRegressor(categorical_features=\"from_dtype\")\n)\n\nresults = cross_validate(pipeline, employees, salaries)\nscores = results[\"test_score\"]\nprint(f\"R2 score:  mean: {np.mean(scores):.3f}; std: {np.std(scores):.3f}\")\nprint(f\"mean fit time: {np.mean(results['fit_time']):.3f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that this new pipeline achieves a similar score but is fitted much faster.\nThis is mostly due to replacing |GapEncoder| with |MinHashEncoder| (however this makes\nthe features less interpretable).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature importances in the statistical model\n\nAs we just saw, we can fit a |MinHashEncoder| faster than a |GapEncoder|. However, the\n|GapEncoder| has a crucial advantage: each dimension of its output space is associated\nwith a topic which can be inspected and interpreted.\nIn this section, after training a regressor, we will plot the feature importances.\n\n.. topic:: Note:\n\n  To minimize computation time, we use the feature importances computed by the\n  |RandomForestRegressor|, but you should prefer |permutation importances|\n  instead (which are less subject to biases).\n\nFirst, we train another scikit-learn regressor, the |RandomForestRegressor|:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n\nvectorizer = TableVectorizer()  # now using the default GapEncoder\nregressor = RandomForestRegressor(n_estimators=50, max_depth=20, random_state=0)\n\npipeline = make_pipeline(vectorizer, regressor)\npipeline.fit(employees, salaries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are retrieving the feature importances:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "avg_importances = regressor.feature_importances_\nstd_importances = np.std(\n    [tree.feature_importances_ for tree in regressor.estimators_], axis=0\n)\nindices = np.argsort(avg_importances)[::-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And plotting the results:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\ntop_indices = indices[:20]\nlabels = vectorizer.get_feature_names_out()[top_indices]\n\nplt.figure(figsize=(12, 9))\nplt.barh(\n    y=labels,\n    width=avg_importances[top_indices],\n    xerr=std_importances[top_indices],\n    ecolor=\"k\",\n    color=\"b\",\n    alpha=0.5,\n)\nplt.yticks(fontsize=15)\nplt.title(\"Feature importances\")\nplt.tight_layout(pad=1)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The |GapEncoder| creates feature names that show the first 3 most important words in\nthe topic associated with each feature. As we can see in the plot above, this helps\ninspecting the model. If we had used a |MinHashEncoder| instead, the features would be\nmuch less helpful, with names such as ``employee_position_title_0``,\n``employee_position_title_1``, etc.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that features such the time elapsed since being hired, having a full-time\nemployment, and the position, seem to be the most informative for prediction. However,\nfeature importances must not be over-interpreted -- they capture statistical\nassociations [rather than causal effects](https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation). Moreover, the\nfast feature importance method used here suffers from biases favouring features with\nlarger cardinality, as illustrated in a scikit-learn [example](https://scikit-learn.org/dev/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py).\nIn general we should prefer |permutation importances|, but it is a slower method.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this example, we motivated the need for a simple machine learning\npipeline, which we built using the |TableVectorizer| and a\n|HGBR|.\n\nWe saw that by default, it works well on a heterogeneous dataset.\n\nTo better understand our dataset, and without much effort, we were also able\nto plot the feature importances.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}