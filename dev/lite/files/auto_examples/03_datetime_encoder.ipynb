{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the skrub examples in JupyterLite is experimental and you mayencounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import skrub` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/skrub-data/skrub/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport micropip\nawait micropip.install('https://test-files.pythonhosted.org/packages/3c/03/e1598c7abe536e56834f568f61497ad075d966c4c8fb7d0ad004b81e7bfc/skrub-0.0.1.dev1-py3-none-any.whl')\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Handling datetime features with the DatetimeEncoder\n\nIn this example, we illustrate how to better integrate datetime features\nin machine learning models with the |DatetimeEncoder|.\n\nThis encoder breaks down passed datetime features into relevant numerical\nfeatures, such as the month, the day of the week, the hour of the day, etc.\n\nIt is used by default in the |TableVectorizer|.\n\n\n.. |DatetimeEncoder| replace::\n    :class:`~skrub.DatetimeEncoder`\n\n.. |TableVectorizer| replace::\n    :class:`~skrub.TableVectorizer`\n\n.. |OneHotEncoder| replace::\n    :class:`~sklearn.preprocessing.OneHotEncoder`\n\n.. |TimeSeriesSplit| replace::\n    :class:`~sklearn.model_selection.TimeSeriesSplit`\n\n.. |ColumnTransformer| replace::\n    :class:`~sklearn.compose.ColumnTransformer`\n\n.. |make_column_transformer| replace::\n    :class:`~sklearn.compose.make_column_transformer`\n\n.. |HGBR| replace::\n    :class:`~sklearn.ensemble.HistGradientBoostingRegressor`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nwarnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A problem with relevant datetime features\n\nWe will use a dataset of air quality measurements in different cities.\nIn this setting, we want to predict the NO2 air concentration, based\non the location, date and time of measurement.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\ndata = pd.read_csv(\n    \"https://raw.githubusercontent.com/pandas-dev/pandas\"\n    \"/main/doc/data/air_quality_no2_long.csv\"\n)\n# Extract our input data (X) and the target column (y)\ny = data[\"value\"]\nX = data[[\"city\", \"date.utc\"]]\n\nX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding the features\n\nWe will construct a |ColumnTransformer| in which we will encode\nthe city names with a |OneHotEncoder|, and the date\nwith a |DatetimeEncoder|.\n\nDuring the instantiation of the |DatetimeEncoder|, we specify that we want\nto extract the day of the week, and that we don't want to extract anything\nfiner than minutes. This is because we don't want to extract seconds and\nlower units, as they are probably unimportant.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n\nfrom skrub import DatetimeEncoder\n\nfrom sklearn.compose import make_column_transformer\n\nencoder = make_column_transformer(\n    (OneHotEncoder(handle_unknown=\"ignore\"), [\"city\"]),\n    (DatetimeEncoder(add_day_of_the_week=True, extract_until=\"minute\"), [\"date.utc\"]),\n    remainder=\"drop\",\n)\n\nX_enc = encoder.fit_transform(X)\nencoder.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the encoder is working as expected: the \"date.utc\" column has\nbeen replaced by features extracting the month, day, hour, and day of the\nweek information.\n\nNote the year and minute features are not present, this is because they\nhave been removed by the encoder as they are constant the whole period.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### One-liner with the |TableVectorizer|\n\nAs mentioned earlier, the |TableVectorizer| makes use of the\n|DatetimeEncoder| by default.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import TableVectorizer\nfrom pprint import pprint\n\nfrom skrub import TableVectorizer\n\ntable_vec = TableVectorizer()\ntable_vec.fit_transform(X)\npprint(table_vec.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to customize the |DatetimeEncoder| inside the |TableVectorizer|,\nwe can replace its default parameter with a new, custom instance:\n\nHere, for example, we want it to extract the day of the week.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "table_vec = TableVectorizer(\n    datetime_transformer=DatetimeEncoder(add_day_of_the_week=True),\n)\ntable_vec.fit_transform(X)\ntable_vec.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. note:\n    For more information on how to customize the |TableVectorizer|, see\n    `sphx_glr_auto_examples_01_dirty_categories.py`.\n\nInspecting the |TableVectorizer| further, we can check that the\n|DatetimeEncoder| is used on the correct column(s).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pprint(table_vec.transformers_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction with datetime features\n\nFor prediction tasks, we recommend using the |TableVectorizer| inside a\npipeline, combined with a model that can use the features extracted by the\n|DatetimeEncoder|.\nHere's we'll use a |HGBR| as our learner.\n\n.. note:\n   You might need to require the experimental feature for scikit-learn\n   versions earlier than 1.0 with:\n   ```py\n   from sklearn.experimental import enable_hist_gradient_boosting\n   ```\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\n\ntable_vec = TableVectorizer(\n    datetime_transformer=DatetimeEncoder(add_day_of_the_week=True),\n)\npipeline = make_pipeline(table_vec, HistGradientBoostingRegressor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the model\n\nWhen using date and time features, we often care about predicting the future.\nIn this case, we have to be careful when evaluating our model, because\nthe standard settings of the cross-validation do not respect time ordering.\n\nInstead, we can use the |TimeSeriesSplit|,\nwhich ensures that the test set is always in the future.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X[\"date.utc\"] = pd.to_datetime(X[\"date.utc\"])\nsorted_indices = np.argsort(X[\"date.utc\"])\nX = X.iloc[sorted_indices]\ny = y.iloc[sorted_indices]\n\nfrom sklearn.model_selection import TimeSeriesSplit, cross_val_score\n\ncross_val_score(\n    pipeline,\n    X,\n    y,\n    scoring=\"neg_mean_squared_error\",\n    cv=TimeSeriesSplit(n_splits=5),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting the prediction\n\nThe mean squared error is not obvious to interpret, so we compare\nvisually the prediction of our model with the actual values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom matplotlib.dates import ConciseDateFormatter\n\nX_train = X[X[\"date.utc\"] < \"2019-06-01\"]\nX_test = X[X[\"date.utc\"] >= \"2019-06-01\"]\n\ny_train = y[X[\"date.utc\"] < \"2019-06-01\"]\ny_test = y[X[\"date.utc\"] >= \"2019-06-01\"]\n\npipeline.fit(X_train, y_train)\n\nall_cities = X_test[\"city\"].unique()\n\nfig, axs = plt.subplots(nrows=len(all_cities), ncols=1, figsize=(12, 9))\n\nfor i, city in enumerate(all_cities):\n    axs[i].plot(\n        X.loc[X.city == city, \"date.utc\"],\n        y.loc[X.city == city],\n        label=\"Actual\",\n    )\n    axs[i].plot(\n        X_test.loc[X_test.city == city, \"date.utc\"],\n        pipeline.predict(X_test.loc[X_test.city == city]),\n        label=\"Predicted\",\n    )\n    axs[i].set_title(city)\n    axs[i].set_ylabel(\"NO2\")\n    axs[i].xaxis.set_major_formatter(\n        ConciseDateFormatter(axs[i].xaxis.get_major_locator())\n    )\n    axs[i].legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's zoom on a few days:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_zoomed = X[X[\"date.utc\"] <= \"2019-06-04\"][X[\"date.utc\"] >= \"2019-06-01\"]\ny_zoomed = y[X[\"date.utc\"] <= \"2019-06-04\"][X[\"date.utc\"] >= \"2019-06-01\"]\n\nX_train_zoomed = X_zoomed[X_zoomed[\"date.utc\"] < \"2019-06-03\"]\nX_test_zoomed = X_zoomed[X_zoomed[\"date.utc\"] >= \"2019-06-03\"]\n\ny_train_zoomed = y[X[\"date.utc\"] < \"2019-06-03\"]\ny_test_zoomed = y[X[\"date.utc\"] >= \"2019-06-03\"]\n\nzoomed_cities = X_test_zoomed[\"city\"].unique()\n\nfig, axs = plt.subplots(nrows=len(zoomed_cities), ncols=1, figsize=(12, 9))\n\nfor i, city in enumerate(zoomed_cities):\n    axs[i].plot(\n        X_zoomed.loc[X_zoomed[\"city\"] == city, \"date.utc\"],\n        y_zoomed.loc[X_zoomed[\"city\"] == city],\n        label=\"Actual\",\n    )\n    axs[i].plot(\n        X_test_zoomed.loc[X_test_zoomed[\"city\"] == city, \"date.utc\"],\n        pipeline.predict(X_test_zoomed.loc[X_test_zoomed[\"city\"] == city]),\n        label=\"Predicted\",\n    )\n    axs[i].set_title(city)\n    axs[i].set_ylabel(\"NO2\")\n    axs[i].xaxis.set_major_formatter(\n        ConciseDateFormatter(axs[i].xaxis.get_major_locator())\n    )\n    axs[i].legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Features importance\n\nUsing the |DatetimeEncoder| allows us to better understand how the date\nimpacts the NO2 concentration. To this aim, we can compute the\nimportance of the features created by the |DatetimeEncoder|, using the\n:func:`~sklearn.inspection.permutation_importance` function, which\nbasically shuffles a feature and sees how the model changes its prediction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n\ntable_vec = TableVectorizer(\n    datetime_transformer=DatetimeEncoder(add_day_of_the_week=True),\n)\n\n# In this case, we don't use a pipeline, because we want to compute the\n# importance of the features created by the DatetimeEncoder\nX_ = table_vec.fit_transform(X)\nreg = HistGradientBoostingRegressor().fit(X_, y)\nresult = permutation_importance(reg, X_, y, n_repeats=10, random_state=0)\nstd = result.importances_std\nimportances = result.importances_mean\nindices = np.argsort(importances)\n# Sort from least to most\nindices = list(reversed(indices))\n\nplt.figure(figsize=(12, 9))\nplt.title(\"Feature importances\")\nn = len(indices)\nlabels = np.array(table_vec.get_feature_names_out())[indices]\nplt.barh(range(n), importances[indices], color=\"b\", yerr=std[indices])\nplt.yticks(range(n), labels, size=15)\nplt.tight_layout(pad=1)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the hour of the day is the most important feature,\nwhich seems reasonable.\n\n## Conclusion\n\nIn this example, we saw how to use the |DatetimeEncoder| to create\nfeatures from a date column.\nAlso check out the |TableVectorizer|, which automatically recognizes\nand transforms datetime columns by default.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}