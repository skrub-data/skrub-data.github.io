{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the skrub examples in JupyterLite is experimental and you mayencounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import skrub` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/skrub-data/skrub/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport micropip\nawait micropip.install('skrub')\n%pip install pyodide-http\nimport pyodide_http\npyodide_http.patch_all()\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Handling datetime features with the DatetimeEncoder\n\nIn this example, we illustrate how to better integrate datetime features\nin machine learning models with the |DatetimeEncoder|.\n\nThis encoder breaks down passed datetime features into relevant numerical\nfeatures, such as the month, the day of the week, the hour of the day, etc.\n\nIt is used by default in the |TableVectorizer|.\n\n\n.. |DatetimeEncoder| replace::\n    :class:`~skrub.DatetimeEncoder`\n\n.. |TableVectorizer| replace::\n    :class:`~skrub.TableVectorizer`\n\n.. |OneHotEncoder| replace::\n    :class:`~sklearn.preprocessing.OneHotEncoder`\n\n.. |TimeSeriesSplit| replace::\n    :class:`~sklearn.model_selection.TimeSeriesSplit`\n\n.. |ColumnTransformer| replace::\n    :class:`~sklearn.compose.ColumnTransformer`\n\n.. |make_column_transformer| replace::\n    :class:`~sklearn.compose.make_column_transformer`\n\n.. |RidgeCV| replace::\n    :class:`~sklearn.linear_model.RidgeCV`\n\n.. |SimpleImputer| replace::\n    :class:`~sklearn.impute.SimpleImputer`\n\n.. |StandardScaler| replace::\n    :class:`~sklearn.preprocessing.StandardScaler`\n\n.. |ToDatetime| replace::\n    :class:`~skrub.ToDatetime`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A problem with relevant datetime features\n\nWe will use a dataset of bike sharing demand in 2011 and 2012.\nIn this setting, we want to predict the number of bike rentals, based\non the date, time and weather conditions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n\nimport pandas as pd\n\nfrom skrub import datasets\n\ndata = datasets.fetch_bike_sharing().bike_sharing\n\n# Extract our input data (X) and the target column (y)\ny = data[\"cnt\"]\nX = data[[\"date\", \"holiday\", \"temp\", \"hum\", \"windspeed\", \"weathersit\"]]\n\nX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We convert the dataframe's ``\"date\"`` column using |ToDatetime|.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import ToDatetime\n\ndate = ToDatetime().fit_transform(X[\"date\"])\n\nprint(\"original dtype:\", X[\"date\"].dtypes, \"\\n\\nconverted dtype:\", date.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoding the features\n\nWe now encode this column with a |DatetimeEncoder|.\n\nDuring the instantiation of the |DatetimeEncoder|, we specify that we want\ndon't want to extract features with a resolution finer than hours. This is\nbecause we don't want to extract minutes, seconds and lower units, as they\nare unimportant here.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import DatetimeEncoder\n\n# DatetimeEncoder has \"hour\" as default resolution\ndate_enc = DatetimeEncoder().fit_transform(date)\n\nprint(date, \"\\n\\nHas been encoded as:\\n\\n\", date_enc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the encoder is working as expected: the column has\nbeen replaced by features extracting the month, day, hour, day of the\nweek and total seconds since Epoch information.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### One-liner with the TableVectorizer\n\nAs mentioned earlier, the |TableVectorizer| makes use of the\n|DatetimeEncoder| by default. Note that ``X[\"date\"]`` is still\na string, but will be automatically transformed into a datetime in the\n|TableVectorizer|.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import TableVectorizer\n\ntable_vec = TableVectorizer().fit(X)\npprint(table_vec.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we want to customize the |DatetimeEncoder| inside the |TableVectorizer|,\nwe can replace its default parameter with a new, custom instance.\n\nHere, for example, we want it to extract the day of the week:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# use the ``datetime`` argument to use a custom DatetimeEncoder in the TableVectorizer\ntable_vec_weekday = TableVectorizer(datetime=DatetimeEncoder(add_weekday=True)).fit(X)\npprint(table_vec_weekday.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".. note:\n    For more information on how to customize the |TableVectorizer|, see\n    `sphx_glr_auto_examples_01_dirty_categories.py`.\n\nInspecting the |TableVectorizer| further, we can check that the\n|DatetimeEncoder| is used on the correct column(s).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pprint(table_vec_weekday.transformers_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature engineering for linear models\n\nThe |DatetimeEncoder| can generate additional periodic features. These are\nparticularly useful for linear models. This is controlled by the\n``periodic encoding`` parameter which can be either  ``circular`` or ``spline``,\nfor trigonometric functions or B-Splines respectively. In this example, we use\n``spline``.\nWe can also add the day in the year with the parameter ``add_day_of_year``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "table_vec_periodic = TableVectorizer(\n    datetime=DatetimeEncoder(\n        add_weekday=True, periodic_encoding=\"spline\", add_day_of_year=True\n    )\n).fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction with datetime features\n\nFor prediction tasks, we recommend using the |TableVectorizer| inside a\npipeline, combined with a model that can use the features extracted by the\n|DatetimeEncoder|.\nHere we'll use a |RidgeCV| model as our learner. We also fill null values with\n|SimpleImputer| and then rescale numeric features with |StandardScaler|.\nTo test the effect of different datetime encodings on the linear model, we train\nthree separate pipelines.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Base pipeline with default DatetimeEncoder parameters\npipeline = make_pipeline(table_vec, StandardScaler(), SimpleImputer(), RidgeCV())\n# Datetime encoder with weekday feature\npipeline_weekday = make_pipeline(\n    table_vec_weekday, StandardScaler(), SimpleImputer(), RidgeCV()\n)\n# Datetime encoder with periodic features\npipeline_periodic = make_pipeline(\n    table_vec_periodic, StandardScaler(), SimpleImputer(), RidgeCV()\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the model\n\nWhen using date and time features, we often care about predicting the future.\nIn this case, we have to be careful when evaluating our model, because\nthe standard settings of the cross-validation do not respect time ordering.\n\nInstead, we can use the |TimeSeriesSplit|,\nwhich ensures that the test set is always in the future.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n\nscore_base = cross_val_score(\n    pipeline,\n    X,\n    y,\n    scoring=\"neg_root_mean_squared_error\",\n    cv=TimeSeriesSplit(n_splits=5),\n)\n\nscore_weekday = cross_val_score(\n    pipeline_weekday,\n    X,\n    y,\n    scoring=\"neg_root_mean_squared_error\",\n    cv=TimeSeriesSplit(n_splits=5),\n)\n\nscore_periodic = cross_val_score(\n    pipeline_periodic,\n    X,\n    y,\n    scoring=\"neg_root_mean_squared_error\",\n    cv=TimeSeriesSplit(n_splits=5),\n)\n\nprint(f\"Base transformer - Mean RMSE : {-score_base.mean():.2f}\")\nprint(f\"Transformer with weekday -  Mean RMSE : {-score_weekday.mean():.2f}\")\nprint(f\"Transformer with periodic features - Mean RMSE : {-score_periodic.mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected for linear models, introducing the periodic features improved\nthe RMSE by a noticeable amount.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting the prediction\n\nThe mean squared error is not obvious to interpret, so we visually\ncompare the prediction of our model with the actual values.\nTo do so, we will divide our dataset into a train and a test set:\nwe use 2011 data to predict what happened in 2012.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.dates as mdates\nimport matplotlib.pyplot as plt\n\nmask_train = X[\"date\"] < \"2012-01-01\"\nX_train, X_test = X.loc[mask_train], X.loc[~mask_train]\ny_train, y_test = y.loc[mask_train], y.loc[~mask_train]\n\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\n\npipeline_weekday.fit(X_train, y_train)\ny_pred_weekday = pipeline_weekday.predict(X_test)\n\npipeline_periodic.fit(X_train, y_train)\ny_pred_periodic = pipeline_periodic.predict(X_test)\n\nX_plot = pd.to_datetime(X.tail(96)[\"date\"]).values\nX_test_plot = pd.to_datetime(X_test.tail(96)[\"date\"]).values\n\nfig, ax = plt.subplots(figsize=(12, 3))\nfig.suptitle(\"Predictions with linear models\")\nax.plot(\n    X_plot,\n    y.tail(96).values,\n    \"x-\",\n    alpha=0.2,\n    label=\"Actual demand\",\n    color=\"black\",\n)\nax.plot(\n    X_test_plot,\n    y_pred[-96:],\n    \"x-\",\n    label=\"DatetimeEncoder() + RidgeCV prediction\",\n)\nax.plot(\n    X_test_plot,\n    y_pred_periodic[-96:],\n    \"x-\",\n    label='DatetimeEncoder(periodic_encoding=\"spline\") + RidgeCV prediction',\n)\n\n\nax.xaxis.set_major_locator(mdates.DayLocator())\nax.xaxis.set_minor_locator(\n    mdates.HourLocator(\n        [0, 6, 12, 18],\n    )\n)\n\n# Major formatter: format date as \"YYYY-MM-DD\"\nax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n# # Minor formatter: format time as \"HH:MM\"\nax.xaxis.set_minor_formatter(mdates.DateFormatter(\"%H:%M\"))\n\nax.tick_params(axis=\"x\", labelsize=7, labelrotation=75)\nax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\"))\n_ = fig.legend(loc=\"upper left\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##############################################################################\n As we can see, the base RidgeCV model struggles to learn the the pattern well\n enough, while the model that is trained on the additional periodic features\n follows the actual demand more accurately.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature importances\n\nUsing the |DatetimeEncoder| allows us to better understand how the date\nimpacts the bike sharing demand. To this aim, we can use the function\n:func:`~sklearn.inspection.permutation_importance` to shuffle the features\ncreated by the |DatetimeEncoder| and measure their importance by observing\nhow the model changes its prediction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n\n# In this case, we don't use the whole pipeline, because we want to compute the\n# importance of the features created by the DatetimeEncoder\nX_test_transform = pipeline_periodic[:-1].transform(X_test)\nresult = permutation_importance(\n    pipeline_periodic[-1], X_test_transform, y_test, n_repeats=10, random_state=0\n)\n\nresult = pd.DataFrame(\n    dict(\n        feature_names=pipeline_periodic[0].all_outputs_,\n        std=result.importances_std,\n        importances=result.importances_mean,\n    )\n).sort_values(\"importances\", ascending=True)\n\nresult.plot.barh(\n    y=\"importances\",\n    x=\"feature_names\",\n    title=\"Feature Importances\",\n    xerr=\"std\",\n    figsize=(12, 9),\n)\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can clearly see that some of the hour splines (``date_hour_spline_18``,\n``date_hour_spline_9``) are more important than other features, likely due to\nthe fact that they match rush hours in the day. Other features, such as the\ntemperature, the month, and the humidity are more important than others.\n\n## Conclusion\n\nIn this example, we saw how to use the |DatetimeEncoder| to create\nfeatures from a datetime column.\nAlso check out the |TableVectorizer|, which automatically recognizes\nand transforms datetime columns by default.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}