{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the skrub examples in JupyterLite is experimental and you mayencounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import skrub` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/skrub-data/skrub/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport micropip\nawait micropip.install('skrub')\n%pip install seaborn\n%pip install pyodide-http\nimport pyodide_http\npyodide_http.patch_all()\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Wikipedia embeddings to enrich the data\n\nWhen the data comprises common entities (cities,\ncompanies or famous people), bringing new information assembled from external\nsources may be the key to improving the analysis.\n\nEmbeddings, or vectorial representations of entities, are a conveniant way to\ncapture and summarize the information on an entity.\nRelational data embeddings capture all common entities from Wikipedia. [#]_\nThese will be called `KEN embeddings` in the following example.\n\nWe will see that these embeddings of common entities significantly\nimprove our results.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This example requires `pyarrow` to be installed.</p></div>\n\n.. [#] https://soda-inria.github.io/ken_embeddings/\n\n\n .. |Pipeline| replace::\n     :class:`~sklearn.pipeline.Pipeline`\n\n .. |OneHotEncoder| replace::\n     :class:`~sklearn.preprocessing.OneHotEncoder`\n\n .. |ColumnTransformer| replace::\n     :class:`~sklearn.compose.ColumnTransformer`\n\n .. |MinHash| replace::\n     :class:`~skrub.MinHashEncoder`\n\n .. |HGBR| replace::\n     :class:`~sklearn.ensemble.HistGradientBoostingRegressor`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The data\n\nWe will take a look at the video game sales dataset.\nLet's retrieve the dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\nX = pd.read_csv(\n    \"https://raw.githubusercontent.com/William2064888/vgsales.csv/main/vgsales.csv\",\n    sep=\";\",\n    on_bad_lines=\"skip\",\n)\n# Shuffle the data\nX = X.sample(frac=1, random_state=11, ignore_index=True)\nX.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our goal will be to predict the sales amount (y, our target column):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y = X[\"Global_Sales\"]\ny"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at the distribution of our target variable:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(style=\"ticks\")\n\nsns.histplot(y)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems better to take the log of sales rather than the absolute values:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\ny = np.log(y)\nsns.histplot(y)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before moving further, let's carry out some basic preprocessing:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get a mask of the rows with missing values in \"Publisher\" and \"Global_Sales\"\nmask = X.isna()[\"Publisher\"] | X.isna()[\"Global_Sales\"]\n# And remove them\nX.dropna(subset=[\"Publisher\", \"Global_Sales\"], inplace=True)\ny = y[~mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extracting entity embeddings\n\nWe will use KEN embeddings to enrich our data.\n\nWe will start by checking out the available tables with\n:class:`~skrub.datasets.fetch_ken_table_aliases`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub.datasets import fetch_ken_table_aliases\n\nfetch_ken_table_aliases()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The *games* table is the most relevant to our case.\nLet's see what kind of types we can find in it with the function\n:class:`~skrub.datasets.fetch_ken_types`:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub.datasets import fetch_ken_types\n\nfetch_ken_types(embedding_table_id=\"games\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interesting, we have a broad range of topics!\n\nNext, we'll use :class:`~skrub.datasets.fetch_ken_embeddings`\nto extract the embeddings of entities we need:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub.datasets import fetch_ken_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "KEN Embeddings are classified by types.\nSee the example on :class:`~skrub.datasets.fetch_ken_embeddings`\nto understand how you can filter types you are interested in.\n\nThe :class:`~skrub.datasets.fetch_ken_embeddings` function\nallows us to specify the types to be included and/or excluded\nso as not to load all Wikipedia entity embeddings in a table.\n\n\nIn a first table, we include all embeddings with the type name \"game\"\nand exclude those with type name \"companies\" or \"developer\".\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "embedding_games = fetch_ken_embeddings(\n    search_types=\"game\",\n    exclude=\"companies|developer\",\n    embedding_table_id=\"games\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a second table, we include all embeddings containing the type name\n\"game_development_companies\", \"game_companies\" or \"game_publish\":\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "embedding_publisher = fetch_ken_embeddings(\n    search_types=\"game_development_companies|game_companies|game_publish\",\n    embedding_table_id=\"games\",\n)\n\n# We keep the 200 embeddings column names in a list (for the |Pipeline|):\nn_dim = 200\n\nemb_columns = [f\"X{j}\" for j in range(n_dim)]\n\nemb_columns2 = [f\"X{j}_aux\" for j in range(n_dim)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merging the entities\n\nWe will now merge the entities from Wikipedia with their equivalent match\nin our video game sales table:\n\nThe entities from the 'embedding_games' table will be merged along the\ncolumn \"Name\" and the ones from 'embedding_publisher' table with the\ncolumn \"Publisher\"\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import Joiner\n\nfa1 = Joiner(embedding_games, aux_key=\"Entity\", main_key=\"Name\")\nfa2 = Joiner(embedding_publisher, aux_key=\"Entity\", main_key=\"Publisher\", suffix=\"_aux\")\n\nX_full = fa1.fit_transform(X)\nX_full = fa2.fit_transform(X_full)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction with base features\n\nWe will forget for now the KEN Embeddings and build a typical learning\npipeline, where will we try to predict the amount of sales only using\nthe base features contained in the initial table.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first use scikit-learn's |ColumnTransformer| to define the columns\nthat will be included in the learning process and the appropriate encoding of\ncategorical variables using the |MinHash| and |OneHotEncoder|:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom skrub import MinHashEncoder\n\nmin_hash = MinHashEncoder(n_components=100)\nohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n\nencoder = make_column_transformer(\n    (\"passthrough\", [\"Year\"]),\n    (ohe, [\"Genre\"]),\n    (min_hash, \"Platform\"),\n    remainder=\"drop\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We incorporate our |ColumnTransformer| into a |Pipeline|.\nWe define a predictor, |HGBR|, fast and reliable for big datasets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\n\nhgb = HistGradientBoostingRegressor(random_state=0)\npipeline = make_pipeline(encoder, hgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The |Pipeline| can now be readily applied to the dataframe for prediction:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_validate\n\n# We will save the results in a dictionnary:\nall_r2_scores = dict()\nall_rmse_scores = dict()\n\ncv_results = cross_validate(\n    pipeline, X_full, y, scoring=[\"r2\", \"neg_root_mean_squared_error\"]\n)\n\nall_r2_scores[\"Base features\"] = cv_results[\"test_r2\"]\nall_rmse_scores[\"Base features\"] = -cv_results[\"test_neg_root_mean_squared_error\"]\n\nprint(\"With base features:\")\nprint(\n    f\"Mean R2 is {all_r2_scores['Base features'].mean():.2f} +-\"\n    f\" {all_r2_scores['Base features'].std():.2f} and the RMSE is\"\n    f\" {all_rmse_scores['Base features'].mean():.2f} +-\"\n    f\" {all_rmse_scores['Base features'].std():.2f}\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction with KEN Embeddings\n\nWe will now build a second learning pipeline using only the KEN embeddings\nfrom Wikipedia.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We keep only the embeddings columns:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "encoder2 = make_column_transformer(\n    (\"passthrough\", emb_columns), (\"passthrough\", emb_columns2), remainder=\"drop\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We redefine the |Pipeline|:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline2 = make_pipeline(encoder2, hgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the results:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cv_results = cross_validate(\n    pipeline2, X_full, y, scoring=[\"r2\", \"neg_root_mean_squared_error\"]\n)\n\nall_r2_scores[\"KEN features\"] = cv_results[\"test_r2\"]\nall_rmse_scores[\"KEN features\"] = -cv_results[\"test_neg_root_mean_squared_error\"]\n\nprint(\"With KEN Embeddings:\")\nprint(\n    f\"Mean R2 is {all_r2_scores['KEN features'].mean():.2f} +-\"\n    f\" {all_r2_scores['KEN features'].std():.2f} and the RMSE is\"\n    f\" {all_rmse_scores['KEN features'].mean():.2f} +-\"\n    f\" {all_rmse_scores['KEN features'].std():.2f}\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It seems including the embeddings is very relevant for the prediction task\nat hand!\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction with KEN Embeddings and base features\n\nAs we have seen the predictions scores in the case when embeddings are\nonly present and when they are missing, we will do a final prediction\nwith all variables included.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We include both the embeddings and the base features:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "encoder3 = make_column_transformer(\n    (\"passthrough\", emb_columns),\n    (\"passthrough\", emb_columns2),\n    (\"passthrough\", [\"Year\"]),\n    (ohe, [\"Genre\"]),\n    (min_hash, \"Platform\"),\n    remainder=\"drop\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We redefine the |Pipeline|:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline3 = make_pipeline(encoder3, hgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the results:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cv_results = cross_validate(\n    pipeline3, X_full, y, scoring=[\"r2\", \"neg_root_mean_squared_error\"]\n)\n\nall_r2_scores[\"Base + KEN features\"] = cv_results[\"test_r2\"]\nall_rmse_scores[\"Base + KEN features\"] = -cv_results[\"test_neg_root_mean_squared_error\"]\n\nprint(\"With KEN Embeddings and base features:\")\nprint(\n    f\"Mean R2 is {all_r2_scores['Base + KEN features'].mean():.2f} +-\"\n    f\" {all_r2_scores['Base + KEN features'].std():.2f} and the RMSE is\"\n    f\" {all_rmse_scores['Base + KEN features'].mean():.2f} +-\"\n    f\" {all_rmse_scores['Base + KEN features'].std():.2f}\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting the results\n\nFinally, we plot the scores on a boxplot:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(5, 3))\n# sphinx_gallery_thumbnail_number = -1\nax = sns.boxplot(data=pd.DataFrame(all_r2_scores), orient=\"h\")\nplt.xlabel(\"Prediction accuracy     \", size=15)\nplt.yticks(size=15)\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is a clear improvement when including the KEN embeddings among the\nexplanatory variables.\n\nIn this case, the embeddings from Wikipedia introduced\nadditional background information on the game and the publisher of the\ngame that would otherwise be missed.\n\nIt helped significantly improve the prediction score.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}