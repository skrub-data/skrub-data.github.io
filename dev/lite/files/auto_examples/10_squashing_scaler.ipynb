{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the skrub examples in JupyterLite is experimental and you mayencounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import skrub` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/skrub-data/skrub/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport micropip\nawait micropip.install('skrub')\n%pip install pyodide-http\nimport pyodide_http\npyodide_http.patch_all()\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# SquashingScaler: Robust numerical preprocessing for neural networks\n\nThe following example illustrates the use of the :class:`~skrub.SquashingScaler`, a\ntransformer that can rescale and squash numerical features to a range that works well\nwith neural networks and perhaps also other related models. Its basic idea is to\nrescale the features based on quantile statistics (to be robust to outliers), and then\nperform a smooth squashing function to limit the outputs to a pre-defined range.\nThis transform has been found to even work well when applied to one-hot encoded\nfeatures.\n\nWe first generate some synthetic data with outliers to show how different scalers\ntransform the data, then we show how the choice of the scaler affects the prediction\nperformance of a simple neural network.\n\n.. |SquashingScaler| replace:: :class:`~skrub.SquashingScaler`\n.. |RobustScaler| replace:: :class:`~sklearn.preprocessing.RobustScaler`\n.. |StandardScaler| replace:: :class:`~sklearn.preprocessing.StandardScaler`\n.. |QuantileTransformer| replace:: :class:`~sklearn.preprocessing.QuantileTransformer`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting the effect of different scalers\n\nFirst, let's import the |SquashingScaler|, as well as the usual scikit-learn\n|StandardScaler| and |RobustScaler|.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.preprocessing import QuantileTransformer, RobustScaler, StandardScaler\n\nfrom skrub import SquashingScaler\n\nnp.random.seed(0)  # for reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then generate some random values sampling from a uniform distribution in the\nrange ``[0, 1]``: note that this will produce values that are always positive.\nWe then add some outliers in random positions in the array.\nSubtracting 50 allows to have some negative outliers in the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "values = np.random.rand(100, 1)\nn_outliers = 15\noutlier_indices = np.random.choice(values.shape[0], size=n_outliers, replace=False)\nvalues[outlier_indices] = np.random.rand(n_outliers, 1) * 100 - 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then create one of each scaler and use them to scale the data independently.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "squash_scaler = SquashingScaler()\nsquash_scaled = squash_scaler.fit_transform(values)\n\nrobust_scaler = RobustScaler()\nrobust_scaled = robust_scaler.fit_transform(values)\n\nstandard_scaler = StandardScaler()\nstandard_scaled = standard_scaler.fit_transform(values)\n\nquantile_transformer = QuantileTransformer(n_quantiles=100)\nquantile_scaled = quantile_transformer.fit_transform(values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To better show the effect of scaling, we create two plots, where we display the\ndata points after sorting them in ascending order: in this way, all outliers\nare close to each other and with the proper sign.\nWe create two subplots because the scale of the outliers is much larger than that\nof the inliers, which means that any detail in the inlier would be hidden.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nx = np.arange(values.shape[0])\n\nfig, axs = plt.subplots(1, 2, layout=\"constrained\", figsize=(10, 5))\n\nax = axs[0]\nax.plot(x, sorted(values), label=\"Original Values\", linewidth=2.5)\nax.plot(x, sorted(squash_scaled), label=\"SquashingScaler\")\nax.plot(x, sorted(robust_scaled), label=\"RobustScaler\", linestyle=\"--\")\nax.plot(x, sorted(standard_scaled), label=\"StandardScaler\")\nax.plot(x, sorted(quantile_scaled), label=\"QuantileTransformer\")\n\n# Add a horizontal band in [-4, +4]\nax.axhspan(-4, 4, color=\"gray\", alpha=0.15)\nax.set(title=\"Original data\", xlim=[0, values.shape[0]], xlabel=\"Percentile\")\nax.legend()\n\nax = axs[1]\nax.plot(x, sorted(values), label=\"Original Values\", linewidth=2.5)\nax.plot(x, sorted(squash_scaled), label=\"SquashingScaler\")\nax.plot(x, sorted(robust_scaled), label=\"RobustScaler\", linestyle=\"--\")\nax.plot(x, sorted(standard_scaled), label=\"StandardScaler\")\nax.plot(x, sorted(quantile_scaled), label=\"QuantileTransformer\")\n\nax.set(ylim=[-4, 4])\nax.set(title=\"In range [-4, 4]\", xlim=[0, values.shape[0]], xlabel=\"Percentile\")\n\n# Highlight the bounds of the SquashingScaler\nax.axhline(y=3, alpha=0.2)\nax.axhline(y=-3, alpha=0.2)\n\nfig.suptitle(\n    \"Comparison of different scalers on sorted data with outliers\", fontsize=20\n)\nfig.supylabel(\"Value\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The figure on the left immediately shows how the scale of the data may be completely\noff because of a minority of outliers, with the RobustScaler following the behavior\nof the original by retaining the larger scale of the outliers. On the other hand,\nboth the SquashingScaler and the StandardScaler remain roughly in the ``[-4, 4]``\nrange (highlighted in grey in the left figure).\n\nIn the right figure we can then spot how the presence of outliers has completely\nflattened the curve produced by the StandardScaler, forcing the inliers to be\nvery close to 0. The RobustScaler and the SquashingScaler instead follow the original\ndata much more closely, after centering it on 0.\n\nFinally, the SquashingScaler performs a smooth clipping of outliers, constraining\nall values to be in the range ``[-max_absolute_value, max_absolute_value]``,\nwhere ``max_absolute_value`` is a parameter specified by the user (3 by default).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing numerical pre-processing methods on a neural network\n\nIn the second part of the example, we want to fit a neural network to predict\nemployee salaries.\nThe dataset contains numerical features, categorical features, text features,\nand dates.\nThese features are first converted to numerical features using\n:class:`~skrub.TableVectorizer`. Since the encoded features are not normalized,\nwe apply a numerical transformation to them.\n\nFinally, we fit a simple neural network and compare the R2 scores obtained with\ndifferent numerical transformations.\n\nWhile we use a simple :class:`~sklearn.neural_network.MLPRegressor` here for\nsimplicity, we generally recommend using better neural network implementations\nor tree-based models whenever low test errors are desired.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We test the :class:`~skrub.SquashingScaler` against the\n:class:`~sklearn.preprocessing.StandardScaler` and the\n:class:`~sklearn.preprocessing.QuantileTransformer` from scikit-learn. We put each of\nthese together in a pipeline with a TableVectorizer and a simple MLPRegressor. In the\nend, we print the R2 scores of each fold's validation set in a three-fold\ncross-validation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nimport numpy as np\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler\n\nfrom skrub import DatetimeEncoder, SquashingScaler, TableVectorizer\nfrom skrub.datasets import fetch_employee_salaries\n\nnp.random.seed(0)\ndata = fetch_employee_salaries()\n\nfor num_transformer in [\n    StandardScaler(),\n    QuantileTransformer(output_distribution=\"normal\", random_state=0),\n    SquashingScaler(),\n]:\n    pipeline = make_pipeline(\n        TableVectorizer(datetime=DatetimeEncoder(periodic_encoding=\"circular\")),\n        num_transformer,\n        TransformedTargetRegressor(\n            # We use lbfgs for faster convergence\n            MLPRegressor(solver=\"lbfgs\", max_iter=100),\n            transformer=StandardScaler(),\n        ),\n    )\n    with warnings.catch_warnings():\n        # Ignore warnings about the MLPRegressor not converging\n        warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n        scores = cross_validate(pipeline, data.X, data.y, cv=3, scoring=\"r2\")\n\n    print(\n        f\"Cross-validation R2 scores for {num_transformer.__class__.__name__}\"\n        f\" (higher is better):\\n{scores['test_score']}\\n\"\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the employee salaries dataset, the SquashingScaler performs\nbetter than StandardScaler and QuantileTransformer on all\ncross-validation folds.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}