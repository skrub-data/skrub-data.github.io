{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the skrub examples in JupyterLite is experimental and you mayencounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import skrub` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/skrub-data/skrub/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport micropip\nawait micropip.install('skrub')\n%pip install pyodide-http\nimport pyodide_http\npyodide_http.patch_all()\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# SquashingScaler: Robust numerical preprocessing for neural networks\n\nThe following example illustrates the use of the :class:`~skrub.SquashingScaler`, a\ntransformer that can rescale and squash numerical features to a range that works well\nwith neural networks and perhaps also other related models. Its basic idea is to\nrescale the features based on quantile statistics (to be robust to outliers), and then\nperform a smooth squashing function to limit the outputs to a pre-defined range.\nThis transform has been found to even work well when applied to one-hot encoded\nfeatures.\n\nIn this example, we want to fit a neural network to predict employee salaries.\nThe dataset contains numerical features, categorical features, text features, and dates.\nThese features are first converted to numerical features using\n:class:`~skrub.TableVectorizer`. Since the encoded features are not normalized, we apply\na numerical transformation to them.\n\nFinally, we fit a simple neural network and compare the R2 scores obtained with\ndifferent numerical transformations.\n\nWhile we use a simple :class:`~sklearn.neural_network.MLPRegressor` here for simplicity,\nwe generally recommend using better neural network implementations or tree-based models\nwhenever low test errors are desired.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing numerical preprocessings\nWe test the :class:`~skrub.SquashingScaler` against the\n:class:`~sklearn.preprocessing.StandardScaler` and the\n:class:`~sklearn.preprocessing.QuantileTransformer` from scikit-learn. We put each of\nthese together in a pipeline with a TableVectorizer and a simple MLPRegressor. In the\nend, we print the R2 scores of each fold's validation set in a three-fold\ncross-validation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nimport numpy as np\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.exceptions import ConvergenceWarning\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler\n\nfrom skrub import DatetimeEncoder, SquashingScaler, TableVectorizer\nfrom skrub.datasets import fetch_employee_salaries\n\nnp.random.seed(0)\ndata = fetch_employee_salaries()\n\nfor num_transformer in [\n    StandardScaler(),\n    QuantileTransformer(output_distribution=\"normal\", random_state=0),\n    SquashingScaler(),\n]:\n    pipeline = make_pipeline(\n        TableVectorizer(datetime=DatetimeEncoder(periodic_encoding=\"circular\")),\n        num_transformer,\n        TransformedTargetRegressor(\n            # We use lbfgs for faster convergence\n            MLPRegressor(solver=\"lbfgs\", max_iter=100),\n            transformer=StandardScaler(),\n        ),\n    )\n    with warnings.catch_warnings():\n        # Ignore warnings about the MLPRegressor not converging\n        warnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n        scores = cross_validate(pipeline, data.X, data.y, cv=3, scoring=\"r2\")\n\n    print(\n        f\"Cross-validation R2 scores for {num_transformer.__class__.__name__}\"\n        f\" (higher is better):\\n{scores['test_score']}\\n\"\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On the employee salaries dataset, the SquashingScaler performs\nbetter than StandardScaler and QuantileTransformer on all\ncross-validation folds.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}