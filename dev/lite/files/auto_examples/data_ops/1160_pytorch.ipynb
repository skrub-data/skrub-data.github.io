{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the skrub examples in JupyterLite is experimental and you mayencounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import skrub` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/skrub-data/skrub/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport micropip\nawait micropip.install('skrub')\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Using PyTorch (via skorch) in DataOps\n\nThis example shows how to wrap a PyTorch model with skorch and plug it into a\nskrub DataOps plan.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This example requires the optional dependencies ``torch`` and ``skorch``.</p></div>\n\nThe main goal here is to show the *integration pattern*:\n\n- **PyTorch** defines the model (an ``nn.Module``)\n- **skorch** wraps it as a scikit-learn compatible estimator\n- **skrub DataOps** builds a plan and can tune skorch (and therefore PyTorch)\n  hyperparameters using the skrub choices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading the data\n\nWe use scikit-learn's digits dataset because it is small and ships with\nscikit-learn. Each sample is an 8x8 grayscale image of a\nhandwritten digit, encoded as 64 pixel intensity values and displays a\nnumber from 0 to 9.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n\ndigits = load_digits()\nX, y = digits.data, digits.target\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Number of classes: {len(set(y))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Start of the DataOps plan\n\nWe start the DataOps plan by creating the skrub variables X and y.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import skrub\n\nX = skrub.X(X)\ny = skrub.y(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data preprocessing\n\nWe start by normalizing the pixel values to [0, 1] by first\ncomputing the global max value and then dividing the pixel values\nby this max value. Importantly, we freeze the max value (scaling factor)\nafter fitting so that the same rescaling is applied later when we use our\ndataop for prediction on new (test) data.\n\nA convolutional network expects images with shape (N, C, H, W) where:\n\n- N: number of samples\n- C: number of color channels (1 for grayscale)\n- H, W: image height and width\n\nSo we reshape the images to (N, 1, 8, 8) for the CNN. The -1 means the first\ndimension (N) is inferred automatically from the array size.\n\nThe advantage of using DataOps is that the preprocessing steps are tracked\nin the plan and will be automatically applied during prediction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "max_value = X.max().skb.freeze_after_fit()\nX_scaled = X / max_value\nX_reshaped = X_scaled.reshape(-1, 1, 8, 8).astype(\"float32\")\nX_reshaped.skb.draw_graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building a NN Classifier\n\nWe'll build a tiny CNN using PyTorch and wrap it with skorch to make it\nscikit-learn compatible. The architecture uses a single convolution + pooling\nstage and a small MLP head. The architectural choices below are meant to be:\n\n- **standard**: 3x3 convolutions and 2x2 max-pooling are very common\n- **small**: the dataset and images are tiny, so we keep the model tiny too\n\nIf you want more background on CNN building blocks and how convolution/pooling\nchanges tensor shapes, see the CS231n notes:\nhttps://cs231n.github.io/convolutional-networks/\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\nclass TinyCNN(nn.Module):\n    def __init__(self, conv_channels: int = 8, hidden_units: int = 32):\n        super().__init__()\n        self.conv_channels = conv_channels\n        self.hidden_units = hidden_units\n\n        # 2-level CNN with 2x2 max-pooling\n        self.conv1 = nn.Conv2d(\n            in_channels=1, out_channels=conv_channels, kernel_size=3, padding=1\n        )\n        self.conv2 = nn.Conv2d(conv_channels, conv_channels, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2)\n\n        # input shape = (8,8) -> conv1: (8,8) -> conv2: (8,8) -> pool: (4,4)\n        image_shape_after_conv = 4 * 4\n\n        # MLP head\n        self.fc1 = nn.Linear(conv_channels * image_shape_after_conv, hidden_units)\n        self.dropout = nn.Dropout(p=0.25)  # Regularization to avoid overfitting\n        self.fc2 = nn.Linear(hidden_units, 10)  # 10 digit classes (0..9)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.flatten(start_dim=1)\n        x = self.dropout(F.relu(self.fc1(x)))\n        return self.fc2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Skorch provides scikit-learn compatible wrappers around torch training loops.\nThat makes the torch model usable by skrub DataOps (and scikit-learn tools in\ngeneral).\n\nWe use :func:`skrub.choose_from()` to define hyperparameters that the DataOps\ngrid search will tune: conv_channels, hidden_units, and max_epochs.\nThe other parameters are set to common choices for this task and training data size.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skorch import NeuralNetClassifier\n\ndevice = \"cpu\"  # use \"cuda\" or \"mps\" if available\n\nnet = NeuralNetClassifier(\n    module=TinyCNN,\n    # These choices are intentionally small so the example runs quickly.\n    module__conv_channels=skrub.choose_from([8, 16], name=\"conv_channels\"),\n    module__hidden_units=skrub.choose_from([8, 16, 32], name=\"hidden_units\"),\n    max_epochs=skrub.choose_from([10, 15], name=\"max_epochs\"),\n    optimizer__lr=0.01,\n    optimizer=optim.Adam,\n    criterion=nn.CrossEntropyLoss,\n    device=device,\n    train_split=None,  # We'll use skrub's grid search for validation\n    verbose=0,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tuning the model's hyperparameters with DataOps\n\nWe integrate the model into the DataOps plan. First, we\nconvert the target labels to integers for the loss computation\nand apply the model to the preprocessed X and y.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_int = y.astype(\"int64\")\npredictor = X_reshaped.skb.apply(net, y=y_int)\npredictor.skb.draw_graph()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we use 4-fold cross-validation for the hyperparameter\ntuning on our DataOps plan.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n\ncv = KFold(n_splits=4, shuffle=True, random_state=42)\nsearch = predictor.skb.make_grid_search(\n    cv=cv,\n    fitted=True,\n    n_jobs=-1,\n)\nprint(\"\\nSearch results:\")\nprint(search.results_.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a better look at the well-performing models by looking\nat the parallel coordinates plot. We filter to models with\nscore >= 0.94 to focus on the top-performing configurations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = search.plot_results(min_score=0.94)\nfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interpreting the results\n\nLooking at the search results, we can observe several patterns:\n\n- **Model capacity matters**: Larger configurations with ``conv_channels=16``\n  and ``hidden_units=32`` tend to perform best. Smaller models with\n  ``conv_channels=8`` and/or ``hidden_units=8`` perform significantly worse,\n  indicating that the task benefits from increased model capacity.\n- **More epochs generally help**: Configurations with ``max_epochs=15`` tend to\n  perform slightly better than those with ``max_epochs=10``, though the gains\n  are modest compared to architectural changes.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n\nIn this example, we've shown how to use **PyTorch** and **skorch** within\nskrub DataOps. The key steps were:\n\n1. Define a PyTorch ``nn.Module`` (our ``TinyCNN``)\n2. Wrap it with skorch's ``NeuralNetClassifier`` to make it scikit-learn compatible\n3. Use :func:`skrub.choose_from()` to specify hyperparameters for tuning\n4. Integrate it into a DataOps plan and use grid search to find the best configuration\n\nThis pattern lets you leverage PyTorch's flexibility for model definition while\nbenefiting from skrub's hyperparameter tuning and data preprocessing capabilities.\n\n.. seealso::\n\n   * `example_tuning_pipelines`: Learn more about using\n     ``skrub.choose_from()`` and other choice objects to tune hyperparameters\n     in DataOps plans.\n   * `example_optuna_choices`: Discover how to use Optuna as a backend\n     for more sophisticated hyperparameter search strategies with skrub DataOps.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}