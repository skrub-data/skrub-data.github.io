{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the skrub examples in JupyterLite is experimental and you mayencounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import skrub` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/skrub-data/skrub/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport micropip\nawait micropip.install('skrub')\n%pip install pyodide-http\nimport pyodide_http\npyodide_http.patch_all()\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Tuning pipelines\n\nOur machine-learning pipeline typically contains some values or choices which\nmay influence its prediction performance, such as hyperparameters (e.g. the\nregularization parameter ``alpha`` of a ``RidgeClassifier``, the\n``learning_rate`` of a ``HistGradientBoostingClassifier``), which estimator to\nuse (e.g. ``RidgeClassifier`` or ``HistGradientBoostingClassifier``), or which\nsteps to include (e.g. should we join a table to bring additional information or\nnot).\n\nWe want to tune those choices by trying several options and keeping those that\ngive the best performance on a validation set.\n\nSkrub [expressions](10_expressions.html) provide a convenient way to specify\nthe range of possible values, by inserting it directly in place of the actual\nvalue. For example we can write:\n\n``RidgeClassifier(alpha=skrub.choose_from([0.1, 1.0, 10.0], name='\u03b1'))``\n\ninstead of:\n\n``RidgeClassifier(alpha=1.0)``.\n\nSkrub then inspects\nour pipeline to discover all the places where we used objects like\n``skrub.choose_from()`` and builds a grid of hyperparameters for us.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will illustrate hyperparameter tuning on the \"toxicity\" dataset. This\n dataset contains 1,000 texts and the task is to predict if they are\n flagged as being toxic or not.\n\nWe start from a very simple pipeline without any hyperparameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n\nimport skrub\nimport skrub.datasets\n\ndata = skrub.datasets.fetch_toxicity().toxicity\n\n# This dataset is sorted -- all toxic tweets appear first, so we shuffle it\ndata = data.sample(frac=1.0, random_state=1)\n\ntexts = data[[\"text\"]]\nlabels = data[\"is_toxic\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We mark the ``texts`` column as the input and the ``labels`` column as the target.\nSee [the previous example](10_expressions.html) for a more detailed explanation\nof ``skrub.X`` and ``skrub.y``.\nWe then encode the text with a ``MinHashEncoder`` and fit a\n``HistGradientBoostingClassifier`` on the resulting features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = skrub.X(texts)\nX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y = skrub.y(labels)\ny"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pred = X.skb.apply(skrub.MinHashEncoder()).skb.apply(\n    HistGradientBoostingClassifier(), y=y\n)\n\npred.skb.cross_validate(n_jobs=4)[\"test_score\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the sake of the example, we will focus on the number of ``MinHashEncoder``\ncomponents and the ``learning_rate`` of the ``HistGradientBoostingClassifier``\nto illustrate the ``skrub.choose_from(...)`` objects.\nWhen we use a scikit-learn hyperparameter-tuner like ``GridSearchCV`` or\n``RandomizedSearchCV``, we need to specify a grid of hyperparameters separately\nfrom the estimator, with something similar to\n``GridSearchCV(my_pipeline, param_grid={\"encoder__n_components: [5, 10, 20]\"})``.\nInstead, with skrub we can use\n``skrub.choose_from(...)`` directly where the actual value\nwould normally go. Skrub then takes care of constructing the\n``GridSearchCV``'s parameter grid for us.\n\nSeveral utilities are available:\n\n- ``choose_from`` to choose from a discrete set of values\n- ``choose_float`` and ``choose_int`` to sample numbers in a given range\n- ``choose_bool`` to choose between ``True`` and ``False``\n- ``optional`` to choose between something and ``None``; typically to make a\n  transformation step optional such as\n  ``X.skb.apply(skrub.optional(StandardScaler()))``\n\nChoices can be given a name which is used to display hyperparameter search\nresults and plots or to override their outcome. The name is optional.\n\nNote that ``skrub.choose_float()`` and ``skrub.choose_int()`` can be given a\n``log`` argument to sample in log scale.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = skrub.X(texts), skrub.y(labels)\n\nencoder = skrub.MinHashEncoder(\n    n_components=skrub.choose_int(5, 50, log=True, name=\"N components\")\n)\nclassifier = HistGradientBoostingClassifier(\n    learning_rate=skrub.choose_float(0.01, 0.9, log=True, name=\"lr\")\n)\npred = X.skb.apply(encoder).skb.apply(classifier, y=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then obtain an estimator that performs the hyperparameter search with\n``.skb.get_grid_search()`` or ``.skb.get_randomized_search()``. They accept\nthe same arguments as their scikit-learn counterparts (e.g. ``scoring`` and\n``n_jobs``). Also, like ``.skb.get_estimator()``, they accept a ``fitted``\nargument and if it is ``True`` the search is fitted on the data we provided\nwhen initializing our pipeline's variables.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search = pred.skb.get_randomized_search(n_iter=8, n_jobs=4, random_state=1, fitted=True)\nsearch.results_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If the plotly library is installed, we can visualize the results of the\nhyperparameter search with ``.plot_results()``.\nIn the plot below, each line represents a combination of hyperparameters (in\nthis case, only ``N components`` and ``learning rate``), and each column of\npoints represents either a hyperparameter, or the score of a given\ncombination of hyperparameters.\nThe color of the line represents the score of the combination of hyperparameters.\nThe plot is interactive, and it is  possible to select only a subset of the\nhyperparameters to visualize by dragging the mouse over each column to select\nthe desired range.\nThis is particularly useful when there are many combinations of hyperparameters,\nand we are interested in understanding which hyperparameters have the largest\nimpact on the score.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search.plot_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choices can appear in many places\n\nChoices are not limited to selecting estimator hyperparameters. They can also be\nused to choose between different estimators, or in place of any value used in\nour pipeline.\n\nFor example, here we pass a choice to pandas DataFrame's ``assign`` method.\nWe want to add a feature that captures the length of the text, but we are not\nsure if it is better to count length in characters or in words. We do not\nwant to add both because it would be redundant. We can add a column to the\ndataframe, which will be chosen among the length in characters or the length\nin words:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = skrub.X(texts), skrub.y(labels)\n\nX.assign(\n    length=skrub.choose_from(\n        {\"words\": X[\"text\"].str.count(r\"\\b\\w+\\b\"), \"chars\": X[\"text\"].str.len()},\n        name=\"length\",\n    )\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "``choose_from`` can be given a dictionary if we want to provide\nnames for the individual outcomes, or a list, when names are not needed:\n``choose_from([1, 100], name='N')``,\n``choose_from({'small': 1, 'big': 100}, name='N')``.\n\nChoices can be nested arbitrarily. For example, here we want to choose\nbetween 2 possible encoder types: the ``MinHashEncoder`` or the\n``StringEncoder``. Each of the possible outcomes contains a choice itself:\nthe number of components.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_components = skrub.choose_int(5, 50, log=True, name=\"N components\")\n\nencoder = skrub.choose_from(\n    {\n        \"minhash\": skrub.MinHashEncoder(n_components=n_components),\n        \"lse\": skrub.StringEncoder(n_components=n_components),\n    },\n    name=\"encoder\",\n)\nX.skb.apply(encoder, cols=\"text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a similar vein, we might want to choose between a HGB classifier and a Ridge\nclassifier, each with its own set of hyperparameters.\nWe can then define a choice for the classifier and a choice for the\nhyperparameters of each classifier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n\nhgb = HistGradientBoostingClassifier(\n    learning_rate=skrub.choose_float(0.01, 0.9, log=True, name=\"lr\")\n)\nridge = RidgeClassifier(alpha=skrub.choose_float(0.01, 100, log=True, name=\"\u03b1\"))\nclassifier = skrub.choose_from({\"hgb\": hgb, \"ridge\": ridge}, name=\"classifier\")\npred = X.skb.apply(encoder).skb.apply(classifier, y=y)\nprint(pred.skb.describe_param_grid())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "search = pred.skb.get_randomized_search(\n    n_iter=16, n_jobs=4, random_state=1, fitted=True\n)\nsearch.plot_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have a more complex pipeline, we can draw more conclusions from the\nparallel coordinate plot. For example, we can see that the\n``HistGradientBoostingClassifier``\nperforms better than the ``RidgeClassifier`` in most cases, that the ``StringEncoder``\noutperforms the ``MinHashEncoder``, and that the choice of the additional ``length``\nfeature does not have a significant impact on the score.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced usage\n\nThis section shows some more advanced or less frequently needed use cases.\n\n### Choices can depend on each other\n\nSometimes not all combinations (cross-product) of hyperparameter values make\nsense, and instead choices may be linked. For example, our downstream estimator\ncan be a ``RidgeClassifier`` or ``HistGradientBoostingClassifier``, and\nstandard scaling should be applied only when it is a ``Ridge``.\n\nSkrub choices have a ``match`` method to obtain different results depending\non the outcome of the choice.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n\nX, y = skrub.X(texts), skrub.y(labels)\n\nvectorized_X = X.skb.apply(skrub.MinHashEncoder())\n\nestimator_kind = skrub.choose_from([\"ridge\", \"HGB\"], name=\"estimator kind\")\n\nscaling = estimator_kind.match({\"ridge\": StandardScaler(), \"HGB\": \"passthrough\"})\nscaled_X = vectorized_X.skb.apply(scaling)\n\nclassifier = estimator_kind.match(\n    {\"ridge\": RidgeClassifier(), \"HGB\": HistGradientBoostingClassifier()}\n)\npred = scaled_X.skb.apply(classifier, y=y)\nprint(pred.skb.describe_param_grid())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we can see that there is only one parameter: the estimator kind. When it\nis ``\"ridge\"``, the ``StandardScaler`` and the ``RidgeClassifier`` are used;\nwhen it is ``\"HGB\"`` ``\"passthrough\"`` and the\n``HistGradientBoostingClassifier`` are used.\n\nSimilarly, objects returned by ``choose_bool`` have a ``if_else()`` method.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Choices can be turned into expressions\n\nWe can turn a choice (or the result of a choice ``match()`` or ``if_else``)\ninto an expression, so that we can keep chaining more operations onto it.\nHere, we create a ``.choose_bool()`` object to choose whether to add the length\nof the text as a feature or not. Then, ``if_else()`` will assign the length\nof the text to a new column ``length`` if the choice is ``True``, or do nothing\nif the choice is ``False``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = skrub.X(texts), skrub.y(labels)\n\nadd_length = skrub.choose_bool(name=\"add_length\")\nwith_length = add_length.if_else(X.assign(length=X[\"text\"].str.len()), X).as_expr()\nvectorized_X = with_length.skb.apply(skrub.MinHashEncoder(n_components=2), cols=\"text\")\n\n# Note: we can manually set the outcome of a choice when evaluating an\n# expression (or fitting an estimator)\n\nvectorized_X.skb.eval({\"add_length\": False})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vectorized_X.skb.eval({\"add_length\": True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Arbitrary logic depending on a choice\n\nWhen ``match`` or ``if_else`` are not enough and we need to apply arbitrary,\neager logic based on a choice we can resort to using ``skrub.deferred``. For\nexample the choice of adding the text length or not could also have been\nwritten as:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = skrub.X(texts), skrub.y(labels)\n\n\n@skrub.deferred\ndef extract_features(df, add_length):\n    if add_length:\n        return df.assign(length=df[\"text\"].str.len())\n    return df\n\n\nfeat = extract_features(X, skrub.choose_bool(name=\"add_length\")).skb.apply(\n    skrub.MinHashEncoder(n_components=2), cols=\"text\"\n)\n\nfeat.skb.eval({\"add_length\": False})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feat.skb.eval({\"add_length\": True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Concluding, we have seen how to use skrub's ``choose_from`` objects to tune\nhyperparameters, choose optional configurations, add features, and nest choices.\nWe then looked at how the different choices affect the pipeline and the prediction\nscores.\n\nThanks to the ``choose_from`` objects, Skrub expressions ease the process of\nhyperparameter tuning.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}