{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the skrub examples in JupyterLite is experimental and you mayencounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import skrub` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/skrub-data/skrub/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport micropip\nawait micropip.install('skrub')\n%pip install pyodide-http\nimport pyodide_http\npyodide_http.patch_all()\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Supervised estimators and cross-validation\n\nWe can use ``.skb.apply()`` to add scikit-learn transformers to the computation,\nbut also to add a supervised learner like ``HistGradientBoostingClassifier``\nor ``LogisticRegression``.\n\nWhen we have built a full estimator, we want to run cross-validation to\nmeasure its performance. We may also want to tune hyperparameters (shown in\nthe next example).\n\nSkrub can run the cross-validation for us. In order to do so, it needs to\nsplit the dataset into training and testing sets. For this to happen, we need\nto tell skrub which items in our pipeline constitute the feature matrix ``X``\nand the targets (labels, outputs) ``y``.\n\nIndeed, an estimator built from a skrub expression can accept inputs that are\nnot yet neatly organized into correctly-aligned ``X`` and ``y`` matrices.\nThere may be some steps (such as loading data, performing joins or\naggregations, separating features and targets from a single source) that need\nto be performed in order to construct the design matrix the targets.\n\nTo indicate which intermediate results to split, we call ``.skb.mark_as_X()``\nand ``.skb.mark_as_y()`` on the appropriate objects:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingRegressor\n\nimport skrub.datasets\n\ndataset = skrub.datasets.fetch_employee_salaries()\n\nfull_data = skrub.var(\"data\", dataset.employee_salaries)\n\nemployees = full_data.drop(columns=\"current_annual_salary\").skb.mark_as_X()\nsalaries = full_data[\"current_annual_salary\"].skb.mark_as_y()\n\nvectorizer = skrub.TableVectorizer(\n    high_cardinality=skrub.MinHashEncoder(n_components=8)\n)\npredictions = employees.skb.apply(vectorizer).skb.apply(\n    HistGradientBoostingRegressor(), y=salaries\n)\npredictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note how ``apply`` works for supervised estimators: we pass the targets\n(another skrub expression) as the argument for the parameter ``y``.\n\nIf you unfold the dropdown to see the computation graph, you will see the\ninner nodes that have been marked as ``X`` and ``y``: these nodes are highlighted\nin blue and red, respectively. Furthermore, the ``data`` is an input variable,\nso it is marked with a double box.\n\nTo perform\ncross-validation, skrub first runs all the prior steps until it has computed\n``X`` and ``y``. Then, it splits those (with the usual scikit-learn\ncross-validation tools, as is done in ``sklearn.cross_validate``) and runs\nthe rest of the computation inside of the cross-validation loop.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predictions.skb.cross_validate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the simplest case where we have ``X`` and ``y`` available right from the\nstart, we can indicate that simply by creating the input variables with\n``skrub.X()`` and ``skrub.y()`` instead of ``skrub.var()``: ``skrub.X()`` is\nsyntactic sugar for ``skrub.var(\"X\").skb.mark_as_X()``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The construction of ``X`` and ``y`` must be done before splitting the samples\ninto cross-validation folds; it happens outside of the cross-validation loop.\n\nThis means that any step we perform in this part of the computation has\naccess to the full data (training and test sets). We should not use\nestimators that learn from the data before reaching the cross-validation\nloop, or we might obtain optimistic scores due to data leakage.\nJoin operations that involve aggregations are not safe either, because they\nmay involve information from the test set. Finally, no\nchoices or hyperparameters can be tuned in this part of the computation\n(tuning is discussed in more detail in the next example).\n\nTherefore, we should build ``X`` and ``y`` at the very start of the\ncomputation and use ``mark_as_X()`` and ``mark_as_y()`` as soon as possible\n-- as soon as we have separate ``X`` and ``y`` tables that are aligned and\nhave one row per sample. In particular, we should use ``mark_as_X()`` before\ndoing any feature extraction and selection.\n\nSkrub will let us apply transformers before reaching ``mark_as_X`` because\nthere are special cases where we know it is safe and faster to do so, but in\ngeneral we should be careful and remember to separate features and targets\nand then use ``mark_as_X()`` as soon as possible.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A full example\n\nNow we know enough about skrub expressions to create a full estimator for a\nrealistic example and evaluate its performance.\n\nWe finally come back to the credit fraud dataset from the previous example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import skrub.datasets\n\ndataset = skrub.datasets.fetch_credit_fraud()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``baskets`` are orders on a e-commerce website. Note that in the drop-down\nmenu, ``\"baskets\"`` is both an input variable and a table that is\nmarked as ``X``, and therefore it is colored blue and has a double box.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "baskets = skrub.var(\"baskets\", dataset.baskets[[\"ID\"]]).skb.mark_as_X()\nbaskets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each basket is associated with a fraud flag. The ``\"fraud_flag\"`` column in the\n``baskets`` table is red as it is marked as ``y``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fraud_flags = skrub.var(\"fraud_flags\", dataset.baskets[\"fraud_flag\"]).skb.mark_as_y()\nfraud_flags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each basket contains one or several products. The ``\"basket_ID\"`` column\nin the ``\"products\"`` refers to the ``\"ID\"`` column in the ``\"baskets\"``\ntable. Given the information we have about a basket, we want to predict if\nits purchase was fraudulent.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "products = skrub.var(\"products\", dataset.products)\nproducts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The ``\"baskets\"`` table itself contains no\ninformation: we have to bring in features extracted from the corresponding\nproducts. To do so, we must first vectorize the products lines, so that we\ncan aggregate the extracted numeric features, and attach the resulting vector\nto the corresponding ``\"basket_ID\"`` and thus the corresponding fraud flag.\n\nWe do the vectorization with a ``TableVectorizer``. When we run the\ncross-validation, the ``\"baskets\"`` table (which we marked as X) will be\nsplit in train and test sets. To fit our ``TableVectorizer``, we want to use\nonly products that belong to a basket in the train set, not one in the test\nset. So we do a semi-join to exclude any products that do not belong to a\nbasket that can be found in the ``\"baskets\"`` table currently being\nprocessed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Note: using deferred or even defining a function is completely optional here,\n# because the computation it contains involves no control flow or assignments.\n# we only do it to simplify the computation graph by collapsing several\n# operations into a single function call.\n\n\n@skrub.deferred\ndef filter_products(products, baskets):\n    return products[products[\"basket_ID\"].isin(baskets[\"ID\"])]\n\n\nproducts = filter_products(products, baskets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We saw before that a transformer can be restricted to a subset of columns in\na dataframe. The ``cols`` argument can be a column name, a list of column\nnames, or a skrub selector. Skrub selectors are similar to Polars or Ibis\nselectors: they can be used to select columns by name (including glob and\nregex patterns), dtype or other criteria, and they can be combined with the\nsame operators as Python sets. Here we have a very simple selection to make:\nwe want to vectorize all columns in the ``\"products\"`` table, _except_ the\n``\"basket_ID\"`` column, which we will need for joining. So we can just use the\n``exclude_cols`` parameter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vectorizer = skrub.TableVectorizer(high_cardinality=skrub.StringEncoder(n_components=8))\nvectorized_products = products.skb.apply(vectorizer, exclude_cols=\"basket_ID\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we vectorized the product rows, we can aggregate them by basket ID\nbefore joining on the ``\"baskets\"`` table.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Here, deferred is still optional and is used to simplify the display of the\n# computation graph. By commenting it out, you will be able to see all the steps\n# that are performed in ``join_baskets_products`` to reach the final result.\n\n\n@skrub.deferred\ndef join_baskets_products(baskets, vectorized_products):\n    aggregated_products = (\n        vectorized_products.groupby(\"basket_ID\").agg(\"mean\").reset_index()\n    )\n    joined = baskets.merge(\n        aggregated_products, left_on=\"ID\", right_on=\"basket_ID\"\n    ).drop(columns=[\"ID\", \"basket_ID\"])\n    return joined\n\n\nfeatures = join_baskets_products(baskets, vectorized_products)\nfeatures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now add the supervised learner, the final step in our computation graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n\npredictions = features.skb.apply(HistGradientBoostingClassifier(), y=fraud_flags)\npredictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we can evaluate our estimator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cv_results = predictions.skb.cross_validate(scoring=\"roc_auc\", verbose=1, n_jobs=4)\ncv_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cv_results[\"test_score\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we are happy with the cross-validation scores, we might want to\nfit the estimator on the data we have, and save the estimator object\nso that we can use it later, say for next week's transactions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "estimator = predictions.skb.get_estimator(fitted=True)\nestimator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normally we would save it in a file; here we simulate that by pickling the\nmodel into a string so that our example notebook does not need to access the\nfilesystem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pickle\n\nsaved_model = pickle.dumps(estimator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us say we got some new data, and we want to use the model we just saved\nto make predictions on it.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_data = skrub.datasets.fetch_credit_fraud(split=\"test\")\nnew_baskets = new_data.baskets[[\"ID\"]]\nnew_products = new_data.products\nnew_products"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then load the saved model to make a prediction on the new data. To do so,\nwe need to pass the new data so that the same transformations can be applied to\nit.\nNote that the loaded estimator will expect the same input variables as the\noriginal pipeline, with the same names: this is why we pass a dictionary that\ncontains the new dataframes and the same variable names (``baskets`` and\n``products``) that we used when we built the original estimator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loaded_model = pickle.loads(saved_model)\nloaded_model.predict({\"baskets\": new_baskets, \"products\": new_products})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\nIn this example we have seen how to build a full estimator with skrub expressions,\nincluding a supervised learner, and how to evaluate it with cross-validation.\nThe example included complex data preparation and aggregation steps that involved\njoining dataframes: thanks to the skrub expressions, we could build a full\npipeline that ensures that no leakage occurs, and we were able to obtain a\nnew estimator that we could save and use later.\n\nThis is not all there is to the skrub expressions: in the next example we will\ngo over hyperparameter tuning, which expressions simplify a lot.\n\nA few more advanced features have not been shown and remain for more\nspecialized examples, for example:\n\n- naming nodes, passing the value for any node with the inputs\n- ``.skb.applied_estimator``\n- ``.skb.concat_horizontal``, ``.skb.drop`` and ``.skb.select``, more skrub selectors\n- ``.skb.freeze_after_fit`` (niche / very advanced)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}