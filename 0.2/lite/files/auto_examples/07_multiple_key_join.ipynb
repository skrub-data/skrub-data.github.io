{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class='alert alert-warning'>\n\n# JupyterLite warning\n\nRunning the skrub examples in JupyterLite is experimental and you mayencounter some unexpected behavior.\n\nThe main difference is that imports will take a lot longer than usual, for example the first `import skrub` can take roughly 10-20s.\n\nIf you notice problems, feel free to open an [issue](https://github.com/skrub-data/skrub/issues/new/choose) about it.\n</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# JupyterLite-specific code\nimport micropip\nawait micropip.install('https://files.pythonhosted.org/packages/eb/6d/0e78d028591bedd9580e49ae1060d9faf7fb4503e1db54227616db8f359d/skrub-0.2.0rc1-py3-none-any.whl')\n%pip install seaborn\n%pip install pyodide-http\nimport pyodide_http\npyodide_http.patch_all()\nimport matplotlib\nimport pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Spatial join for flight data: Joining across multiple columns\n\nJoining tables may be difficult if one entry on one side does not have\nan exact match on the other side.\n\nThis problem becomes even more complex when multiple columns\nare significant for the join. For instance, this is the case\nfor **spatial joins** on two columns, typically\nlongitude and latitude.\n\n|joiner| is a scikit-learn compatible transformer that enables\nperforming joins across multiple keys,\nindependantly of the data type (numerical, string or mixed).\n\nThe following example uses US domestic flights data\nto illustrate how space and time information from a\npool of tables are combined for machine learning.\n\n.. |fj| replace:: :func:`~skrub.fuzzy_join`\n\n.. |joiner| replace:: :func:`~skrub.Joiner`\n\n.. |Pipeline| replace::\n     :class:`~sklearn.pipeline.Pipeline`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Flight-delays data\nThe goal is to predict flight delays.\nWe have a pool of tables that we will use to improve our prediction.\n\nThe following tables are at our disposal:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The main table: flights dataset\n    - The `flights` datasets. It contains all US flights date, origin\n      and destination airports and flight time.\n      Here, we consider only flights from 2008.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\nfrom skrub.datasets import fetch_figshare\n\nflights = fetch_figshare(\"41771418\").X\n# Sampling for faster computation.\nflights = flights.sample(20_000, random_state=1, ignore_index=True)\nflights.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us see the arrival delay of the flights in the dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(style=\"ticks\")\n\nax = sns.histplot(data=flights, x=\"ArrDelay\")\nax.set_yscale(\"log\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interesting, most delays are relatively short (<100 min), but there\nare some very long ones.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Airport data: an auxiliary table from the same database\n    - The ``airports`` dataset, with information such as their name\n      and location (longitude, latitude).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "airports = fetch_figshare(\"41710257\").X\nairports.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Weather data: auxiliary tables from external sources\n    - The ``weather`` table. Weather details by measurement station.\n      Both tables are from the Global Historical Climatology Network.\n      Here, we consider only weather measurements from 2008.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "weather = fetch_figshare(\"41771457\").X\n# Sampling for faster computation.\nweather = weather.sample(100_000, random_state=1, ignore_index=True)\nweather.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The ``stations`` dataset. Provides location of all the weather\n  measurement stations in the US.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stations = fetch_figshare(\"41710524\").X\nstations.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Joining: feature augmentation across tables\nFirst we join the stations with weather on the ID (exact join):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "aux = pd.merge(stations, weather, on=\"ID\")\naux.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we join this table with the airports so that we get all auxilliary\ntables into one.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub import Joiner\n\njoiner = Joiner(airports, aux_key=[\"lat\", \"long\"], main_key=[\"LATITUDE\", \"LONGITUDE\"])\n\naux_augmented = joiner.fit_transform(aux)\n\naux_augmented.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Joining airports with flights data:\nLet's instanciate another multiple key joiner on the date and the airport:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "joiner = Joiner(\n    aux_augmented,\n    aux_key=[\"YEAR/MONTH/DAY\", \"iata\"],\n    main_key=[\"Year_Month_DayofMonth\", \"Origin\"],\n)\n\nflights.drop(columns=[\"TailNum\", \"FlightNum\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training data is then passed through a |Pipeline|:\n\n- We will combine all the information from our pool of tables into \"flights\",\nour main table.\n- We will use this main table to model the prediction of flight delay.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.pipeline import make_pipeline\n\nfrom skrub import TableVectorizer\n\ntv = TableVectorizer()\nhgb = HistGradientBoostingClassifier()\n\npipeline_hgb = make_pipeline(joiner, tv, hgb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We isolate our target variable and remove useless ID variables:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y = flights[\"ArrDelay\"]\nX = flights.drop(columns=[\"ArrDelay\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to frame this as a classification problem:\nsuppose that your company is obliged to reimburse the ticket\nprice if the flight is delayed.\n\nWe have a binary classification problem:\nthe flight was delayed (1) or not (0).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y = (y > 0).astype(int)\ny.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(pipeline_hgb, X, y)\nscores.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this example, we have combined multiple tables with complex joins\non imprecise and multiple-key correspondences.\nThis is made easy by skrub's |Joiner| transformer.\n\nOur final cross-validated accuracy score is 0.58.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}