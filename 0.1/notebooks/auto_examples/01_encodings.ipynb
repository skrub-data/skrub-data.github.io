{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Encoding: from a dataframe to a numerical matrix for machine learning\n\nThis example demonstrates how to transform a somewhat complicated dataframe\nto a matrix well suited for machine-learning. We study the case of predicting wages\nusing the [employee salaries](https://www.openml.org/d/42125) dataset.\n\n.. |TableVectorizer| replace::\n    :class:`~skrub.TableVectorizer`\n\n.. |Pipeline| replace::\n    :class:`~sklearn.pipeline.Pipeline`\n\n.. |OneHotEncoder| replace::\n     :class:`~sklearn.preprocessing.OneHotEncoder`\n\n.. |GapEncoder| replace::\n    :class:`~skrub.GapEncoder`\n\n.. |DatetimeEncoder| replace::\n    :class:`~skrub.DatetimeEncoder`\n\n.. |HGBR| replace::\n    :class:`~sklearn.ensemble.HistGradientBoostingRegressor`\n\n.. |RandomForestRegressor| replace::\n     :class:`~sklearn.ensemble.RandomForestRegressor`\n\n.. |permutation importances| replace::\n     :func:`~sklearn.inspection.permutation_importance`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A simple prediction pipeline\n\nLet's first retrieve the dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from skrub.datasets import fetch_employee_salaries\n\ndataset = fetch_employee_salaries()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We denote *X*, employees characteristics (our input data), and *y*,\nthe annual salary (our target column):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = dataset.X\ny = dataset.y\n\nX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe diverse columns in the dataset:\n  - binary (``'gender'``),\n  - numerical (``'employee_annual_salary'``),\n  - categorical (``'department'``, ``'department_name'``, ``'assignment_category'``),\n  - datetime (``'date_first_hired'``)\n  - dirty categorical (``'employee_position_title'``, ``'division'``).\n\nUsing skrub's |TableVectorizer|, we can now already build a machine-learning\npipeline and train it:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom skrub import TableVectorizer\n\npipeline = make_pipeline(TableVectorizer(), HistGradientBoostingRegressor())\npipeline.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What just happened here?\n\nWe actually gave our dataframe as an input to the |TableVectorizer| and it\nreturned an output useful for the scikit-learn model.\n\nLet's explore the internals of our encoder, the |TableVectorizer|:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n\n# Recover the TableVectorizer from the Pipeline\ntv = pipeline.named_steps[\"tablevectorizer\"]\n\npprint(tv.transformers_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe it has automatically assigned an appropriate encoder to\ncorresponding columns:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The |OneHotEncoder| for low cardinality string variables, the columns\n  ``'gender'``, ``'department'``, ``'department_name'`` and ``'assignment_category'``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tv.named_transformers_[\"low_cardinality\"].get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The |GapEncoder| for high cardinality string columns, ``'employee_position_title'``\n  and ``'division'``. The |GapEncoder| is a powerful encoder that can handle dirty\n  categorical columns.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tv.named_transformers_[\"high_cardinality\"].get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The |DatetimeEncoder| to the ``'date_first_hired'`` column. The |DatetimeEncoder|\n  can encode datetime columns for machine learning.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tv.named_transformers_[\"datetime\"].get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, it gave us interpretable column names.\n\nIn total, we have a reasonable number of encoded columns:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feature_names = tv.get_feature_names_out()\n\nlen(feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the cross-validated R2 score of our model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\nimport numpy as np\n\nscores = cross_val_score(pipeline, X, y)\nprint(f\"R2 score:  mean: {np.mean(scores):.3f}; std: {np.std(scores):.3f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The simple pipeline applied on this complex dataset gave us very good results.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature importances in the statistical model\n\nIn this section, after training a regressor, we will plot the feature importances.\n\n.. topic:: Note:\n\n  To minimize computation time, we use the feature importances computed by the\n  |RandomForestRegressor|, but you should prefer |permutation importances|\n  instead (which are less subject to biases).\n\nFirst, let's train another scikit-learn regressor, the |RandomForestRegressor|:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor()\n\npipeline = make_pipeline(TableVectorizer(), regressor)\npipeline.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are retrieving the feature importances:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "avg_importances = regressor.feature_importances_\nstd_importances = np.std(\n    [tree.feature_importances_ for tree in regressor.estimators_], axis=0\n)\nindices = np.argsort(avg_importances)[::-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And plotting the results:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\ntop_indices = indices[:20]\nlabels = feature_names[top_indices]\n\nplt.figure(figsize=(12, 9))\nplt.barh(\n    y=labels,\n    width=avg_importances[top_indices],\n    yerr=std_importances[top_indices],\n    color=\"b\",\n)\nplt.yticks(fontsize=15)\nplt.title(\"Feature importances\")\nplt.tight_layout(pad=1)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that features such the time elapsed since being hired, having a full-time employment, and the position, seem to be the most informative for prediction.\nHowever, feature importances must not be over-interpreted -- they capture statistical associations [rather than causal effects](https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation).\nMoreover, the fast feature importance method used here suffers from biases favouring features with larger cardinality, as illustrated in a scikit-learn [example](https://scikit-learn.org/dev/auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py).\nIn general we should prefer |permutation importances|, but it is a slower method.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this example, we motivated the need for a simple machine learning\npipeline, which we built using the |TableVectorizer| and a\n|HGBR|.\n\nWe saw that by default, it works well on a heterogeneous dataset.\n\nTo better understand our dataset, and without much effort, we were also able\nto plot the feature importances.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}